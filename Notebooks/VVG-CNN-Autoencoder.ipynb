{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "useful links:\n",
    "\n",
    "- Data Preparation for Variable Length Input Sequences, URL: https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "- Masking and padding with Keras, URL: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "- Step-by-step understanding LSTM Autoencoder layers, URL: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352XX, \n",
    "- Understanding input_shape parameter in LSTM with Keras, URL: https://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\n",
    "- tf.convert_to_tensor, URL: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n",
    "- ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int) in Python, URL: https://datascience.stackexchange.com/questions/82440/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample handling sklearn package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# # Keras + Tensorflow ML libraries\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(img_fpn):\n",
    "    ans = cv2.imread(img_fpn, cv2.IMREAD_UNCHANGED)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_img(img, minv, maxv):\n",
    "    rangev = maxv - minv\n",
    "    ans = img.astype(\"float32\")/float(rangev)\n",
    "    # ans = pd.Series(ans)\n",
    "    # respuesta de la funcion\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_img(img, h, w):\n",
    "    #  in case when you have odd number\n",
    "    top_pad = np.floor((h - img.shape[0]) / 2).astype(np.uint16)\n",
    "    bottom_pad = np.ceil((h - img.shape[0]) / 2).astype(np.uint16)\n",
    "    right_pad = np.ceil((w - img.shape[1]) / 2).astype(np.uint16)\n",
    "    left_pad = np.floor((w - img.shape[1]) / 2).astype(np.uint16)\n",
    "    ans = np.copy(np.pad(img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode=\"constant\", constant_values=0))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(rootf, src_df, src_col, tgt_col):\n",
    "    ans = src_df\n",
    "    src_files = list(ans[src_col])\n",
    "    tgt_files = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for tfile in src_files:\n",
    "        tfpn = os.path.join(rootf, tfile)\n",
    "        # print(tfpn)\n",
    "        # print(os.path.exists(tfpn))\n",
    "        timg = read_img(tfpn)\n",
    "        tgt_files.append(timg)\n",
    "\n",
    "    ans[tgt_col] = tgt_files\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_images(src_df, src_col, tgt_col, max_shape):\n",
    "    ans = src_df\n",
    "    src_images = list(ans[src_col])\n",
    "    tgt_images = list()\n",
    "    max_x, max_y = max_shape[0], max_shape[1]\n",
    "    padding = None\n",
    "\n",
    "    # ansdict = {}\n",
    "    for timg in src_images:\n",
    "        # print(timg)\n",
    "        timg = np.array(timg, dtype=\"object\")\n",
    "        # std_timg = std_img(timg, 0, 255)\n",
    "        pimg = pad_img(timg, max_y, max_x)\n",
    "        tgt_images.append(pimg)\n",
    "\n",
    "    ans[tgt_col] = tgt_images\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_images(src_df, src_col, tgt_col):\n",
    "    ans = src_df\n",
    "    src_images = list(ans[src_col])\n",
    "    tgt_images = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for timg in src_images:\n",
    "        # print(timg)\n",
    "        timg = np.array(timg, dtype=\"object\")\n",
    "        std_timg = std_img(timg, 0, 255)\n",
    "        tgt_images.append(std_timg)\n",
    "\n",
    "    ans[tgt_col] = tgt_images\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the max shape in the image dataset\n",
    "def get_mshape(shape_data, imgt):\n",
    "\n",
    "    max_x, max_y, max_ch = 0, 0, 0\n",
    "    shape_data = list(shape_data)\n",
    "    ans = None\n",
    "\n",
    "    if imgt == \"rgb\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            tshape = eval(tshape)\n",
    "            tx, ty, tch = tshape[0], tshape[1], tshape[2]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            if tch > max_ch:\n",
    "                max_ch = tch\n",
    "            \n",
    "        ans = (max_x, max_y, max_ch)\n",
    "    \n",
    "    elif imgt == \"bw\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            tshape = eval(tshape)\n",
    "            tx, ty = tshape[0], tshape[1]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            \n",
    "        ans = (max_x, max_y)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with predictions txt data\n",
    "imagef = \"Img\"\n",
    "\n",
    "# report subfolder\n",
    "reportf = \"Reports\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "stdf = \"Std\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "\n",
    "imgf = \"jpg\"\n",
    "\n",
    "rgb_sufix = \"rgb\"\n",
    "bw_sufix = \"bw\"\n",
    "\n",
    "# standard sufix\n",
    "stdprefix = \"std-\"\n",
    "\n",
    "# ml model useful data\n",
    "mltprefix = \"ml-\"\n",
    "\n",
    "# report names\n",
    "str_date = datetime.date.today().strftime(\"%d-%b-%Y\")\n",
    "\n",
    "small_sufix = \"Img-Data-Small\"\n",
    "large_sufix = \"Img-Data-Large\"\n",
    "\n",
    "gallery_prefix = \"VVG-Gallery\"\n",
    "\n",
    "# dataframe file name\n",
    "small_fn = stdprefix + gallery_prefix + \"-\" + small_sufix + \".\" + fext\n",
    "large_fn = stdprefix + gallery_prefix + \"-\" + large_sufix + \".\" + fext\n",
    "\n",
    "\n",
    "# ramdom seed\n",
    "randseed = 42\n",
    "\n",
    "# sample distribution train vs test sample size\n",
    "trainf = 0.80\n",
    "testf = 0.20\n",
    "\n",
    "# regex to know that column Im interested in\n",
    "keeper_regex = r\"(^ID$)|(^std_)\"\n",
    "\n",
    "imgt = rgb_sufix\n",
    "\n",
    "# default values\n",
    "work_fn, work_imgt, work_sufix = small_fn, imgt, small_sufix\n",
    "# work_fn, work_imgt, work_sufix  = large_fn, imgt, large_sufix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = os.getcwd()\n",
    "root_folder = os.path.split(root_folder)[0]\n",
    "root_folder = os.path.normpath(root_folder)\n",
    "print(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable reading\n",
    "# dataframe filepath\n",
    "fn_path = os.path.join(root_folder, dataf, stdf, work_fn)\n",
    "print(fn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rading training data\n",
    "# loading file\n",
    "source_df = pd.read_csv(\n",
    "                fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking everything is allrigth\n",
    "source_df.head(5)\n",
    "# chekcing the dataframe\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading images from folder and loading images into df\n",
    "# working variables\n",
    "src_col = work_imgt + \"_img\"\n",
    "tgt_col = work_imgt + \"_img\" + \"_data\"\n",
    "work_shape = work_imgt + \"_shape\"\n",
    "\n",
    "print(src_col, tgt_col)\n",
    "source_df = get_images(root_folder, source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching the biggest shape in the image files\n",
    "print(work_shape)\n",
    "shape_data = source_df[work_shape]\n",
    "max_shape = get_mshape(shape_data, work_imgt)\n",
    "print(max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding training data according to max shape of the images in gallery\n",
    "pad_prefix = \"pad_\"\n",
    "conv_prefix = \"cnn_\"\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = pad_prefix + conv_prefix + src_col\n",
    "\n",
    "print(src_col, tgt_col)\n",
    "source_df = padding_images(source_df, src_col, tgt_col, max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.info()\n",
    "# test = source_df[\"pad_cnn_rgb_img_data\"].value_counts()\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading images from folder and stadarizing images into df\n",
    "# working variables\n",
    "print(\"standarizing regular images...\")\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "\n",
    "source_df = standarize_images(source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"standarizing padded images...\")\n",
    "src_col = pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "\n",
    "source_df = standarize_images(source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting data to train\n",
    "# want to keep the columns starting with STD_\n",
    "df_columns = list(source_df.columns)\n",
    "print(\"------ original input/interested columns ------\")\n",
    "print(df_columns)\n",
    "\n",
    "# create the columns Im interesting in\n",
    "keep_columns = [i for i in df_columns if re.search(keeper_regex, i)]\n",
    "\n",
    "print(\"\\n\\n------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training dataframe\n",
    "train_df = pd.DataFrame(source_df, columns=keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the train dataframe\n",
    "train_df.head(5)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_img_col = \"std_\" + work_imgt + \"_img\" + \"_data\"\n",
    "padded_img_col = \"std_\" + pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_img_col = padded_img_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "X = np.array([np.array(i, dtype=\"object\") for i in train_df[working_img_col]], dtype=\"object\")\n",
    "y = np.array([np.array(j, dtype=\"object\") for j in train_df[working_img_col]], dtype=\"object\")\n",
    "\n",
    "# X = train_df[padded_col]\n",
    "# y = train_df[padded_col]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X[0]))\n",
    "print(type(X[0][0]))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y[0]))\n",
    "print(type(y[0][0]))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0].shape)\n",
    "print(y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn = X\n",
    "y_cnn = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing according to train/test proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cnn, y_cnn, train_size = trainf, test_size = testf, random_state = randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shaped\n",
    "print(\"training shape (X, y)\")\n",
    "print(\"X: \", X_train.shape, \" y: \", y_train.shape)\n",
    "\n",
    "print(\"testing shape (X, y)\")\n",
    "print(\"X: \", X_test.shape, \" y: \", y_test.shape)\n",
    "\n",
    "print(\"data types\")\n",
    "print(type(X_train), type(X_test), type(y_train), type(y_test))\n",
    "print(type(X_train[0]), type(X_test[0]), type(y_train[0]), type(y_test[0]))\n",
    "print(type(X_train[0][0]), type(X_test[0][0]), type(y_train[0][0]), type(y_test[0][0]))\n",
    "print(type(X_train[0][0][0]), type(X_test[0][0][0]), type(y_train[0][0][0]), type(y_test[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if working_img_col == regular_img_col:\n",
    "\n",
    "    Xtf_train = X_train\n",
    "    Xtf_test = X_test\n",
    "    ytf_train = y_train\n",
    "    ytf_test = y_test\n",
    "\n",
    "elif working_img_col == padded_img_col:\n",
    "    print(\"using\", working_img_col)\n",
    "    Xtf_train = tf.convert_to_tensor(X_train, dtype=\"float64\")\n",
    "    Xtf_test = tf.convert_to_tensor(X_test, dtype=\"float64\")\n",
    "    ytf_train = tf.convert_to_tensor(y_train, dtype=\"float64\")\n",
    "    ytf_test = tf.convert_to_tensor(y_test, dtype=\"float64\")\n",
    "\n",
    "# y_tensor = tf.convert_to_tensor(y, dtype=tf.float23) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neurons or processing units in LSTM\n",
    "# the number is because of good practices for NLP\n",
    "# min 200 max 500, normaly 300 (related to the semantic number of themes)\n",
    "# 120 for now in this test\n",
    "\n",
    "# timestep is 1 because you read a word at a time\n",
    "filters = 32\n",
    "print(\"CNN filter number:\", filters)\n",
    "\n",
    "in_shape = X_train[0].shape\n",
    "# in_shape = (None, None, 3)\n",
    "# in_shape = (None, None, 1)\n",
    "print(\"Input training shape:\", in_shape)\n",
    "\n",
    "# batch size\n",
    "bs = int(X_train.shape[0]*0.05)+1\n",
    "print(\"CNN learning batch size:\", bs)\n",
    "\n",
    "ksize = (3,3)\n",
    "psize = (2,2)\n",
    "print(\"CNN kernel size:\", ksize)\n",
    "print(\"CNN pad size:\", psize)\n",
    "\n",
    "# neurons/processing units size in the dense layer (THIS SHOULD BE SOM!!!!)\n",
    "mdn = 8*8*3\n",
    "mid_reshape = (8,8,3)\n",
    "print(\"Dense middle processing units:\", mdn)\n",
    "# dn2 = len(XB_set[0])*SECURITY_FACTOR\n",
    "\n",
    "# numero de neuronas de salida\n",
    "out_shape = X_train[0].shape\n",
    "print(\"Output prediction shape:\", out_shape)\n",
    "\n",
    "# axtivation functions\n",
    "inn = \"relu\"\n",
    "act = \"relu\"\n",
    "out = \"softmax\"\n",
    "\n",
    "# loss percentage\n",
    "ldrop = 0.2\n",
    "\n",
    "# padding policy\n",
    "pad = \"same\"\n",
    "\n",
    "# random seed\n",
    "randseed = 42\n",
    "\n",
    "# parameters to compile model\n",
    "# loss function\n",
    "# ls = \"mean_squared_error\"\n",
    "# ls = \"categorical_crossentropy\"\n",
    "ls = \"binary_crossentropy\"\n",
    "\n",
    "# doptimization function\n",
    "opti = \"adam\"\n",
    "# evaluation score\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parameters to exeute training\n",
    "# verbose mode\n",
    "ver = 1\n",
    "# training epocha\n",
    "epo = 300\n",
    "print(\"training epochs:\", epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model layers\n",
    "# to avoid overfit you need to use dropout in someplaces\n",
    "# options:\n",
    "# 1) Mask -> Drop -> LSTM1 -> LSTM2 -> LSTM3 -> Drop -> Dense -> Drop -> LSTM3 -> LSTM2 -> LSTM1 -> Drop -> TimeDistDense\n",
    "# 1) Mask -> Drop -> LSTM1 -> LSTM2 -> LSTM3 -> Dense -> LSTM3 -> LSTM2 -> LSTM1 -> Drop -> TimeDistDense\n",
    "# 2) Mask -> Drop -> LSTM1 -> LSTM2 -> LSTM2 -> Dense -> LSTM2 -> LSTM2 -> LSTM1 -> Drop -> TimeDistDense\n",
    "# 3) Mask -> Drop -> LSTM1 -> LSTM2 -> Drop -> Dense -> Drop -> LSTM2 -> LSTM1 -> Drop -> TimeDistDense\n",
    "# 3) Mask -> Drop -> LSTM1 -> Drop -> Dense -> Drop -> STM1 -> Drop -> TimeDistDense\n",
    "# 5) Mask -> Drop -> LSTM1 -> Drop -> Dense -> Drop -> LSTM1 -> TimeDistDense\n",
    "\n",
    "cnn_layers = (\n",
    "\n",
    "    # input layer (padding and prep)\n",
    "    Input(shape = in_shape, name = \"LayIn\"),\n",
    "\n",
    "    # intermediate convolutional encoder layer\n",
    "    Conv2D(filters, ksize, activation = act, padding = pad, input_shape = in_shape, name = \"EnConv1\"),\n",
    "    MaxPooling2D(psize, padding = pad, name = \"EnPool1\"),\n",
    "    Dropout(ldrop, name = \"EnDrop1\"),\n",
    "\n",
    "    # intermediate convolutional encoder layer\n",
    "    Conv2D(int(filters/2), ksize, activation=act, padding = pad, name = \"EnConv2\"),\n",
    "    MaxPooling2D(psize, padding = pad, name = \"EnPool2\"),\n",
    "    Dropout(ldrop, name = \"EnDrop2\"),\n",
    "\n",
    "    # intermediate convolutional encoder layer\n",
    "    Conv2D(int(filters/4), ksize, activation=act, padding = pad, name = \"EnConv3\"),\n",
    "    MaxPooling2D(psize, padding = pad, name = \"EnPool3\"),\n",
    "    Dropout(ldrop, name = \"EnDrop3\"),\n",
    "\n",
    "    # #from 2D to 1D\n",
    "    # Flatten(name = \"LayFlat\"),\n",
    "    # # mid dense encoding layer\n",
    "    # # dense layer for abstraction (THIS SHOULD COULD SOM!!!!)\n",
    "    Dense(mdn, activation = act, name = \"DenseMid\"),\n",
    "    # Dropout(ldrop, name = \"MidDrop\"),\n",
    "    # # from 1D to 2D\n",
    "    # Reshape(mid_reshape, name = \"layReshape\"),\n",
    "    \n",
    "    # intermediate convolutional decoder layer\n",
    "    Conv2D(int(filters/4), ksize, activation = act, padding = pad, name = \"DeConv1\"),\n",
    "    UpSampling2D(psize, name = \"DeUpsam1\"),\n",
    "    Dropout(ldrop, name = \"DeDrop1\"),\n",
    "\n",
    "    # intermediate convolutional decoder layer\n",
    "    Conv2D(int(filters/2), ksize, activation = act, padding = pad, name = \"DeConv2\"),\n",
    "    UpSampling2D(psize, name = \"DeUpsam2\"),\n",
    "    Dropout(ldrop, name = \"DeDrop2\"),\n",
    "\n",
    "    # intermediate convolutional decoder layer\n",
    "    Conv2D(filters, ksize, activation = act, padding = pad, name = \"DeConv3\"),\n",
    "    UpSampling2D(psize, name = \"DeUpsam3\"),\n",
    "    Dropout(ldrop, name = \"DeDrop3\"),\n",
    "    # capa de salida\n",
    "    # Reshape(inshape),\n",
    "    Conv2D(3, ksize, activation = out, padding = pad, input_shape = out_shape, name = \"LayOut\"),\n",
    ")"
   ]
  },
  {
   "source": [
    "# defining model\n",
    "cnn_model = Sequential(cnn_layers)\n",
    "cnn_model.model_name = \"CNN_Autoencoder\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "cnn_model.compile(loss = ls, optimizer = opti, metrics = met)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping condition BECAAUSE THIS ARE TESTS\n",
    "EarlyStopCNN_Acc = EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.01, patience = 30, verbose = 1, mode = \"max\", restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "history_cnn = cnn_model.fit(\n",
    "    x = Xtf_train, \n",
    "    y = ytf_train,\n",
    "    epochs = epo, \n",
    "    verbose = ver, \n",
    "    workers = 6,\n",
    "    batch_size = bs, \n",
    "    callbacks = [EarlyStopCNN_Acc],\n",
    "    shuffle = False,\n",
    "    use_multiprocessing = True,\n",
    "    validation_data = (Xtf_test, ytf_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationg model\n",
    "cnn_eval = cnn_model.evaluate(x = Xtf_test, y = ytf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general evaluation \n",
    "print(\"avg loss: \", cnn_eval[0])\n",
    "print(\"avg acc: \", cnn_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model\n",
    "cnn_results = cnn_model.predict(X_test, batch_size = bs, verbose = 1)#, batch_size = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "wdir = os.getcwd()\n",
    "models_folder = \"Models\"\n",
    "model_fname = \"vvg_cnn_autoencoder\"\n",
    "model_fpn = os.path.join(root_folder, dataf, models_folder, model_fname)\n",
    "print(\"The trained  model is:\", model_fpn)\n",
    "lstm_model.save(model_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking test shape\n",
    "print(lstm_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting results\n",
    "# reporte de entrenamiento para el modelo\n",
    "# base de la figura\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "# datos de la figura en de perdida y precision\n",
    "ax1.plot(history_lstm.history[\"loss\"], 'green', label = \"Train Loss\")\n",
    "ax1.plot(history_lstm.history[\"val_loss\"], 'royalblue', label = \"Test Loss\")\n",
    "ax2.plot(history_lstm.history[\"accuracy\"], 'green', label = \"Train Accuracy\")\n",
    "ax2.plot(history_lstm.history[\"val_accuracy\"], 'royalblue', label = \"Test Accuracy\")\n",
    "\n",
    "# leyenda de la grafica\n",
    "fig.suptitle(\"LEARNING BEHAVIOR\")\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "ax1.set_title(\"Loss\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax1.set(xlabel = \"Epoch [cycle]\", ylabel = \"loss [%]\")\n",
    "ax2.set(xlabel = \"Epoch [cycle]\", ylabel = \"Acc [%]\")\n",
    "fig.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving image in png file\n",
    "work_learn_img = model_fname + \"-\" + str_date + \"-\" + work_sufix + \"-learn-curve.\" + imgext\n",
    "img_fpn = os.path.join(root_folder, dataf, reportf, work_learn_img)\n",
    "print(os.path.exists(img_fpn))\n",
    "print(img_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving rendered image\n",
    "fig.savefig(img_fpn, dpi = fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FUTURE USE!!!! DEMO!!!\n",
    "lstm_test_text = \"\"\"\n",
    "                There is a khan's daughter\n",
    "                Who steps on in a SWINGING manner\n",
    "                And has the marks of twenty tigers,\n",
    "                Who steps on in a GRACEFUL manner\n",
    "                And has the marks of thirty manner\n",
    "                Who steps on in an ELEGANT manner\n",
    "                And has the marks of forty tigers,\n",
    "                Who steps on in a DELICATE manner\n",
    "                And has the marks of fifty tigers.\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}