{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "useful links:\n",
    "\n",
    "- Data Preparation for Variable Length Input Sequences, URL: https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "- Masking and padding with Keras, URL: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "- Step-by-step understanding LSTM Autoencoder layers, URL: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352XX, \n",
    "- Understanding input_shape parameter in LSTM with Keras, URL: https://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\n",
    "- tf.convert_to_tensor, URL: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n",
    "- ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int) in Python, URL: https://datascience.stackexchange.com/questions/82440/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type\n",
    "- How to Identify and Diagnose GAN Failure Modes, URL: https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/\n",
    "- How to Develop a GAN for Generating MNIST Handwritten Digits\n",
    ", URL: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "- How to Visualize a Deep Learning Neural Network Model in Keras\n",
    ", URL: https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- Tips for Training Stable Generative Adversarial Networks\n",
    ", URL: https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- How to Configure Image Data Augmentation in Keras, URL: https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "import datetime\n",
    "import copy\n",
    "import gc\n",
    "from statistics import mean\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# natural language processing packages\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading nlkt data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# sample handling sklearn package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "# # Keras + Tensorflow ML libraries\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers\n",
    "\n",
    "# preprocessing and processing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# shapping layers\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# basic layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# data processing layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "\n",
    "# recurrent and convolutional layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "\n",
    "# activarion function\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# optimization loss functions\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import SGD # OJO!\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam # OJO!\n",
    "from tensorflow.keras.optimizers import Adadelta # OJO!\n",
    "from tensorflow.keras.optimizers import Adagrad # OJO!\n",
    "\n",
    "# image augmentation and processing\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# GPU config if I have\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(physical_devices)"
   ]
  },
  {
   "source": [
    "# FUNCTION DEFINITION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the image from file with cv2\n",
    "def read_img(img_fpn):\n",
    "    ans = cv2.imread(img_fpn, cv2.IMREAD_UNCHANGED)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction to scale the image and reduce cv2\n",
    "def scale_img(img, scale_pct):\n",
    "\n",
    "    width = int(img.shape[1]*scale_pct/100)\n",
    "    height = int(img.shape[0]*scale_pct/100)\n",
    "    dim = (width, height)\n",
    "    # resize image\n",
    "    ans = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img.astype(\"float32\")/float(rangev)\n",
    "    \n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = (img.astype(\"float32\")-rangev)/rangev\n",
    "    # ans = pd.Series(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pad the image in the center\n",
    "def pad_img(img, h, w, img_type):\n",
    "    #  in case when you have odd number\n",
    "    ans = None\n",
    "    top_pad = np.floor((h - img.shape[0]) / 2).astype(np.uint8) # floor\n",
    "    bottom_pad = np.ceil((h - img.shape[0]) / 2).astype(np.uint8)\n",
    "    right_pad = np.ceil((w - img.shape[1]) / 2).astype(np.uint8)\n",
    "    left_pad = np.floor((w - img.shape[1]) / 2).astype(np.uint8) # floor\n",
    "    # print((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "    if img_type == \"rgb\":\n",
    "        ans = np.copy(np.pad(img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode=\"constant\", constant_values=0.0))   \n",
    "    if img_type == \"bw\":\n",
    "        ans = np.copy(np.pad(img, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), mode=\"constant\", constant_values=0))\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shape(src_df, img_col, shape_col):\n",
    "\n",
    "    ans = src_df\n",
    "    src_col = list(ans[img_col])\n",
    "    tgt_col = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for data in src_col:\n",
    "        tshape = data.shape\n",
    "        tgt_col.append(tshape)\n",
    "\n",
    "    ans[shape_col] = tgt_col\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to padd the images in the dataset, needs the shape, the type of image and the src + tgt columns of the frame to work with\n",
    "def padding_images(src_df, src_col, tgt_col, max_shape, img_type):\n",
    "    # ans = src_df\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "    max_x, max_y = max_shape[0], max_shape[1]\n",
    "\n",
    "    for timg in src_images:        \n",
    "        pimg = pad_img(timg, max_y, max_x, img_type)\n",
    "        tgt_images.append(pimg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the images in in memory\n",
    "def get_images(rootf, src_df, src_col, tgt_col, scale_pct):\n",
    "    ans = src_df\n",
    "    src_files = list(ans[src_col])\n",
    "    tgt_files = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for tfile in src_files:\n",
    "        tfpn = os.path.join(rootf, tfile)\n",
    "        timg = read_img(tfpn)\n",
    "        timg = scale_img(timg, scale_pct)\n",
    "        tgt_files.append(timg)\n",
    "\n",
    "    ans[tgt_col] = tgt_files\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to augment the images in the dataset and virtualy exapnd the training examples\n",
    "def augment_images(src_df, src_col, tgt_col, syth_num):\n",
    "\n",
    "    cols = [list(src_df.columns.values)]\n",
    "    # print(cols)\n",
    "    ans = pd.DataFrame()\n",
    "    other_cols = list(src_df.columns.values)\n",
    "    other_cols.remove(tgt_col)\n",
    "    other_cols.remove(src_col)\n",
    "    # print(other_cols)\n",
    "\n",
    "    for index, row in src_df.iterrows():\n",
    "        t_txt = row[src_col]\n",
    "        t_img = row[tgt_col]\n",
    "        t_tags = row[other_cols]\n",
    "\n",
    "        gen_rows = list()\n",
    "        for i in range(syth_num):\n",
    "\n",
    "            gen_tags = copy.deepcopy(t_tags)\n",
    "            gen_img = syth_img(t_img)\n",
    "            gen_txt = syth_text(t_txt)\n",
    "            # print(type(gen_tags), type(gen_img)) \n",
    "            gen_tags[tgt_col] = gen_img\n",
    "            gen_tags[src_col] = gen_txt\n",
    "            gen_rows.append(gen_tags)\n",
    "            # print(gen_tags) # , type(gen_img)) \n",
    "            # [other_cols], row[tgt_col])\n",
    "        \n",
    "        ans = ans.append(gen_rows, ignore_index=True)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize the images in the dataset, it has 2 options\n",
    "def standarize_images(src_df, src_col, tgt_col, img_type, std_opt):\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "\n",
    "    for timg in src_images:\n",
    "        # pcolor image\n",
    "        if img_type == \"rgb\":\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # b&w image\n",
    "        if img_type == \"rb\":\n",
    "            timg = np.asarray(timg) #, dtype=\"uint8\")\n",
    "            timg = timg[:,:,np.newaxis]\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # std_opt affect the standarization results\n",
    "        # result 0.0 < std_timg < 1.0\n",
    "        # result -1.0 < std_timg < 1.0\n",
    "        std_timg = std_img(timg, 0, 255, std_opt)\n",
    "        tgt_images.append(std_timg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function than rotates the original image to create a new example\n",
    "def syth_rgb_img(data):\n",
    "\n",
    "    samples = expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(rotation_range=90)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"uint8\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the max shape in the image dataset\n",
    "def get_mshape(shape_data, imgt):\n",
    "\n",
    "    max_x, max_y, max_ch = 0, 0, 0\n",
    "    shape_data = list(shape_data)\n",
    "    ans = None\n",
    "\n",
    "    if imgt == \"rgb\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty, tch = tshape[0], tshape[1], tshape[2]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            if tch > max_ch:\n",
    "                max_ch = tch\n",
    "            \n",
    "        ans = (max_x, max_y, max_ch)\n",
    "    \n",
    "    elif imgt == \"bw\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty = tshape[0], tshape[1]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            \n",
    "        ans = (max_x, max_y)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the pandas df data into usable word dense vector representation, YOU NEED IT FOR THE CSV to be useful!\n",
    "def format_dvector(work_corpus):\n",
    "\n",
    "    ans = list()\n",
    "    for dvector in work_corpus:\n",
    "        dvector = eval(dvector)\n",
    "        dvector = np.asarray(dvector)\n",
    "        ans.append(dvector)\n",
    "    ans = np.asarray(ans, dtype=\"object\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct to concatenate all label columns into one for a single y in ML training, returns a list\n",
    "def concat_labels(row, cname):\n",
    "\n",
    "    ans = list()\n",
    "    for c in cname:\n",
    "        r = row[c]\n",
    "        r = eval(r)\n",
    "        ans = ans + r\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the ML model\n",
    "def save_model(model, m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    # print(fpn)\n",
    "    model.save(fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the ML model\n",
    "def load_model(m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    model = keras.models.load_model(fpn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cast dataframe and avoid problems with keras\n",
    "def cast_batch(X_txt, X_img, y, labels):\n",
    "    X_txt = np.asarray(X_txt).astype(\"float32\")\n",
    "    X_img = np.asarray(X_img).astype(\"float32\")\n",
    "    y = np.asarray(y).astype(\"float32\")\n",
    "    labels = np.asarray(labels).astype(\"float32\")\n",
    "    return X_txt, X_img, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select real elements to train the discriminator\n",
    "def gen_real_samples(X_txt, X_img, y, y_labels, sample_size, half_batch):\n",
    "\n",
    "    rand_index = np.random.randint(0, sample_size, size=half_batch)\n",
    "    Xt_real = X_txt[rand_index]\n",
    "    Xi_real = X_img[rand_index]\n",
    "    y_real = y[rand_index]\n",
    "    yl_real = y_labels[rand_index]\n",
    "    # noise = np.random.uniform(0.0, 0.05, size=y_real.shape)\n",
    "    # y_real = np.subtract(y_real, noise)\n",
    "    Xt_real, Xi_real, y_real, yl_real = cast_batch(Xt_real, Xi_real, y_real, yl_real)\n",
    "    real_data = (Xt_real, Xi_real, y_real, yl_real)\n",
    "    return real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create fake elements to train the discriminator\n",
    "def gen_fake_samples(gen_model, txt_size, cat_size, label_size, half_batch):\n",
    "    \n",
    "    # random text\n",
    "    Xt_fake = gen_latent_txt(txt_size, half_batch)\n",
    "    # random labels for the text\n",
    "    yl_fake = get_fake_neglbls(label_size, half_batch)\n",
    "    # random generated image from the model\n",
    "    Xi_fake = gen_model.predict([Xt_fake, yl_fake])\n",
    "    # marking the images as fake in all accounts\n",
    "    y_fake = get_fake_negative(cat_size, half_batch)\n",
    "\n",
    "    # casting data type\n",
    "    Xt_fake, Xi_fake, y_fake, yl_fake = cast_batch(Xt_fake, Xi_fake, y_fake, yl_fake)\n",
    "    fake_data = (Xt_fake, Xi_fake, y_fake, yl_fake)\n",
    "    \n",
    "    return fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create one fake + real samples to train the discriminator\n",
    "def complete_batch(Xt_real, Xi_real, y_real, Xt_fake, Xi_fake, y_fake):\n",
    "\n",
    "    # this batch needs txt to create images, the images themselves, and the images labels\n",
    "    Xt = np.concatenate((Xt_real, Xt_fake), axis=0)\n",
    "    Xi = np.concatenate((Xi_real, Xi_fake), axis=0)\n",
    "    y = np.concatenate((y_real, y_fake), axis=0)\n",
    "    # y = np.concatenate((y_real, y_fake), axis=0)\n",
    "    # Xt, Xi, y = cast_batch(Xt, Xi, y)\n",
    "    \n",
    "    return Xt, Xi, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate random/latent text for image generator\n",
    "def gen_latent_txt(txt_shape, n_samples):\n",
    "\n",
    "    ans = None\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # be aware of this!!!!!!!\n",
    "        noise = np.random.normal(0.0, 1.0, size=txt_shape)\n",
    "        if ans is None:\n",
    "            txt = np.expand_dims(noise, axis=0)\n",
    "            ans = txt\n",
    "        else:\n",
    "            img = np.expand_dims(txt, axis=0)\n",
    "            ans = np.concatenate((ans, txt), axis=0)\n",
    "    # print(ans.shape)\n",
    "    # print(ans[0])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfunction to smooth the fake positives\n",
    "def smooth_positive_labels(y):\n",
    "\treturn y - 0.3 + (np.random.random(y.shape)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfunction to smooth the fake negatives\n",
    "def smooth_negative_labels(y):\n",
    "\treturn y + np.random.random(y.shape)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly flip some labels\n",
    "def noisy_labels(y, p_flip):\n",
    "\t# determine the number of labels to flip\n",
    "\tn_select = int(p_flip * y.shape[0])\n",
    "\t# choose labels to flip\n",
    "\tflip_ix = np.random.choice([i for i in range(y.shape[0])], size=n_select)\n",
    "\t# invert the labels in place\n",
    "\ty[flip_ix] = 1 - y[flip_ix]\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake true categories for the generator\n",
    "def get_fake_positive(cat_size, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_size)\n",
    "    ans = np.ones(sz)\n",
    "    # smooothing fakes\n",
    "    ans = smooth_positive_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    # ans = np.ones((batch_size, cat_size), dtype=\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake negative category to train the GAN\n",
    "def get_fake_negative(cat_size, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_size)\n",
    "    ans = np.zeros(sz)\n",
    "    ans = smooth_negative_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    # ans = np.ones((batch_size, cat_size), dtype=\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate fake labels to train the gan\n",
    "def get_fake_neglbls(label_size, batch_size):\n",
    "\n",
    "    sz = (batch_size, label_size)\n",
    "    # ans = np.random.randint(0,1, size=sz)\n",
    "    ans = np.zeros(sz)\n",
    "    ans = smooth_negative_labels(ans)\n",
    "    # ans = smooth_positive_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    # ans = np.ones((batch_size, cat_size), dtype=\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_fake_labels\n",
    "# function to generate fake labels to train the gan\n",
    "def get_fake_poslbls(label_size, batch_size):\n",
    "\n",
    "    sz = (batch_size, label_size)\n",
    "    # ans = np.random.randint(0,1, size=sz)\n",
    "    ans = np.ones(sz)\n",
    "    # ans = smooth_negative_labels(ans)\n",
    "    ans = smooth_positive_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    # ans = np.ones((batch_size, cat_size), dtype=\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an expanded bath of images for training with some synthetic ones\n",
    "def gen_synthetic_images(X_img, img_size, batch_size, synth_size):\n",
    "\n",
    "    ans = None\n",
    "\n",
    "    # iterating the images and synth new ones\n",
    "    for img in X_img:\n",
    "        gen_img = None\n",
    "\n",
    "        # creating new ones\n",
    "        for j in range(synth_size):\n",
    "\n",
    "            if gen_img is None:\n",
    "                timg = syth_std_img(img)\n",
    "                timg = np.expand_dims(timg, axis=0)\n",
    "                gen_img = timg\n",
    "            \n",
    "            else:\n",
    "                timg = syth_std_img(img)\n",
    "                timg = np.expand_dims(timg, axis=0)\n",
    "                gen_img = np.concatenate((gen_img, timg), axis=0)\n",
    "        \n",
    "        # adding it to the training batch\n",
    "        if ans is None:\n",
    "            ans = gen_img\n",
    "\n",
    "        else:\n",
    "            ans = np.concatenate((ans, gen_img), axis=0)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create text similar to the original one with 5% of noise\n",
    "def syth_text(data, nptc=0.02):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetizing a noisy std image from real data\n",
    "def syth_std_img(data):\n",
    "\n",
    "    samples = np.expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=15)\n",
    "    # datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True, vertical_flip=True)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"float32\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create new categories with some noise, default 5%\n",
    "def syth_categories(data, nptc=0.03):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to artificially span a batch with some noise and alterations by an specific number\n",
    "def expand_samples(data, synth_batch):\n",
    "\n",
    "    X_txt = data[0]\n",
    "    X_img = data[1]\n",
    "    y = data[2]\n",
    "    labels = data[3]\n",
    "\n",
    "    # creating the exapnded batch response\n",
    "    Xe_txt, Xe_img, ye, lbe = None, None, None, None\n",
    "\n",
    "    # iterating in the original batch\n",
    "    for Xtt, Xit, yt, lb in zip(X_txt, X_img, y, labels):\n",
    "\n",
    "        # temporal synth minibatch per original image\n",
    "        synth_Xt, synth_Xi, synth_y, synth_lb = None, None, None, None\n",
    "\n",
    "        # synthetizing artificial data for the batch\n",
    "        for i in range(synth_batch):\n",
    "\n",
    "            # generating first element\n",
    "            if (synth_Xt is None) and (synth_Xi is None) and (synth_y is None) and (synth_lb is None):\n",
    "                # gen text\n",
    "                gen_Xt = copy.deepcopy(Xtt)\n",
    "                gen_Xt = np.array(gen_Xt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = gen_Xt\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = gen_Xi\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = gen_yt\n",
    "\n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = gen_lb\n",
    "\n",
    "            # generatin the rest of the elements\n",
    "            else:\n",
    "                # gen text\n",
    "                gen_Xt = syth_text(Xtt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = np.concatenate((synth_Xt, gen_Xt), axis=0)\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = np.concatenate((synth_Xi, gen_Xi), axis=0)\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = np.concatenate((synth_y, gen_yt), axis=0)\n",
    "        \n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = np.concatenate((synth_lb, gen_lb), axis=0)\n",
    "\n",
    "        # adding the first part to the training batch\n",
    "        if (Xe_txt is None) and (Xe_img is None) and (ye is None) and (lbe is None):\n",
    "            # adding text\n",
    "            Xe_txt = synth_Xt\n",
    "            # adding images\n",
    "            Xe_img = synth_Xi\n",
    "            # adding categories\n",
    "            ye = synth_y\n",
    "            # adding labels\n",
    "            lbe = synth_lb\n",
    "\n",
    "        # adding the rest of the batch\n",
    "        else:\n",
    "            # adding text\n",
    "            Xe_txt = np.concatenate((Xe_txt, synth_Xt), axis=0)\n",
    "            # adding images\n",
    "            Xe_img = np.concatenate((Xe_img, synth_Xi), axis=0)\n",
    "            # adding category\n",
    "            ye = np.concatenate((ye, synth_y), axis=0)\n",
    "            # adding labels\n",
    "            lbe = np.concatenate((lbe, synth_lb), axis=0)\n",
    "\n",
    "    Xe_txt, Xe_img, ye, lbe = cast_batch(Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    e_data = (Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    return e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drift_labels(Xt_real, Xi_real, y_real, Xt_fake, Xi_fake, y_fake, batch_size, drift_pct):\n",
    "def drift_labels(real_data, fake_data, batch_size, drift_pct):\n",
    "\n",
    "    # real data asignation\n",
    "    Xt_real = real_data[0]\n",
    "    Xi_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "    yl_real = real_data[3]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xt_fake = fake_data[0]\n",
    "    Xi_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "    yl_fake = fake_data[3]\n",
    "\n",
    "    # setting the size for the drift labels\n",
    "    drift_size = int(math.ceil(drift_pct*batch_size))\n",
    "    # random index for drift elements!!!\n",
    "    rand_drifters = np.random.choice(batch_size, size=drift_size, replace=False)\n",
    "    # print(\"batch size\", batch_size, \"\\nrandom choise to change\", drift_size, \"\\n\", rand_drifters)\n",
    "\n",
    "    for drift in rand_drifters:\n",
    "\n",
    "        # copying temporal real data\n",
    "        Xt_drift = copy.deepcopy(Xt_real[drift])\n",
    "        Xi_drift = copy.deepcopy(Xi_real[drift])\n",
    "        y_drift = copy.deepcopy(y_real[drift])\n",
    "        yl_drift = copy.deepcopy(yl_real[drift])\n",
    "        # print(\"OG real y:\", y_drift)\n",
    "        # print(\"OG fake y:\", y_fake[drift])\n",
    "        \n",
    "        # replacing real with fakes\n",
    "        Xt_real[drift] = copy.deepcopy(Xt_fake[drift])\n",
    "        Xi_real[drift] = copy.deepcopy(Xi_fake[drift])\n",
    "        y_real[drift] = copy.deepcopy(y_fake[drift])\n",
    "        yl_real[drift] = copy.deepcopy(yl_fake[drift])\n",
    "        # print(\"New real y:\", y_real[drift])\n",
    "\n",
    "        # updating fakes with temporal original\n",
    "        Xt_fake[drift] = Xt_drift\n",
    "        Xi_fake[drift] = Xi_drift\n",
    "        y_fake[drift] = y_drift\n",
    "        yl_fake[drift] = yl_drift\n",
    "        # print(\"New fake y:\", y_fake[drift])\n",
    "\n",
    "        real_data = (Xt_real, Xi_real, y_real, yl_real)\n",
    "        fake_data = (Xt_fake, Xi_fake, y_fake, yl_fake)\n",
    "\n",
    "    return real_data, fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functioon to log the training results\n",
    "def test_model(epoch, gen_model, dis_model, X_txt, X_img, y, labels, half_batch, report_fn_path, synth_batch):\n",
    "\n",
    "    # register shape\n",
    "    txt_shape = X_txt[0].shape\n",
    "    img_shape = X_img[0].shape\n",
    "    cat_shape = y[0].shape\n",
    "    label_shape = labels[0].shape\n",
    "\n",
    "    # sample size\n",
    "    txt_size = X_txt.shape[0]\n",
    "    img_size = X_img.shape[0]\n",
    "    cat_size = y.shape[0]\n",
    "    label_size = labels.shape[0]\n",
    "\n",
    "    # select real txt2img for discrimintator\n",
    "    real_data = gen_real_samples(X_txt, X_img, y, labels, img_size, half_batch)\n",
    "    # expand the training sample for the discriminator\n",
    "    # real_data = expand_samples(real_data, synth_batch)\n",
    "    # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "\n",
    "    # create false txt for txt2img for generator\n",
    "    fake_data = gen_fake_samples(gen_model, txt_shape, cat_shape[0], label_shape[0], half_batch)\n",
    "    # expand the training sample for the discriminator\n",
    "    # fake_data = expand_samples(fake_data, synth_batch)\n",
    "    # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[1]\n",
    "\n",
    "    # plotting gen images\n",
    "    plot_gen_images(Xi_fake, epoch, report_fn_path, 3)\n",
    "\n",
    "    # real_batch = int((half_batch*synth_batch)/2)\n",
    "    real_batch = int(half_batch)\n",
    "\n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, half_batch, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xt_real = real_data[0]\n",
    "    Xi_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "    yl_real = real_data[3]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xt_fake = fake_data[0]\n",
    "    Xi_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "    yl_fake = fake_data[3]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate([Xi_real, yl_real], y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate([Xi_fake, yl_fake], y_fake, verbose=0)\n",
    "    # test_cgen = gen_model.evaluate([Xt_fake, yl_fake], y_fake, verbose=0)\n",
    "\n",
    "    # summarize discriminator performance\n",
    "    print(\"Batch Size %d -> Samples: Fake: %d & Real: %d\" % (half_batch*synth_batch, real_batch, real_batch))\n",
    "    print(\">>> Test Fake -> Acc: %.3f || Loss: %.3f\" % (test_fake[1], test_fake[0]))\n",
    "    print(\">>> Test Real -> Acc: %.3f || Loss: %.3f\" % (test_real[1], test_real[0]))\n",
    "    # print(\">>> Test Gen -> Acc: %.3f || Loss: %.3f\" % (test_cgen[1], test_cgen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def inv_std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = img+rangev\n",
    "        ans = ans*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the generated images within a training epoch\n",
    "def plot_gen_images(examples, epoch, report_fp_name, n_sample):\n",
    "\n",
    "    # get important data for iterating\n",
    "    example_size = examples.shape[0]\n",
    "    og_shape = examples[0].shape\n",
    "    rand_img = np.random.choice(example_size, size=n_sample*n_sample, replace=False) \n",
    "    # (0, example_size, size=n_sample*n_sample)\n",
    "\n",
    "    # prep the figure\n",
    "    fig, ax = plt.subplots(n_sample,n_sample, figsize=(20,20))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n_sample*n_sample):\n",
    "        # define subplot\n",
    "        plt.subplot(n_sample, n_sample, 1+i)\n",
    "\n",
    "        # getting the images from sample\n",
    "        rand_i = rand_img[i]\n",
    "        gimg = examples[rand_i]\n",
    "        gimg = inv_std_img(gimg, 0, 255, \"ctr\")\n",
    "\n",
    "        # turn off axis\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(gimg) #, interpolation=\"nearest\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"GENERATED IMAGES\", fontsize=50)\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-Gen-img-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_metrics(disr_hist, disf_hist, gan_hist, report_fp_name, epoch):\n",
    "\n",
    "    # reporting results\n",
    "    disr_hist = np.array(disr_hist)\n",
    "    disf_hist = np.array(disf_hist)\n",
    "    gan_hist = np.array(gan_hist)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # loss\n",
    "    ax1.plot(disr_hist[:,1], \"royalblue\", label=\"Loss: R-Dis\")\n",
    "    ax1.plot(disf_hist[:,1], \"crimson\", label=\"Loss: F-Dis\")\n",
    "    ax1.plot(gan_hist[:,1], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "    # ax1.plot(gan_hist[:], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "\n",
    "    # acc_\n",
    "    ax2.plot(disr_hist[:,0], \"royalblue\", label=\"Acc: R-Dis\")\n",
    "    ax2.plot(disf_hist[:,0], \"crimson\", label=\"Acc: F-Dis\")\n",
    "    ax2.plot(gan_hist[:,0], \"blueviolet\", label=\"Acc: GAN/Gem\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"LEARNING BEHAVIOR\", fontsize=20)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax1.set(xlabel = \"Epoch [cycle]\", ylabel = \"Loss\")\n",
    "    ax2.set(xlabel = \"Epoch [cycle]\", ylabel = \"Acc\")\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-learn-curve-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the loss and accuracy avg in multiple batchs of an epoch\n",
    "def epoch_avg(log):\n",
    "    loss, acc = None, None\n",
    "\n",
    "    # if acc and loss are present to avg\n",
    "    if type(log[0]) is list:\n",
    "        if len(log) > 0:\n",
    "\n",
    "            acc_list = list()\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                ta = l[0]\n",
    "                tl = l[1]\n",
    "\n",
    "                acc_list.append(ta)\n",
    "                loss_list.append(tl)\n",
    "\n",
    "            loss, acc = mean(loss_list), mean(acc_list)\n",
    "        return loss, acc\n",
    "    \n",
    "    else:\n",
    "        # if only loss is present\n",
    "        if len(log) > 0:\n",
    "\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                loss_list.append(l)\n",
    "\n",
    "            loss = mean(loss_list)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model, needs the dirpath, the name and the datetime to save\n",
    "def export_model(model, models_fp_name, filename, datetime):\n",
    "\n",
    "    ss = True\n",
    "    sln = True\n",
    "    fext = \"png\"\n",
    "    fpn = filename + \"-\" + datetime\n",
    "    fpn = filename + \".\" + fext\n",
    "    fpn = os.path.join(models_fp_name, fpn)\n",
    "    plot_model(model, to_file=fpn, show_shapes=ss, show_layer_names=sln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format data to save in file\n",
    "def format_metrics(disr_history, disf_history, gan_history):\n",
    "\n",
    "    headers, data = None, None\n",
    "\n",
    "    disr_hist = np.array(disr_history)\n",
    "    disf_hist = np.array(disf_history)\n",
    "    gan_hist = np.array(gan_history)\n",
    "\n",
    "    # formating file headers\n",
    "    headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\", \"gen_gan_acc\"]\n",
    "    # headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\",] # \"gen_gan_acc\"]\n",
    "\n",
    "    # formating fake discriminator train data\n",
    "    drhl = disr_hist[:,1]\n",
    "    drha = disr_hist[:,0]\n",
    "\n",
    "    # formating real discrimintator train data\n",
    "    dfhl = disf_hist[:,1]\n",
    "    dfha = disf_hist[:,0]\n",
    "\n",
    "    # formating gan/gen train data\n",
    "    # gghl = gan_hist[:]# .flatten()\n",
    "    gghl = gan_hist[:,1]\n",
    "    ggha = gan_hist[:,0]\n",
    "\n",
    "    # adding all formatted data into list\n",
    "    data = np.column_stack((drhl, drha, dfhl, dfha, gghl, ggha))\n",
    "    # data = np.column_stack((drhl, drha, dfhl, dfha, gghl)) #, ggha))\n",
    "\n",
    "    return data, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write data in csv file\n",
    "def write_metrics(data, headers, report_fp_name, filename):\n",
    "\n",
    "    # print(report_fp_name, filename)\n",
    "    fpn = filename + \"-train-history.csv\"\n",
    "    fpn = os.path.join(report_fp_name, fpn)\n",
    "\n",
    "    history_df = pd.DataFrame(data, columns=headers)\n",
    "    tdata = history_df.to_csv(\n",
    "                            fpn,\n",
    "                            sep=\",\",\n",
    "                            index=False,\n",
    "                            encoding=\"utf-8\",\n",
    "                            mode=\"w\",\n",
    "                            quoting=csv.QUOTE_ALL\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to safe the loss/acc logs in training for the gan/gen/dis models\n",
    "def save_metrics(disr_history, disf_history, gan_history, report_fp_name, filename):\n",
    "\n",
    "    data, headers = format_metrics(disr_history, disf_history, gan_history)\n",
    "    write_metrics(data, headers, report_fp_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to know the time between epochs or batchs it return the new time for a new calculation\n",
    "def lapse_time(last_time, epoch):\n",
    "\n",
    "    now_time = datetime.datetime.now()\n",
    "    deltatime = now_time - last_time\n",
    "    deltatime = deltatime.total_seconds()\n",
    "    deltatime = \"%.2f\" % deltatime\n",
    "    msg = \"Epoch:%3d \" % int(epoch+1)\n",
    "    msg = msg + \"elapsed time: \" + str(deltatime) + \" [s]\"\n",
    "    print(msg)\n",
    "    return now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special function to train the GAN\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "def train(gen_model, dis_model, gan_model, X_img, X_txt, y, labels, epochs, batch_size, save_intervas, fn_config):\n",
    "\n",
    "    # sample shape\n",
    "    txt_shape = X_txt[0].shape\n",
    "    img_shape = X_img[0].shape\n",
    "    cat_shape = y[0].shape\n",
    "    label_shape = labels[0].shape\n",
    "\n",
    "    # sample size\n",
    "    txt_size = X_txt.shape[0]\n",
    "    img_size = X_img.shape[0]\n",
    "    cat_size = y.shape[0]\n",
    "    label_size = labels.shape[0]\n",
    "\n",
    "    # augmentation factor\n",
    "    synth_batch = 1 # OJO!\n",
    "    n = 3\n",
    "\n",
    "    # model IO configuration\n",
    "    model_fn_path = fn_config[0]\n",
    "    report_fn_path = fn_config[1]\n",
    "    dis_model_name = fn_config[2]\n",
    "    gen_model_name = fn_config[3]\n",
    "    gan_model_name = fn_config[4]\n",
    "\n",
    "    # fake/real batch division\n",
    "    half_batch = int(batch_size/2)\n",
    "    batch_per_epoch = int(txt_size/batch_size)\n",
    "    real_batch = int((batch_size*synth_batch)/2)\n",
    "    # batch_per_epoch = int((txt_size*synth_batch)/batch_size)\n",
    "\n",
    "\t# prepare lists for storing stats each epoch\n",
    "    # disf_hist, disr_hist, gen_hist, gan_hist = list(), list(), list(), list()\n",
    "    disf_hist, disr_hist, gan_hist = list(), list(), list()\n",
    "    train_time = None\n",
    "    # iterating in training epochs:\n",
    "    for ep in range(epochs+1):\n",
    "        # epoch logs\n",
    "        # ep_disf_hist, ep_disr_hist, ep_gen_hist, ep_gan_hist = list(), list(), list(), list()\n",
    "        ep_disf_hist, ep_disr_hist, ep_gan_hist = list(), list(), list()\n",
    "        train_time = datetime.datetime.now()\n",
    "\n",
    "        # iterating over training batchs\n",
    "        for batch in range(batch_per_epoch):\n",
    "\n",
    "            # select real txt2img for discrimintator\n",
    "            real_data = gen_real_samples(X_txt, X_img, y, labels, img_size, half_batch)\n",
    "            # expand the training sample for the discriminator\n",
    "            # real_data = expand_samples(real_data, synth_batch)\n",
    "            # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "\n",
    "            # create false txt for txt2img for generator\n",
    "            fake_data = gen_fake_samples(gen_model, txt_shape, cat_shape[0], label_shape[0], half_batch)\n",
    "            # expand the training sample for the discriminator\n",
    "            # fake_data = expand_samples(fake_data, synth_batch)\n",
    "\n",
    "            # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "            # drift labels to confuse the model\n",
    "            real_data, fake_data = drift_labels(real_data, fake_data, half_batch, 0.05)\n",
    "    \n",
    "            # real data asignation\n",
    "            Xt_real = real_data[0]\n",
    "            Xi_real = real_data[1]\n",
    "            y_real = real_data[2]\n",
    "            yl_real = real_data[3]\n",
    "\n",
    "            # fake data asignation\n",
    "            Xt_fake = fake_data[0]\n",
    "            Xi_fake = fake_data[1]\n",
    "            y_fake = fake_data[2]\n",
    "            yl_fake = fake_data[3]\n",
    "\n",
    "            # train for real samples batch\n",
    "            dhr = dis_model.train_on_batch([Xi_real, yl_real], y_real)\n",
    "            # train for fake samples batch\n",
    "            dhf = dis_model.train_on_batch([Xi_fake, yl_fake], y_fake)\n",
    "\n",
    "            # prepare noisy text of latent space as input for the generator\n",
    "            Xt_gen = gen_latent_txt(txt_shape, batch_size)\n",
    "            # create inverted category for the fake noisy text\n",
    "            y_gen = get_fake_positive(cat_shape[0], batch_size)\n",
    "            # create inverted labels for the fake noisy text\n",
    "            yl_gen = get_fake_poslbls(label_shape[0], batch_size)\n",
    "            # update the generator via the discriminator's error\n",
    "            gh = gan_model.train_on_batch([Xt_gen, yl_gen], y_gen)\n",
    "            # print(\"ojo GAN!\", gh)\n",
    "\n",
    "            ep_disr_hist.append(dhf)\n",
    "            ep_disf_hist.append(dhr)\n",
    "            # ep_gen_hist.append(gh)\n",
    "            ep_gan_hist.append(gh)\n",
    "\n",
    "\t\t\t# print('>%d, %d/%d, dis_=%.3f, gen=%.3f' % (ep+1, batch+1, bat_per_epo, dis_history, gen_history))\n",
    "            log_msg = \">>> Epoch: %d, B/Ep: %d/%d, Batch S: %d\" % (ep+1, batch+1, batch_per_epoch, batch_size*synth_batch)\n",
    "            log_msg = \"%s -> [R-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhr[0], dhr[1])\n",
    "            log_msg = \"%s || [F-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhf[0], dhf[1])\n",
    "            log_msg = \"%s || [Gen loss: %.3f, acc: %.3f]\" % (log_msg, gh[0], gh[1])\n",
    "            # log_msg = \"%s || [Gen loss: %.3f]\" % (log_msg, gh) # [0], gh[1])\n",
    "            print(log_msg)\n",
    "\n",
    "        # record history for epoch\n",
    "        disr_hist.append(epoch_avg(ep_disr_hist))\n",
    "        disf_hist.append(epoch_avg(ep_disf_hist))\n",
    "        # gen_hist.append(epoch_avg(ep_gen_hist))\n",
    "        gan_hist.append(epoch_avg(ep_gan_hist))\n",
    "\n",
    "\t\t# evaluate the model performance sometimes\n",
    "        if (ep) % save_intervas == 0:\n",
    "            print(\"Epoch:\", ep+1, \"Saving the training progress...\")\n",
    "            test_model(ep, gen_model, dis_model, X_txt, X_img, y, labels, half_batch, report_fn_path, synth_batch)\n",
    "            plot_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, ep)\n",
    "            save_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, gan_model_name)\n",
    "\n",
    "\t\t# saving the model sometimes\n",
    "        if (ep) % int(save_intervas*5) == 0:\n",
    "            epoch_sufix = \"-epoch%d\" % int(ep)\n",
    "            # epoch_sufix = \"-last\"\n",
    "            epoch_sufix = str(epoch_sufix)\n",
    "            dis_mn = dis_model_name + epoch_sufix\n",
    "            gen_mn = gen_model_name + epoch_sufix\n",
    "            gan_mn = gan_model_name + epoch_sufix\n",
    "\n",
    "            dis_path = os.path.join(model_fn_path, \"Dis\")\n",
    "            gen_path = os.path.join(model_fn_path, \"Gen\")\n",
    "            gan_path = os.path.join(model_fn_path, \"GAN\")\n",
    "\n",
    "            save_model(dis_model, dis_path, dis_mn)\n",
    "            save_model(gen_model, gen_path, gen_mn)\n",
    "            save_model(gan_model, gan_path, gan_mn)\n",
    "        \n",
    "        train_time = lapse_time(train_time, ep)"
   ]
  },
  {
   "source": [
    "# EXEC SCRIPT\n",
    "\n",
    "## Dataset prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== working files ===\n\n std-VVG-Gallery-Text-Data-Small.csv \n std-VVG-Gallery-Img-Data-Small.csv \n Small \n rgb \n Validation-GAN-Text-Data-Small.csv\n"
     ]
    }
   ],
   "source": [
    "# variable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with predictions txt data\n",
    "imagef = \"Img\"\n",
    "\n",
    "# report subfolder\n",
    "reportf = \"Reports\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "trainf = \"Train\"\n",
    "testf = \"Test\"\n",
    "\n",
    "# subfolder for model IO\n",
    "modelf = \"Models\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "\n",
    "imgf = \"jpg\"\n",
    "\n",
    "rgb_sufix = \"rgb\"\n",
    "bw_sufix = \"bw\"\n",
    "\n",
    "# standard sufix\n",
    "stdprefix = \"std-\"\n",
    "\n",
    "# ml model useful data\n",
    "mltprefix = \"ml-\"\n",
    "\n",
    "# report names\n",
    "# timestamp = datetime.date.today().strftime(\"%d-%b-%Y\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "sample_sufix = \"Small\"\n",
    "# sample_sufix = \"Large\"\n",
    "# sample_sufix = \"Paintings\"\n",
    "imgf_sufix = \"Img-Data-\"\n",
    "text_sufix = \"Text-Data-\"\n",
    "\n",
    "# std-VVG-Gallery-Text-Data-Paintings\n",
    "gallery_prefix = \"VVG-Gallery-\"\n",
    "\n",
    "# dataframe file name\n",
    "text_fn = stdprefix + gallery_prefix + text_sufix + sample_sufix + \".\" + fext\n",
    "imgf_fn = stdprefix + gallery_prefix + imgf_sufix + sample_sufix + \".\" + fext\n",
    "valt_fn = \"Validation-GAN-\" + text_sufix + sample_sufix + \".\" + fext\n",
    "\n",
    "# model names\n",
    "dis_model_name = \"VVG-Text2Img-CDiscriminator\"\n",
    "gen_model_name = \"VVG-Text2Img-CGenerator\"\n",
    "gan_model_name = \"VVG-Text2Img-CGAN\"\n",
    "\n",
    "# to continue training after stoping script\n",
    "continue_training = True\n",
    "\n",
    "# ramdom seed\n",
    "randseed = 42\n",
    "\n",
    "# sample distribution train vs test sample size\n",
    "train_split = 0.80\n",
    "test_split = 1.0 - train_split\n",
    "\n",
    "# regex to know that column Im interested in\n",
    "keeper_regex = r\"(^ID$)|(^std_)\"\n",
    "\n",
    "imgt = rgb_sufix\n",
    "# imgt = bw_sufix\n",
    "\n",
    "# woring values for code\n",
    "work_txtf, work_imgf, work_sufix, work_imgt = text_fn, imgf_fn, sample_sufix, imgt\n",
    "\n",
    "print(\"=== working files ===\")\n",
    "print(\"\\n\", work_txtf, \"\\n\", work_imgf, \"\\n\", work_sufix, \"\\n\", work_imgt, \"\\n\", valt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\n"
     ]
    }
   ],
   "source": [
    "root_folder = os.getcwd()\n",
    "root_folder = os.path.split(root_folder)[0]\n",
    "root_folder = os.path.normpath(root_folder)\n",
    "print(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Text-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Img-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Test\\Validation-GAN-Text-Data-Small.csv False\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Models True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Reports True\n"
     ]
    }
   ],
   "source": [
    "# variable reading\n",
    "# dataframe filepath for texttual data\n",
    "text_fn_path = os.path.join(root_folder, dataf, trainf, work_txtf)\n",
    "print(text_fn_path, os.path.exists(text_fn_path))\n",
    "\n",
    "# dataframe filepath for img data\n",
    "img_fn_path = os.path.join(root_folder, dataf, trainf, work_imgf)\n",
    "print(img_fn_path, os.path.exists(img_fn_path))\n",
    "\n",
    "# dataframe filepath form GAN data\n",
    "val_fn_path = os.path.join(root_folder, dataf, testf, valt_fn)\n",
    "print(val_fn_path, os.path.exists(val_fn_path))\n",
    "\n",
    "# filepath for the models\n",
    "model_fn_path = os.path.join(root_folder, dataf, modelf)\n",
    "print(model_fn_path, os.path.exists(model_fn_path))\n",
    "\n",
    "# filepath for the reports\n",
    "report_fn_path = os.path.join(root_folder, dataf, reportf)\n",
    "print(report_fn_path, os.path.exists(report_fn_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rading training data\n",
    "# loading textual file\n",
    "text_df = pd.read_csv(\n",
    "                text_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "text_cols = text_df.columns.values\n",
    "\n",
    "# loading image file\n",
    "img_df = pd.read_csv(\n",
    "                img_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "img_cols = img_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ID', 'F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "idx_cols = list()\n",
    "\n",
    "for tcol in text_cols:\n",
    "    if tcol in img_cols:\n",
    "        idx_cols.append(tcol)\n",
    "print(idx_cols)\n",
    "\n",
    "source_df = pd.merge(text_df, img_df, how=\"inner\", on=idx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns (total 22 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   ID                     59 non-null     object\n 1   F-number               59 non-null     object\n 2   JH-number              59 non-null     object\n 3   creator-date           59 non-null     object\n 4   creator-place          59 non-null     object\n 5   Dimensions             59 non-null     object\n 6   details                59 non-null     object\n 7   MUS_TEXT               59 non-null     object\n 8   std_cat_creator-date   59 non-null     object\n 9   std_cat_creator-place  59 non-null     object\n 10  std_cat_Dimensions     59 non-null     object\n 11  std_cat_details        59 non-null     object\n 12  clr_tokens             59 non-null     object\n 13  lemmas                 59 non-null     object\n 14  bows_tokens            59 non-null     object\n 15  idxs_tokens            59 non-null     object\n 16  tfidf_tokens           59 non-null     object\n 17  std_dvec_tokens        59 non-null     object\n 18  rgb_img                59 non-null     object\n 19  bw_img                 59 non-null     object\n 20  rgb_shape              59 non-null     object\n 21  bw_shape               59 non-null     object\ndtypes: object(22)\nmemory usage: 10.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking everything is allrigth\n",
    "img_df = None\n",
    "text_df = None\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = source_df.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and loading images into df\n",
    "# working variables\n",
    "src_col = work_imgt + \"_img\"\n",
    "tgt_col = work_imgt + \"_img\" + \"_data\"\n",
    "work_shape = work_imgt + \"_shape\"\n",
    "scale = 50\n",
    "print(src_col, tgt_col)\n",
    "source_df = get_images(root_folder, source_df, src_col, tgt_col, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update image shape\n",
    "source_df = update_shape(source_df, tgt_col, work_shape)\n",
    "\n",
    "# data augmentation\n",
    "# source_df = augment_images(source_df, src_col, tgt_col, 6)\n",
    "# source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_shape\n(400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# searching the biggest shape in the image files\n",
    "print(work_shape)\n",
    "shape_data = source_df[work_shape]\n",
    "max_shape = get_mshape(shape_data, work_imgt)\n",
    "print(max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img_data pad_cnn_rgb_img_data rgb\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max shape of the images in gallery\n",
    "pad_prefix = \"pad_\"\n",
    "conv_prefix = \"cnn_\"\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = pad_prefix + conv_prefix + src_col\n",
    "\n",
    "print(src_col, tgt_col, work_imgt)\n",
    "source_df = padding_images(source_df, src_col, tgt_col, max_shape, work_imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing regular images...\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and stadarizing images into df\n",
    "# working variables\n",
    "print(\"standarizing regular images...\")\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "\n",
    "# source_df = standarize_images(source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing padded images...\npad_cnn_rgb_img_data std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "print(\"standarizing padded images...\")\n",
    "src_col = pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "print(src_col, tgt_col)\n",
    "\n",
    "# std_opt = \"std\"\n",
    "std_opt = \"ctr\"\n",
    "source_df = standarize_images(source_df, src_col, tgt_col, work_imgt, std_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, s0004V1962r to d1125S2005\nData columns (total 24 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   F-number                  59 non-null     object\n 1   JH-number                 59 non-null     object\n 2   creator-date              59 non-null     object\n 3   creator-place             59 non-null     object\n 4   Dimensions                59 non-null     object\n 5   details                   59 non-null     object\n 6   MUS_TEXT                  59 non-null     object\n 7   std_cat_creator-date      59 non-null     object\n 8   std_cat_creator-place     59 non-null     object\n 9   std_cat_Dimensions        59 non-null     object\n 10  std_cat_details           59 non-null     object\n 11  clr_tokens                59 non-null     object\n 12  lemmas                    59 non-null     object\n 13  bows_tokens               59 non-null     object\n 14  idxs_tokens               59 non-null     object\n 15  tfidf_tokens              59 non-null     object\n 16  std_dvec_tokens           59 non-null     object\n 17  rgb_img                   59 non-null     object\n 18  bw_img                    59 non-null     object\n 19  rgb_shape                 59 non-null     object\n 20  bw_shape                  59 non-null     object\n 21  rgb_img_data              59 non-null     object\n 22  pad_cnn_rgb_img_data      59 non-null     object\n 23  std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(24)\nmemory usage: 11.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# shuffle the DataFrame rows\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "474113"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "# cleaning memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find a name of column names according to a regex\n",
    "def get_keeper_cols(col_names, search_regex):\n",
    "    ans = [i for i in col_names if re.search(search_regex, i)]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the disperse columns in the df\n",
    "def get_disperse_categories(src_df, keep_cols, max_dis, check_cols, ignore_col):\n",
    "\n",
    "    ans = list()\n",
    "\n",
    "    max_dis = 2\n",
    "    tcount = 0\n",
    "\n",
    "    while tcount < max_dis:\n",
    "        for label_col in keep_columns:\n",
    "\n",
    "            if label_col != ignore_col:\n",
    "\n",
    "                label_count = src_df[label_col].value_counts(normalize=False)\n",
    "\n",
    "                if tcount < label_count.shape[0] and (check_cols in label_col):\n",
    "                    tcount = label_count.shape[0]\n",
    "                    ans.append(label_col)\n",
    "                # print(\"count values of\", label_col, \":=\", label_count.shape)#.__dict__)\n",
    "        tcount = tcount + 1\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the disperse columns from the interesting ones\n",
    "def remove_disperse_categories(keep_columns, too_disperse):\n",
    "    for too in too_disperse:\n",
    "        keep_columns.remove(too)\n",
    "    return keep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_corpus(train_df, dvector_col, pad_prefix):\n",
    "    # getting the corpus dense vectors\n",
    "    work_corpus = np.asarray(train_df[dvector_col], dtype=\"object\")\n",
    "\n",
    "    # converting list of list to array of array\n",
    "    print(\"Original txt shape\", work_corpus.shape)\n",
    "\n",
    "    # padding the representation\n",
    "    work_corpus = pad_sequences(work_corpus, dtype='object', padding=\"post\")\n",
    "    # print(\"Padded txt shape\", work_corpus.shape)\n",
    "\n",
    "    # creating the new column and saving padded data\n",
    "    padded_col_dvector = pad_prefix + dvector_col\n",
    "\n",
    "    # print(padded_col)\n",
    "    train_df[padded_col_dvector] = list(work_corpus)\n",
    "    print(\"Padded txt shape\", work_corpus.shape)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_categories(train_df, cat_cols, tgt_col):\n",
    "\n",
    "    labels_data = train_df[cat_cols]\n",
    "    labels_concat = list()\n",
    "\n",
    "    # concatenating all category labels from dataframe\n",
    "    for index, row in labels_data.iterrows():\n",
    "        row = concat_labels(row, labels_cols)\n",
    "        labels_concat.append(row)\n",
    "\n",
    "    # print(len(labels_concat[0]), type(labels_concat[0]))\n",
    "    # updating dataframe\n",
    "    tcat_label_col = \"std_cat_labels\"\n",
    "    train_df[tgt_col] = labels_concat\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to adjust the textual data for the LSTM layers in the model\n",
    "def format_corpus(corpus, timesteps, features):\n",
    "\n",
    "    # preparation for reshape lstm model\n",
    "    corpus = temporalize(corpus, timesteps)\n",
    "    print(corpus.shape)\n",
    "\n",
    "    corpus = corpus.reshape((corpus.shape[0], timesteps, features))\n",
    "    print(corpus.shape)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ original input/interested columns ------\n['F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'MUS_TEXT', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'clr_tokens', 'lemmas', 'bows_tokens', 'idxs_tokens', 'tfidf_tokens', 'std_dvec_tokens', 'rgb_img', 'bw_img', 'rgb_shape', 'bw_shape', 'rgb_img_data', 'pad_cnn_rgb_img_data', 'std_pad_cnn_rgb_img_data']\n\n\n------ Interesting columns ------\n['std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# selecting data to train\n",
    "# want to keep the columns starting with STD_\n",
    "keep_columns = list(source_df.columns)\n",
    "print(\"------ original input/interested columns ------\")\n",
    "print(keep_columns)\n",
    "\n",
    "# create the columns Im interesting in\n",
    "keep_columns = get_keeper_cols(keep_columns, keeper_regex)\n",
    "# keep_columns = [i for i in df_columns if re.search(keeper_regex, i)]\n",
    "\n",
    "print(\"\\n\\n------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-date', 'std_cat_Dimensions']\n"
     ]
    }
   ],
   "source": [
    "too_disperse = get_disperse_categories(source_df, keep_columns, 2, \"std_cat_\", \"std_pad_cnn_rgb_img_data\")\n",
    "print(too_disperse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ Interesting columns ------\n['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# creating the training dataframe\n",
    "keep_columns = remove_disperse_categories(keep_columns, too_disperse)\n",
    "# keep_columns.remove(\"ID\")\n",
    "print(\"------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training dataframe\n",
    "train_df = pd.DataFrame(source_df, columns=keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the stuff\n",
    "train_df = train_df.sample(frac = 1)\n",
    "source_df = None\n",
    "df_columns = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, d0158V1962 to d0031V1962v\nData columns (total 4 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(4)\nmemory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Padded image column in dataframe:  std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "pad_regex = u\"^std_pad_\"\n",
    "padimg_col = get_keeper_cols(df_columns, pad_regex)\n",
    "padimg_col = padimg_col[0]\n",
    "print(\"Padded image column in dataframe: \", str(padimg_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dense vector column in dataframe:  std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "dvec_regex = u\"^std_dvec\"\n",
    "dvector_col = get_keeper_cols(df_columns, dvec_regex)\n",
    "dvector_col = dvector_col[0]\n",
    "print(\"Dense vector column in dataframe: \", str(dvector_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix column data type\n",
    "work_corpus = train_df[dvector_col]\n",
    "work_corpus = format_dvector(work_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing type in dataframe\n",
    "train_df[dvector_col] = work_corpus\n",
    "work_corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original txt shape (59,)\nPadded txt shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max length of text corpus\n",
    "pad_prefix = \"pad_\"\n",
    "recurrent_prefix = \"lstm_\"\n",
    "\n",
    "train_df = padding_corpus(train_df, dvector_col, pad_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_img_col = \"std_\" + work_imgt + \"_img\" + \"_data\"\n",
    "padded_img_col = \"std_\" + pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "padded_col_dvector = pad_prefix + dvector_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n",
      "Classifier trainable labels in dataframe:  ['std_cat_creator-place', 'std_cat_details']\n",
      "categories heat column: std_cat_labels\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "cat_regex = u\"^std_cat_\"\n",
    "labels_cols = get_keeper_cols(keep_columns, cat_regex)\n",
    "print(\"Classifier trainable labels in dataframe: \", str(labels_cols))\n",
    "\n",
    "# updating dataframe with hot/concatenated categories\n",
    "tcat_label_col = \"std_cat_labels\"\n",
    "print(\"categories heat column:\", tcat_label_col)\n",
    "train_df = heat_categories(train_df, labels_cols, tcat_label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n",
      "Trainable labels columns in dataframe:  ['std_cat_creator-place', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "labels_cols = [i for i in keep_columns if re.search(u\"^std_cat_\", i)]\n",
    "print(\"Trainable labels columns in dataframe: \", str(labels_cols))\n",
    "\n",
    "labels_data = train_df[labels_cols]\n",
    "labels_concat = list()\n",
    "\n",
    "# concatenating all category labels from dataframe\n",
    "for index, row in labels_data.iterrows():\n",
    "    row = concat_labels(row, labels_cols)\n",
    "    labels_concat.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pad_std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "text_lstm_col = padded_col_dvector\n",
    "print(text_lstm_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "working_img_col = padded_img_col\n",
    "# working_img_col = regular_img_col\n",
    "print(working_img_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, d0158V1962 to d0031V1962v\nData columns (total 6 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\n 4   pad_std_dvec_tokens       59 non-null     object\n 5   std_cat_labels            59 non-null     object\ndtypes: object(6)\nmemory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 (400, 400, 3)\n",
      "final X_img shape (59, 400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "og_shape = train_df[working_img_col][0].shape# y[0].shape\n",
    "X_img_len = train_df[working_img_col].shape[0] #y.shape[0]\n",
    "print(X_img_len, og_shape)\n",
    "\n",
    "X_img = None\n",
    "\n",
    "for img in train_df[working_img_col]:\n",
    "\n",
    "    if X_img is None:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = img\n",
    "    else:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = np.concatenate((X_img, img), axis=0)\n",
    "\n",
    "print(\"final X_img shape\", X_img.shape)\n",
    "# y.shape = (1899, 800, 800, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_img[0]))\n",
    "print(type(X_img[0][0]))\n",
    "print(X_img[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_img.shape) == 3:\n",
    "    X_img = X_img[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 16)\n"
     ]
    }
   ],
   "source": [
    "# y = train_df[working_img_col]\n",
    "# y = np.expand_dims(y, axis=0)\n",
    "y_labels = np.asarray([np.asarray(j, dtype=\"object\") for j in train_df[tcat_label_col]], dtype=\"object\")\n",
    "print(\"y shape\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.ones((y_labels.shape[0],1)).astype(\"float32\")\n",
    "print(\"y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y classification category\n<class 'numpy.ndarray'>\n<class 'numpy.float32'>\n(1,)\ny labels category\n<class 'numpy.ndarray'>\n<class 'float'>\n(16,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y classification category\")\n",
    "print(type(y[0]))\n",
    "print(type(y[0][0]))\n",
    "print(y[1].shape)\n",
    "\n",
    "print(\"y labels category\")\n",
    "print(type(y_labels[0]))\n",
    "print(type(y_labels[0][0]))\n",
    "print(y_labels[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "final X_lstm shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "X_txt = np.asarray([np.asarray(i, dtype=\"object\") for i in train_df[text_lstm_col]], dtype=\"object\")\n",
    "# X = np.array(train_df[text_lstm_col]).astype(\"object\")\n",
    "# X = train_df[text_lstm_col]\n",
    "print(\"final X_lstm shape\", X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'float'>\n(142,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_txt[0]))\n",
    "print(type(X_txt[0][0]))\n",
    "print(X_txt[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 142\n"
     ]
    }
   ],
   "source": [
    "# timestep is the memory of what i read, this is the longest sentence I can remember in the short term\n",
    "# neet to look for the best option, in small the max is 15\n",
    "timesteps = 15\n",
    "\n",
    "# features is the max length in the corpus, after padding!!!!\n",
    "features = X_txt[0].shape[0]\n",
    "print(timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 1, 142)\n(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "X_txt = format_corpus(X_txt, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "diff_txt = y.shape[0] - X_txt.shape[0]\n",
    "print(diff_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "Xa = X_txt[-diff_txt:]\n",
    "X_txt = np.append(X_txt, Xa, axis=0)\n",
    "print(X_txt.shape)\n",
    "Xa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n(59, 400, 400, 3)\n(59, 1)\n(59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)\n",
    "print(X_img.shape)\n",
    "print(y.shape)\n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n",
      "(400, 400, 3)\n",
      "(1,)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt[0].shape)\n",
    "print(X_img[0].shape)\n",
    "print(y[0].shape)\n",
    "print(y_labels[0].shape)\n",
    "txt_og_shape = X_txt[0].shape\n",
    "img_og_shape = X_img[0].shape\n",
    "cat_og_shape = y[0].shape\n",
    "lab_og_shape = y_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xt = X_txt # np.array(X).astype(\"object\")\n",
    "# Xi = X_img\n",
    "# yt = y # np.array(y).astype(\"object\")\n",
    "# # ya = y[0:timesteps]\n",
    "# train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "source": [
    "## ML Model Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras model definition for the GAN conditional discriminator\n",
    "def create_cdiscriminator(img_shape, txt_shape, n_labels, layer_cfg):\n",
    "\n",
    "    # model layer configuration!!!! get from layer_cfg!!!\n",
    "    dis_ldrop = layer_cfg[0]\n",
    "    dis_filters = layer_cfg[1]\n",
    "    ksize = layer_cfg[2]\n",
    "    stsize = layer_cfg[3]\n",
    "    in_dis_actf = layer_cfg[4]\n",
    "    hid_ly_actf = layer_cfg[5]\n",
    "    psize = layer_cfg[6]\n",
    "    pad = layer_cfg[7]\n",
    "    dis_midn = layer_cfg[8]\n",
    "    out_dis = layer_cfg[9]\n",
    "    out_dis_act = layer_cfg[10]\n",
    "    gen_reshape = layer_cfg[11]\n",
    "\n",
    "    # scalar to fit the image dimensions in a dense layer\n",
    "    img_label_neurons = img_shape[0]*img_shape[1]*img_shape[2]\n",
    "\n",
    "    # LABEL IMG LAYERS\n",
    "    # label inpuy\n",
    "    in_labels = Input(shape=(n_labels,), \n",
    "                        dtype=\"float32\", \n",
    "                        name=\"ImgLabelsIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    embed1 = Embedding(txt_shape[0], txt_shape[1], \n",
    "                        input_length=n_labels, \n",
    "                        name=\"ImgLblEmb_1\")(in_labels)\n",
    "    # flat layer\n",
    "    label1 = Flatten(name=\"ImgLblFlat\")(embed1)\n",
    "    label2 = Dense(dis_midn, activation=hid_ly_actf, \n",
    "                    name=\"ImgLblDense_1\")(label1)\n",
    "    # reshape layer\n",
    "    label3 = Reshape(gen_reshape, name=\"ImgLblReshape\")(label2)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    label4 = Conv2D(int(dis_filters/8), kernel_size=ksize, \n",
    "                        strides=stsize, activation=hid_ly_actf, \n",
    "                        padding=pad, name=\"ImglblConv2D_1\")(label3)\n",
    "    label5 = UpSampling2D(psize, name=\"ImgLblUpsam_1\")(label4)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch5 = BatchNormalization(name=\"ImgBaNorm_1\")(label5)\n",
    "    drop5 = Dropout(dis_ldrop, name=\"ImgDrop_1\")(batch5)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    label6 = Conv2D(int(dis_filters/4), kernel_size=ksize, \n",
    "                        strides=stsize, activation=hid_ly_actf, \n",
    "                        padding=pad, name=\"ImglblConv2D_2\")(drop5)\n",
    "    label7 = UpSampling2D(psize, name=\"ImgLblUpsam_2\")(label6)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    label8 = Conv2D(int(dis_filters/2), kernel_size=ksize, \n",
    "                        strides=stsize, activation=hid_ly_actf, \n",
    "                        padding=pad, name=\"ImglblConv2D_3\")(label7)\n",
    "    label9 = UpSampling2D(psize, name=\"ImgLblUpsam_3\")(label8)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch6 = BatchNormalization(name=\"ImgBaNorm_2\")(label9)\n",
    "    drop6 = Dropout(dis_ldrop, name=\"ImgDrop_2\")(batch6)\n",
    "\n",
    "    label10 = Conv2D(img_shape[2], kernel_size=ksize, \n",
    "                        strides=stsize, activation=hid_ly_actf, \n",
    "                        padding=pad, name=\"ImglblOut\")(drop6)\n",
    "\n",
    "    # MERGING DISCRIMINATOR + LABEL MODEL\n",
    "    # input layer\n",
    "    in_layer = Input(shape=(img_shape[0], img_shape[1], img_shape[2]), name=\"DisIn\")\n",
    "\n",
    "    concat1 = Concatenate(axis=-1, name=\"DisConcat_1\")([in_layer, label10])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer\n",
    "    conv1 = Conv2D(int(dis_filters), kernel_size=ksize, \n",
    "                    padding=pad, activation=in_dis_actf, \n",
    "                    name=\"DisConv2D_1\")(concat1)\n",
    "    pool1 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_1\")(conv1)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch1 = BatchNormalization(name=\"DisBaNorm_1\")(pool1)\n",
    "    drop1 = Dropout(dis_ldrop, name=\"DisDrop_1\")(batch1)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv2 = Conv2D(int(dis_filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    name=\"DisConv2D_2\")(drop1)\n",
    "    pool2 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_2\")(conv2)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv3 = Conv2D(int(dis_filters/4), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    name=\"DisConv2D_3\")(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_3\")(conv3)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv4 = Conv2D(int(dis_filters/8), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    name=\"DisConv2D_4\")(pool3)\n",
    "    pool4 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_4\")(conv4)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch2 = BatchNormalization(name=\"DisBaNorm_2\")(pool4)\n",
    "    drop2 = Dropout(dis_ldrop, name=\"DisDrop_2\")(batch2)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    flat = Flatten(name=\"DisMidFlat\")(pool4)\n",
    "\n",
    "    # dense classifier layers\n",
    "    hidden1 = Dense(int(dis_midn), activation=hid_ly_actf, name=\"DisDense_1\")(flat)\n",
    "    hidden2 = Dense(int(dis_midn/2), activation=hid_ly_actf, name=\"DisDense_2\")(hidden1)\n",
    "    # drop layer\n",
    "    drop3 = Dropout(dis_ldrop, name=\"DisDrop_3\")(hidden2)\n",
    "\n",
    "    # dense classifier layers\n",
    "    hidden3 = Dense(int(dis_midn/4), activation=hid_ly_actf, name=\"DisDense_3\")(drop3)\n",
    "    hidden4 = Dense(int(dis_midn/8), activation=hid_ly_actf, name=\"DisDense_4\")(hidden3)\n",
    "    # drop layer\n",
    "    drop4 = Dropout(dis_ldrop, name=\"DisDrop_4\")(hidden4)\n",
    "\n",
    "    # output layer\n",
    "    out_layer = Dense(out_dis, activation=out_dis_act, name=\"DisOut\")(drop4)\n",
    "\n",
    "    # model definition\n",
    "    dis_model = Model(inputs=[in_layer, in_labels], outputs=out_layer)\n",
    "\n",
    "    return dis_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cdiscriminator_v2(img_shape, txt_shape, n_labels, layer_cfg):\n",
    "\n",
    "    # model layer configuration!!!! get from layer_cfg!!!\n",
    "    dis_ldrop = layer_cfg[0]\n",
    "    dis_filters = layer_cfg[1]\n",
    "    ksize = layer_cfg[2]\n",
    "    stsize = layer_cfg[3]\n",
    "    in_dis_actf = layer_cfg[4]\n",
    "    hid_ly_actf = layer_cfg[5]\n",
    "    psize = layer_cfg[6]\n",
    "    pad = layer_cfg[7]\n",
    "    dis_midn = layer_cfg[8]\n",
    "    out_dis = layer_cfg[9]\n",
    "    out_dis_act = layer_cfg[10]\n",
    "    gen_reshape = layer_cfg[11]\n",
    "\n",
    "    # scalar to fit the image dimensions in a dense layer\n",
    "    img_label_neurons = img_shape[0]*img_shape[1]*img_shape[2]\n",
    "\n",
    "    # LABEL IMG LAYERS\n",
    "    # label inpuy\n",
    "    in_labels = Input(shape=(n_labels,), \n",
    "                        dtype=\"float32\", \n",
    "                        name=\"ImgLabelsIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    embed1 = Embedding(txt_shape[0], txt_shape[1], \n",
    "                        input_length=n_labels, \n",
    "                        name=\"ImgLblEmb_1\")(in_labels)\n",
    "    # flat layer\n",
    "    label1 = Flatten(name=\"ImgLblFlat\")(embed1)\n",
    "    label2 = Dense(dis_midn, activation=hid_ly_actf, \n",
    "                    name=\"ImgLblDense_1\")(label1)\n",
    "    # reshape layer\n",
    "    label3 = Reshape(gen_reshape, name=\"ImgLblReshape\")(label2)\n",
    "\n",
    "    # transpose conv2D layers\n",
    "    label4 = Conv2DTranspose(int(dis_filters/8), kernel_size=ksize, \n",
    "                                strides=stsize, activation=hid_ly_actf, \n",
    "                                padding=pad, name=\"ImglblConv2D_1\")(label3)\n",
    "    # label5 = UpSampling2D(psize, name=\"ImgLblUpsam_1\")(label4)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch5 = BatchNormalization(name=\"ImgBaNorm_1\")(label4)\n",
    "    drop5 = Dropout(dis_ldrop, name=\"ImgDrop_1\")(batch5)\n",
    "\n",
    "    # trnaspose conv2D layers\n",
    "    label6 = Conv2DTranspose(int(dis_filters/4), kernel_size=ksize, \n",
    "                                strides=stsize, activation=\"tanh\", \n",
    "                                padding=pad, name=\"ImglblConv2D_2\")(drop5)\n",
    "    # label7 = UpSampling2D(psize, name=\"ImgLblUpsam_2\")(label6)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch6 = BatchNormalization(name=\"ImgBaNorm_2\")(label6)\n",
    "    drop6 = Dropout(dis_ldrop, name=\"ImgDrop_2\")(batch6)\n",
    "\n",
    "    # conditional layer output\n",
    "    label10 = Conv2DTranspose(img_shape[2], kernel_size=ksize, \n",
    "                                strides=stsize, activation=hid_ly_actf, \n",
    "                                padding=pad, name=\"ImglblOut\")(drop6)\n",
    "\n",
    "    # MERGING DISCRIMINATOR + LABEL MODEL\n",
    "    # input layer\n",
    "    in_layer = Input(shape=(img_shape[0], img_shape[1], img_shape[2]), name=\"DisIn\")\n",
    "\n",
    "    concat1 = Concatenate(axis=-1, name=\"DisConcat_1\")([in_layer, label10])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer\n",
    "    conv1 = Conv2D(int(dis_filters), kernel_size=ksize, \n",
    "                    padding=pad, activation=in_dis_actf, \n",
    "                    strides=stsize, name=\"DisConv2D_1\")(concat1)\n",
    "    # pool1 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_1\")(conv1)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch1 = BatchNormalization(name=\"DisBaNorm_1\")(conv1)\n",
    "    drop1 = Dropout(dis_ldrop, name=\"DisDrop_1\")(batch1)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv2 = Conv2D(int(dis_filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    strides=stsize, name=\"DisConv2D_2\")(drop1)\n",
    "    # pool2 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_2\")(conv2)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv3 = Conv2D(int(dis_filters/4), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    strides=stsize, name=\"DisConv2D_3\")(conv2)\n",
    "    # pool3 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_3\")(conv3)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    conv4 = Conv2D(int(dis_filters/8), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_ly_actf, \n",
    "                    strides=stsize, name=\"DisConv2D_4\")(conv3)\n",
    "    # pool4 = MaxPooling2D(pool_size=psize, name=\"DisMax2D_4\")(conv4)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch2 = BatchNormalization(name=\"DisBaNorm_2\")(conv4)\n",
    "    drop2 = Dropout(dis_ldrop, name=\"DisDrop_2\")(batch2)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    flat = Flatten(name=\"DisMidFlat\")(drop2)\n",
    "\n",
    "    # dense classifier layers\n",
    "    hidden1 = Dense(int(dis_midn), activation=hid_ly_actf, name=\"DisDense_1\")(flat)\n",
    "    hidden2 = Dense(int(dis_midn/2), activation=hid_ly_actf, name=\"DisDense_2\")(hidden1)\n",
    "    # drop layer\n",
    "    drop3 = Dropout(dis_ldrop, name=\"DisDrop_3\")(hidden2)\n",
    "\n",
    "    # dense classifier layers\n",
    "    hidden3 = Dense(int(dis_midn/4), activation=hid_ly_actf, name=\"DisDense_3\")(drop3)\n",
    "    hidden4 = Dense(int(dis_midn/8), activation=hid_ly_actf, name=\"DisDense_4\")(hidden3)\n",
    "    # drop layer\n",
    "    drop4 = Dropout(dis_ldrop, name=\"DisDrop_4\")(hidden4)\n",
    "\n",
    "    # dense classifier layers\n",
    "    hidden5 = Dense(int(dis_midn/16), activation=hid_ly_actf, name=\"DisDense_5\")(drop4)\n",
    "    hidden6 = Dense(int(dis_midn/32), activation=hid_ly_actf, name=\"DisDense_6\")(hidden5)\n",
    "\n",
    "    # output layer\n",
    "    out_layer = Dense(out_dis, activation=out_dis_act, name=\"DisOut\")(hidden6)\n",
    "\n",
    "    # model definition\n",
    "    dis_model = Model(inputs=[in_layer, in_labels], outputs=out_layer)\n",
    "\n",
    "    return dis_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cgenerator(img_shape, txt_shape, n_labels, layer_cfg):\n",
    "\n",
    "    # features for text\n",
    "    memory = txt_shape[0]\n",
    "    features = txt_shape[1]\n",
    "\n",
    "    # features for layers\n",
    "    lstm_units = layer_cfg[0]\n",
    "    gen_ldrop = layer_cfg[1]\n",
    "    gen_filters = layer_cfg[2]\n",
    "    ksize = layer_cfg[3]\n",
    "    stsize = layer_cfg[4]\n",
    "    in_gen_actf = layer_cfg[5]\n",
    "    hid_ly_actf = layer_cfg[6]\n",
    "    psize = layer_cfg[7]\n",
    "    pad = layer_cfg[8]\n",
    "    gen_midn = layer_cfg[9]\n",
    "    out_gen_act = layer_cfg[10]\n",
    "    rs = layer_cfg[11]\n",
    "    gen_reshape = layer_cfg[12]\n",
    "\n",
    "    # TXT LABEL LAYERS\n",
    "    # label inpuy\n",
    "    in_labels = Input(shape=(n_labels,), dtype=\"float32\", name=\"TxtLabelsIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    embed1 = Embedding(txt_shape[0], txt_shape[1], input_length=n_labels, name=\"TxtLblEmb_1\")(in_labels)\n",
    "    # flat layer\n",
    "    label1 = Flatten(name=\"TxtLblFlat\")(embed1)\n",
    "    # dense layers\n",
    "    label2 = Dense(dis_midn, activation=hid_ly_actf, name=\"LblDense_1\")(label1)\n",
    "    # std and drop layer\n",
    "    batch5 = BatchNormalization(name=\"TxtBaNorm_1\")(label2)\n",
    "    drop5 = Dropout(dis_ldrop, name=\"TxtDrop_1\")(batch5)\n",
    "\n",
    "    label3 = Dense(int(txt_shape[0]*txt_shape[1]), activation=\"sigmoid\", name=\"TxtLblDense_2\")(drop5)\n",
    "    # reshape layer\n",
    "    label4 = Reshape(txt_shape, name=\"TxtLblOut\")(label3)\n",
    "\n",
    "\n",
    "    # GENERATOR LAYERS\n",
    "    # input layer\n",
    "    in_layer = Input(shape=(txt_shape[0], txt_shape[1]), name=\"GenIn\")\n",
    "\n",
    "    # concat layer\n",
    "    concat1 = Concatenate(axis=-1, name=\"GenConcat_1\")([in_layer, label4])\n",
    "\n",
    "    # masking input text\n",
    "    mask = Masking(mask_value=0.0, input_shape=(memory, features), \n",
    "                    name = \"GenMaskIn\")(concat1) # concat1\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lstm1 = LSTM(lstm_units, activation=in_gen_actf, \n",
    "                    input_shape=(memory, features), \n",
    "                    return_sequences=rs, name=\"GenLSTM_1\")(mask)\n",
    "    for img\n",
    "    # std and drop layer\n",
    "    batch1 = BatchNormalization(name=\"GenBaNorm_1\")(lstm1)\n",
    "    drop1 = Dropout(dis_ldrop, name=\"GenDrop_1\")(batch1)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lstm2 = LSTM(int(lstm_units/2), activation=in_gen_actf, \n",
    "                    input_shape=(memory, features), \n",
    "                    return_sequences=rs, name=\"GenLSTM_2\")(drop1)\n",
    "    # intermediate recurrent layer\n",
    "    lstm3 = LSTM(int(lstm_units/4), activation=in_gen_actf, \n",
    "                    input_shape=(memory, features), \n",
    "                    return_sequences=rs, name=\"GenLSTM_3\")(lstm2)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch2 = BatchNormalization(name=\"GenBaNorm_2\")(lstm3)\n",
    "    drop2 = Dropout(dis_ldrop, name=\"GenDrop_2\")(batch2)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    flat = Flatten(name=\"GenMidFlat\")(drop2)\n",
    "    # dense classifier layers\n",
    "    mid = Dense(int(dis_midn), activation=hid_ly_actf, \n",
    "                name=\"GenGenDense_1\")(flat)\n",
    "    # reshape layer\n",
    "    gen1 = Reshape(gen_reshape, name=\"GenReshape\")(mid)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    gen2 = Conv2D(int(gen_filters/8), kernel_size=ksize, \n",
    "                    strides=stsize, activation=hid_ly_actf, \n",
    "                    padding=pad, name=\"GenConv2D_1\")(gen1)\n",
    "    gen3 = UpSampling2D(psize, name=\"GenUpsam_1\")(gen2)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch3 = BatchNormalization(name=\"GenBaNorm_3\")(gen3)\n",
    "    drop3 = Dropout(gen_ldrop, name=\"GenDrop_3\")(batch3)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    gen4 = Conv2D(int(gen_filters/4), kernel_size=ksize, \n",
    "                    strides=stsize, activation=hid_ly_actf, \n",
    "                    padding=pad, name=\"GenConv2D_2\")(drop3)\n",
    "    gen5 = UpSampling2D(psize, name=\"GenUpsam_2\")(gen4)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    gen6 = Conv2D(int(gen_filters/2), kernel_size=ksize, \n",
    "                    strides=stsize, activation=hid_ly_actf, \n",
    "                    padding=pad, name=\"GenConv2D_3\")(gen5)\n",
    "    gen7 = UpSampling2D(psize, name=\"GenUpsam_3\")(gen6)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch4 = BatchNormalization(name=\"GenBaNorm_4\")(gen7)\n",
    "    drop4 = Dropout(gen_ldrop, name=\"GenDrop_4\")(batch4)\n",
    "\n",
    "    # output layer\n",
    "    out_layer = Conv2D(img_shape[2], kernel_size=ksize, \n",
    "                        strides=stsize, activation=hid_ly_actf, \n",
    "                        padding=pad, input_shape=img_shape, \n",
    "                        name=\"GenOut\")(drop4)\n",
    "\n",
    "    # model definition\n",
    "    gen_model = Model(inputs=[in_layer, in_labels], outputs=out_layer)\n",
    "    # gen_model = Model(inputs=in_layer, outputs=out_layer)\n",
    "\n",
    "    return gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cgenerator_v2(img_shape, txt_shape, n_labels, layer_cfg):\n",
    "\n",
    "    # features for text\n",
    "    memory = txt_shape[0]\n",
    "    features = txt_shape[1]\n",
    "\n",
    "    # features for layers\n",
    "    lstm_units = layer_cfg[0]\n",
    "    gen_ldrop = layer_cfg[1]\n",
    "    gen_filters = layer_cfg[2]\n",
    "    ksize = layer_cfg[3]\n",
    "    stsize = layer_cfg[4]\n",
    "    in_gen_actf = layer_cfg[5]\n",
    "    hid_ly_actf = layer_cfg[6]\n",
    "    psize = layer_cfg[7]\n",
    "    pad = layer_cfg[8]\n",
    "    gen_midn = layer_cfg[9]\n",
    "    out_gen_act = layer_cfg[10]\n",
    "    rs = layer_cfg[11]\n",
    "    gen_reshape = layer_cfg[12]\n",
    "\n",
    "    # TXT LABEL LAYERS\n",
    "    # label inpuy\n",
    "    in_labels = Input(shape=(n_labels,), dtype=\"float32\", name=\"TxtLabelsIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    embed1 = Embedding(txt_shape[0], txt_shape[1], input_length=n_labels, name=\"TxtLblEmb_1\")(in_labels)\n",
    "    # flat layer\n",
    "    label1 = Flatten(name=\"TxtLblFlat\")(embed1)\n",
    "    # dense layers\n",
    "    label2 = Dense(dis_midn, activation=hid_ly_actf, name=\"LblDense_1\")(label1)\n",
    "    # std and drop layer\n",
    "    batch5 = BatchNormalization(name=\"TxtBaNorm_1\")(label2)\n",
    "    drop5 = Dropout(dis_ldrop, name=\"TxtDrop_1\")(batch5)\n",
    "\n",
    "    label3 = Dense(int(txt_shape[0]*txt_shape[1]), activation=hid_ly_actf, name=\"TxtLblDense_2\")(drop5)\n",
    "    # reshape layer\n",
    "    label4 = Reshape(txt_shape, name=\"TxtLblOut\")(label3)\n",
    "\n",
    "    # GENERATOR LAYERS\n",
    "    # input layer\n",
    "    in_layer = Input(shape=(txt_shape[0], txt_shape[1]), name=\"GenIn\")\n",
    "\n",
    "    # concat layer\n",
    "    concat1 = Concatenate(axis=-1, name=\"GenConcat_1\")([in_layer, label4])\n",
    "\n",
    "    # masking input text\n",
    "    mask = Masking(mask_value=0.0, input_shape=(memory, features), \n",
    "                    name = \"GenMaskIn\")(concat1) # concat1\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lstm1 = LSTM(lstm_units, activation=in_gen_actf, \n",
    "                    input_shape=(memory, features), \n",
    "                    return_sequences=rs, name=\"GenLSTM_1\")(mask)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch1 = BatchNormalization(name=\"GenBaNorm_1\")(lstm1)\n",
    "    drop1 = Dropout(dis_ldrop, name=\"GenDrop_1\")(batch1)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lstm2 = LSTM(int(lstm_units/2), activation=in_gen_actf, \n",
    "                    input_shape=(memory, features), \n",
    "                    return_sequences=rs, name=\"GenLSTM_2\")(drop1)\n",
    "    # intermediate recurrent layer\n",
    "    # lstm3 = LSTM(int(lstm_units/4), activation=in_gen_actf, \n",
    "    #                 input_shape=(memory, features), \n",
    "    #                 return_sequences=rs, name=\"GenLSTM_3\")(lstm2)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch2 = BatchNormalization(name=\"GenBaNorm_2\")(lstm2)\n",
    "    drop2 = Dropout(dis_ldrop, name=\"GenDrop_2\")(batch2)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    flat = Flatten(name=\"GenMidFlat\")(drop2)\n",
    "    # dense classifier layers\n",
    "    mid = Dense(int(dis_midn), activation=hid_ly_actf, \n",
    "                name=\"GenGenDense_1\")(flat)\n",
    "    # reshape layer\n",
    "    gen1 = Reshape(gen_reshape, name=\"GenReshape\")(mid)\n",
    "\n",
    "    # transpose conv2D layers\n",
    "    gen2 = Conv2DTranspose(int(gen_filters/8), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_ly_actf, \n",
    "                            padding=pad, name=\"GenConv2D_1\")(gen1)\n",
    "    # gen3 = UpSampling2D(psize, name=\"GenUpsam_1\")(gen2)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch3 = BatchNormalization(name=\"GenBaNorm_3\")(gen2)\n",
    "    drop3 = Dropout(gen_ldrop, name=\"GenDrop_3\")(batch3)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    gen4 = Conv2DTranspose(int(gen_filters/4), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_ly_actf, \n",
    "                            padding=pad, name=\"GenConv2D_2\")(drop3)\n",
    "    # gen5 = UpSampling2D(psize, name=\"GenUpsam_2\")(gen4)\n",
    "\n",
    "    # upsampling conv2D layers\n",
    "    gen6 = Conv2DTranspose(int(gen_filters/2), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_ly_actf, \n",
    "                            padding=pad, name=\"GenConv2D_3\")(gen4)\n",
    "    # gen7 = UpSampling2D(psize, name=\"GenUpsam_3\")(gen6)\n",
    "\n",
    "    # std and drop layer\n",
    "    batch4 = BatchNormalization(name=\"GenBaNorm_4\")(gen6)\n",
    "    drop4 = Dropout(gen_ldrop, name=\"GenDrop_4\")(batch4)\n",
    "\n",
    "    # output layer\n",
    "    out_layer = Conv2D(img_shape[2], kernel_size=(3,3), \n",
    "                        strides=(1,1), activation=hid_ly_actf, \n",
    "                        padding=pad, input_shape=img_shape, \n",
    "                        name=\"GenOut\")(drop4)\n",
    "\n",
    "    # model definition\n",
    "    gen_model = Model(inputs=[in_layer, in_labels], outputs=out_layer)\n",
    "    # gen_model = Model(inputs=in_layer, outputs=out_layer)\n",
    "\n",
    "    return gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generator LSMT neurons: 500\nGenerator LSTM memory span: 15\nGenerator LSTM learning features: 142\nDiscriminator & Generator learning batch size: 32\nGenerator CNN filter number: 128\nDiscriminator CNN filter number: 64\nDiscriminator Input Img shape: (400, 400, 3)\nGenerator Output Img shape: (400, 400, 3)\nGenewrator Input Text shape: (15, 142)\nDiscriminator & Generator CNN kernel size: (3, 3)\nDiscriminator & Generator CNN pad size: (2, 2)\nGenerator Dense middle neurons: 7500\nDiscriminator Output prediction labels: 16\nDiscriminator Output prediction Classification: 1\nDiscriminator Dense middle neurons: 7500\ntraining epochs: 500\n"
     ]
    }
   ],
   "source": [
    "# number of neurons or processing units in LSTM(())\n",
    "# the number is because of good practices for NLP\n",
    "# min 200 max 500, normaly 300 (related to the semantic number of themes)\n",
    "# 120 for now in this test\n",
    "lstm_units = 500\n",
    "print(\"Generator LSMT neurons:\", lstm_units)\n",
    "\n",
    "# timestep is 1 because you read a word at a time\n",
    "memory = timesteps\n",
    "print(\"Generator LSTM memory span:\", memory)\n",
    "# configuration to remember previous recurrent layer\n",
    "rs = True\n",
    "\n",
    "# features is the max length in the corpus, after padding!!!!\n",
    "# print(X_train.shape)\n",
    "features = X_txt.shape[2]\n",
    "print(\"Generator LSTM learning features:\", features)\n",
    "\n",
    "# batch size\n",
    "bs = 32\n",
    "print(\"Discriminator & Generator learning batch size:\", bs)\n",
    "\n",
    "# number of filters or processing units in CNN\n",
    "# the number is because of good practices from computer vision\n",
    "# min 8 max 64, normaly 32 (related to the size of the images)\n",
    "# 16 for now in this test\n",
    "\n",
    "# imgage filters\n",
    "# filters = 16\n",
    "# filters = 32\n",
    "# filters = 64\n",
    "# filters = 128\n",
    "gen_filters = 128\n",
    "dis_filters = 64\n",
    "\n",
    "print(\"Generator CNN filter number:\", gen_filters)\n",
    "print(\"Discriminator CNN filter number:\", dis_filters)\n",
    "\n",
    "disin_shape = X_img[0].shape\n",
    "genout_shape = X_img[0].shape\n",
    "genin_shape = X_txt[0].shape\n",
    "# in_shape = (None, None, 1)\n",
    "# in_shape = (794, 794, 3)\n",
    "print(\"Discriminator Input Img shape:\", disin_shape)\n",
    "print(\"Generator Output Img shape:\", genout_shape)\n",
    "print(\"Genewrator Input Text shape:\", X_txt[0].shape)\n",
    "\n",
    "ksize = (3,3) \n",
    "ksize_s = (4,4)\n",
    "stsize = (1,1)\n",
    "stsize_s = (2,2)\n",
    "# psize = (5,5)\n",
    "psize = (2,2)\n",
    "\n",
    "print(\"Discriminator & Generator CNN kernel size:\", ksize)\n",
    "print(\"Discriminator & Generator CNN pad size:\", psize)\n",
    "\n",
    "# neurons/processing units size in the dense layer (THIS SHOULD BE SOM!!!!)\n",
    "gen_midn =  50*50*3 # 100*100*3 # \n",
    "gen_reshape = (50,50,3) # (100,100,3) # \n",
    "print(\"Generator Dense middle neurons:\", gen_midn)\n",
    "# dn2 = len(XB_set[0])*SECURITY_FACTOR\n",
    "\n",
    "# numero de neuronas de salida\n",
    "# out_shape = X_train[0].shape\n",
    "# out_shape = (None, None, 3)\n",
    "# out_shape = in_shape\n",
    "# out_dis = y[0].shape[0]\n",
    "n_labels = y_labels[0].shape[0]\n",
    "out_dis = y[0].shape[0]\n",
    "print(\"Discriminator Output prediction labels:\", n_labels)\n",
    "print(\"Discriminator Output prediction Classification:\", out_dis)\n",
    "\n",
    "channels = img_og_shape[2]\n",
    "# channels = 8\n",
    "# dis_midn = filters*out_dis*channels*15*5\n",
    "dis_midn = 50*50*3\n",
    "print(\"Discriminator Dense middle neurons:\", dis_midn)\n",
    "\n",
    "# axtivation functions\n",
    "in_dis_actf = LeakyReLU(alpha=0.2) # \"relu\"\n",
    "in_gen_actf = LeakyReLU(alpha=0.2) # \"relu\"\n",
    "hid_ly_actf = LeakyReLU(alpha=0.2) # \"relu\",\n",
    "# in_dis_actf = 0.2 # \"relu\"\n",
    "# in_gen_actf = 0.2 # \"relu\"\n",
    "# hid_ly_actf = 0.2 # \"relu\",\n",
    "\n",
    "out_dis_act = \"sigmoid\" # \"softmax\" #\n",
    "out_gen_act = \"tanh\" # \"softmax\" #\n",
    "\n",
    "# loss percentage\n",
    "dis_ldrop = 0.4\n",
    "gen_ldrop = 0.2\n",
    "\n",
    "# padding policy\n",
    "pad = \"same\"\n",
    "\n",
    "# random seed\n",
    "randseed = 42\n",
    "\n",
    "# parameters to compile model\n",
    "# loss function\n",
    "# ls = \"mean_squared_error\"\n",
    "# ls = \"categorical_crossentropy\"\n",
    "ls = \"binary_crossentropy\"\n",
    "\n",
    "##########################################\n",
    "# discriminator optimization function\n",
    "# Adam option 0.0002, 0.0003, 0.0004\n",
    "dis_opti = Adam(learning_rate=0.0003, beta_1=0.5) #, amsgrad=True)\n",
    "\n",
    "# Adadelta option\n",
    "# dis_opti = Adadelta(learning_rate=0.00020)\n",
    "\n",
    "# Adagrad option\n",
    "# dis_opti = Adagrad(learning_rate=0.00020, momentum=0.5)\n",
    "\n",
    "##########################################\n",
    "# gan/genenerator optimization function\n",
    "# Adam option\n",
    "gan_opti = Adam(learning_rate=0.0002, beta_1=0.5) #, amsgrad=True)\n",
    "\n",
    "# Adadelta option\n",
    "# gan_opti = Adadelta(learning_rate=0.00030)\n",
    "\n",
    "# Adagrad option\n",
    "# gan_opti = Adagrad(learning_rate=0.00030, momentum=0.5)\n",
    "\n",
    "# SGD option\n",
    "# gan_opti = SGD(learning_rate=0.00020, momentum=0.5)\n",
    "\n",
    "# evaluation score\n",
    "# met = [\"categorical_accuracy\"]\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parameters to exeute training\n",
    "# verbose mode\n",
    "ver = 0\n",
    "# training epocha\n",
    "epo = 500\n",
    "print(\"training epochs:\", epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.4, 64, (4, 4), (2, 2), <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001E46C77F910>, <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001E46C77F070>, (2, 2), 'same', 7500, 1, 'sigmoid', (50, 50, 3))\n"
     ]
    }
   ],
   "source": [
    "dis_layer_cfg = (\n",
    "    dis_ldrop,\n",
    "    dis_filters,\n",
    "    ksize_s,\n",
    "    stsize_s,\n",
    "    # ksize,\n",
    "    # stsize,\n",
    "    in_dis_actf,\n",
    "    hid_ly_actf,\n",
    "    psize,\n",
    "    pad,\n",
    "    dis_midn,\n",
    "    out_dis,\n",
    "    out_dis_act,\n",
    "    gen_reshape,\n",
    ")\n",
    "\n",
    "print(dis_layer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdis_model = create_cdiscriminator_v2(disin_shape, genin_shape, n_labels, dis_layer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN Discriminator Definition\nModel: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nImgLabelsIn (InputLayer)        [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nImgLblEmb_1 (Embedding)         (None, 16, 142)      2130        ImgLabelsIn[0][0]                \n__________________________________________________________________________________________________\nImgLblFlat (Flatten)            (None, 2272)         0           ImgLblEmb_1[0][0]                \n__________________________________________________________________________________________________\nImgLblDense_1 (Dense)           (None, 7500)         17047500    ImgLblFlat[0][0]                 \n__________________________________________________________________________________________________\nImgLblReshape (Reshape)         (None, 50, 50, 3)    0           ImgLblDense_1[0][0]              \n__________________________________________________________________________________________________\nImglblConv2D_1 (Conv2DTranspose (None, 100, 100, 8)  392         ImgLblReshape[0][0]              \n__________________________________________________________________________________________________\nImgBaNorm_1 (BatchNormalization (None, 100, 100, 8)  32          ImglblConv2D_1[0][0]             \n__________________________________________________________________________________________________\nImgDrop_1 (Dropout)             (None, 100, 100, 8)  0           ImgBaNorm_1[0][0]                \n__________________________________________________________________________________________________\nImglblConv2D_2 (Conv2DTranspose (None, 200, 200, 16) 2064        ImgDrop_1[0][0]                  \n__________________________________________________________________________________________________\nImgBaNorm_2 (BatchNormalization (None, 200, 200, 16) 64          ImglblConv2D_2[0][0]             \n__________________________________________________________________________________________________\nImgDrop_2 (Dropout)             (None, 200, 200, 16) 0           ImgBaNorm_2[0][0]                \n__________________________________________________________________________________________________\nDisIn (InputLayer)              [(None, 400, 400, 3) 0                                            \n__________________________________________________________________________________________________\nImglblOut (Conv2DTranspose)     (None, 400, 400, 3)  771         ImgDrop_2[0][0]                  \n__________________________________________________________________________________________________\nDisConcat_1 (Concatenate)       (None, 400, 400, 6)  0           DisIn[0][0]                      \n                                                                 ImglblOut[0][0]                  \n__________________________________________________________________________________________________\nDisConv2D_1 (Conv2D)            (None, 200, 200, 64) 6208        DisConcat_1[0][0]                \n__________________________________________________________________________________________________\nDisBaNorm_1 (BatchNormalization (None, 200, 200, 64) 256         DisConv2D_1[0][0]                \n__________________________________________________________________________________________________\nDisDrop_1 (Dropout)             (None, 200, 200, 64) 0           DisBaNorm_1[0][0]                \n__________________________________________________________________________________________________\nDisConv2D_2 (Conv2D)            (None, 100, 100, 32) 32800       DisDrop_1[0][0]                  \n__________________________________________________________________________________________________\nDisConv2D_3 (Conv2D)            (None, 50, 50, 16)   8208        DisConv2D_2[0][0]                \n__________________________________________________________________________________________________\nDisConv2D_4 (Conv2D)            (None, 25, 25, 8)    2056        DisConv2D_3[0][0]                \n__________________________________________________________________________________________________\nDisBaNorm_2 (BatchNormalization (None, 25, 25, 8)    32          DisConv2D_4[0][0]                \n__________________________________________________________________________________________________\nDisDrop_2 (Dropout)             (None, 25, 25, 8)    0           DisBaNorm_2[0][0]                \n__________________________________________________________________________________________________\nDisMidFlat (Flatten)            (None, 5000)         0           DisDrop_2[0][0]                  \n__________________________________________________________________________________________________\nDisDense_1 (Dense)              (None, 7500)         37507500    DisMidFlat[0][0]                 \n__________________________________________________________________________________________________\nDisDense_2 (Dense)              (None, 3750)         28128750    DisDense_1[0][0]                 \n__________________________________________________________________________________________________\nDisDrop_3 (Dropout)             (None, 3750)         0           DisDense_2[0][0]                 \n__________________________________________________________________________________________________\nDisDense_3 (Dense)              (None, 1875)         7033125     DisDrop_3[0][0]                  \n__________________________________________________________________________________________________\nDisDense_4 (Dense)              (None, 937)          1757812     DisDense_3[0][0]                 \n__________________________________________________________________________________________________\nDisDrop_4 (Dropout)             (None, 937)          0           DisDense_4[0][0]                 \n__________________________________________________________________________________________________\nDisDense_5 (Dense)              (None, 468)          438984      DisDrop_4[0][0]                  \n__________________________________________________________________________________________________\nDisDense_6 (Dense)              (None, 234)          109746      DisDense_5[0][0]                 \n__________________________________________________________________________________________________\nDisOut (Dense)                  (None, 1)            235         DisDense_6[0][0]                 \n==================================================================================================\nTotal params: 92,078,665\nTrainable params: 92,078,473\nNon-trainable params: 192\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"CGAN Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "cdis_model.model_name = dis_model_name\n",
    "\n",
    "# compile model\n",
    "cdis_model.compile(loss=ls, optimizer=dis_opti, metrics=met)\n",
    "# cdis_model.trainable = False\n",
    "cdis_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(500, 0.2, 128, (4, 4), (2, 2), <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001E46C77F9D0>, <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001E46C77F070>, (2, 2), 'same', 7500, 'tanh', True, (50, 50, 3))\n"
     ]
    }
   ],
   "source": [
    "gen_layer_cfg = (\n",
    "    lstm_units,\n",
    "    gen_ldrop,\n",
    "    gen_filters,\n",
    "    ksize_s,\n",
    "    stsize_s,\n",
    "    # ksize,\n",
    "    # stsize,\n",
    "    in_gen_actf,\n",
    "    hid_ly_actf,\n",
    "    psize,\n",
    "    pad,\n",
    "    gen_midn,\n",
    "    out_gen_act,\n",
    "    rs,\n",
    "    gen_reshape,\n",
    ")\n",
    "\n",
    "print(gen_layer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgen_model = create_cgenerator_v2(disin_shape, genin_shape, n_labels, gen_layer_cfg)"
   ]
  },
  {
   "source": [
    "# defining model\n",
    "print(\"CGAN Generator definition\")\n",
    "cgen_model.model_name = gen_model_name\n",
    "\n",
    "# NOT compile model\n",
    "# gen_model.compile(loss=ls, optimizer=gan_opti, metrics=met)\n",
    "cgen_model.summary()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 253,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN Generator definition\nModel: \"model_7\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nTxtLabelsIn (InputLayer)        [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nTxtLblEmb_1 (Embedding)         (None, 16, 142)      2130        TxtLabelsIn[0][0]                \n__________________________________________________________________________________________________\nTxtLblFlat (Flatten)            (None, 2272)         0           TxtLblEmb_1[0][0]                \n__________________________________________________________________________________________________\nLblDense_1 (Dense)              (None, 7500)         17047500    TxtLblFlat[0][0]                 \n__________________________________________________________________________________________________\nTxtBaNorm_1 (BatchNormalization (None, 7500)         30000       LblDense_1[0][0]                 \n__________________________________________________________________________________________________\nTxtDrop_1 (Dropout)             (None, 7500)         0           TxtBaNorm_1[0][0]                \n__________________________________________________________________________________________________\nTxtLblDense_2 (Dense)           (None, 2130)         15977130    TxtDrop_1[0][0]                  \n__________________________________________________________________________________________________\nGenIn (InputLayer)              [(None, 15, 142)]    0                                            \n__________________________________________________________________________________________________\nTxtLblOut (Reshape)             (None, 15, 142)      0           TxtLblDense_2[0][0]              \n__________________________________________________________________________________________________\nGenConcat_1 (Concatenate)       (None, 15, 284)      0           GenIn[0][0]                      \n                                                                 TxtLblOut[0][0]                  \n__________________________________________________________________________________________________\nGenMaskIn (Masking)             (None, 15, 284)      0           GenConcat_1[0][0]                \n__________________________________________________________________________________________________\nGenLSTM_1 (LSTM)                (None, 15, 500)      1570000     GenMaskIn[0][0]                  \n__________________________________________________________________________________________________\nGenBaNorm_1 (BatchNormalization (None, 15, 500)      2000        GenLSTM_1[0][0]                  \n__________________________________________________________________________________________________\nGenDrop_1 (Dropout)             (None, 15, 500)      0           GenBaNorm_1[0][0]                \n__________________________________________________________________________________________________\nGenLSTM_2 (LSTM)                (None, 15, 250)      751000      GenDrop_1[0][0]                  \n__________________________________________________________________________________________________\nGenBaNorm_2 (BatchNormalization (None, 15, 250)      1000        GenLSTM_2[0][0]                  \n__________________________________________________________________________________________________\nGenDrop_2 (Dropout)             (None, 15, 250)      0           GenBaNorm_2[0][0]                \n__________________________________________________________________________________________________\nGenMidFlat (Flatten)            (None, 3750)         0           GenDrop_2[0][0]                  \n__________________________________________________________________________________________________\nGenGenDense_1 (Dense)           (None, 7500)         28132500    GenMidFlat[0][0]                 \n__________________________________________________________________________________________________\nGenReshape (Reshape)            (None, 50, 50, 3)    0           GenGenDense_1[0][0]              \n__________________________________________________________________________________________________\nGenConv2D_1 (Conv2DTranspose)   (None, 100, 100, 16) 784         GenReshape[0][0]                 \n__________________________________________________________________________________________________\nGenBaNorm_3 (BatchNormalization (None, 100, 100, 16) 64          GenConv2D_1[0][0]                \n__________________________________________________________________________________________________\nGenDrop_3 (Dropout)             (None, 100, 100, 16) 0           GenBaNorm_3[0][0]                \n__________________________________________________________________________________________________\nGenConv2D_2 (Conv2DTranspose)   (None, 200, 200, 32) 8224        GenDrop_3[0][0]                  \n__________________________________________________________________________________________________\nGenConv2D_3 (Conv2DTranspose)   (None, 400, 400, 64) 32832       GenConv2D_2[0][0]                \n__________________________________________________________________________________________________\nGenBaNorm_4 (BatchNormalization (None, 400, 400, 64) 256         GenConv2D_3[0][0]                \n__________________________________________________________________________________________________\nGenDrop_4 (Dropout)             (None, 400, 400, 64) 0           GenBaNorm_4[0][0]                \n__________________________________________________________________________________________________\nGenOut (Conv2D)                 (None, 400, 400, 3)  1731        GenDrop_4[0][0]                  \n==================================================================================================\nTotal params: 63,557,151\nTrainable params: 63,540,491\nNon-trainable params: 16,660\n__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cgan(cgen_model, cdis_model, gan_cfg):\n",
    "\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tcdis_model.trainable = False\n",
    "\t# get noise and label inputs from generator model\n",
    "\tgen_noise, gen_labels = cgen_model.input\n",
    "\t# get image output from the generator model\n",
    "\tgen_output = cgen_model.output\n",
    "\t# connect image output and label input from generator as inputs to discriminator\n",
    "\tgan_output = cdis_model([gen_output, gen_labels])\n",
    "\t# define gan model as taking noise and label and outputting a classification\n",
    "\tcgan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "\t# compile model\n",
    "\tcgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1], metrics=gan_cfg[2])\n",
    "\t# cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "\treturn cgan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_layer_cfg = (\n",
    "    ls,\n",
    "    gan_opti,\n",
    "    met,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN Model definition\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "TxtLabelsIn (InputLayer)        [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TxtLblEmb_1 (Embedding)         (None, 16, 142)      2130        TxtLabelsIn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TxtLblFlat (Flatten)            (None, 2272)         0           TxtLblEmb_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LblDense_1 (Dense)              (None, 7500)         17047500    TxtLblFlat[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TxtBaNorm_1 (BatchNormalization (None, 7500)         30000       LblDense_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "TxtDrop_1 (Dropout)             (None, 7500)         0           TxtBaNorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TxtLblDense_2 (Dense)           (None, 2130)         15977130    TxtDrop_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenIn (InputLayer)              [(None, 15, 142)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TxtLblOut (Reshape)             (None, 15, 142)      0           TxtLblDense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "GenConcat_1 (Concatenate)       (None, 15, 284)      0           GenIn[0][0]                      \n",
      "                                                                 TxtLblOut[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenMaskIn (Masking)             (None, 15, 284)      0           GenConcat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenLSTM_1 (LSTM)                (None, 15, 500)      1570000     GenMaskIn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenBaNorm_1 (BatchNormalization (None, 15, 500)      2000        GenLSTM_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenDrop_1 (Dropout)             (None, 15, 500)      0           GenBaNorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenLSTM_2 (LSTM)                (None, 15, 250)      751000      GenDrop_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenBaNorm_2 (BatchNormalization (None, 15, 250)      1000        GenLSTM_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenDrop_2 (Dropout)             (None, 15, 250)      0           GenBaNorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenMidFlat (Flatten)            (None, 3750)         0           GenDrop_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenGenDense_1 (Dense)           (None, 7500)         28132500    GenMidFlat[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "GenReshape (Reshape)            (None, 50, 50, 3)    0           GenGenDense_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "GenConv2D_1 (Conv2DTranspose)   (None, 100, 100, 16) 784         GenReshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "GenBaNorm_3 (BatchNormalization (None, 100, 100, 16) 64          GenConv2D_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenDrop_3 (Dropout)             (None, 100, 100, 16) 0           GenBaNorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenConv2D_2 (Conv2DTranspose)   (None, 200, 200, 32) 8224        GenDrop_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GenConv2D_3 (Conv2DTranspose)   (None, 400, 400, 64) 32832       GenConv2D_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenBaNorm_4 (BatchNormalization (None, 400, 400, 64) 256         GenConv2D_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenDrop_4 (Dropout)             (None, 400, 400, 64) 0           GenBaNorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "GenOut (Conv2D)                 (None, 400, 400, 3)  1731        GenDrop_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Functional)            (None, 1)            92078665    GenOut[0][0]                     \n",
      "                                                                 TxtLabelsIn[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 155,635,816\n",
      "Trainable params: 63,540,491\n",
      "Non-trainable params: 92,095,325\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"CGAN Model definition\")\n",
    "gan_model = create_cgan(cgen_model, cdis_model, cgan_layer_cfg)\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model topology into png files\n",
    "export_model(cgen_model, model_fn_path, gen_model_name, timestamp)\n",
    "export_model(cdis_model, model_fn_path, dis_model_name, timestamp)\n",
    "export_model(gan_model, model_fn_path, gan_model_name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for training\n",
    "fn_config = (model_fn_path, report_fn_path, dis_model_name, gen_model_name, gan_model_name)\n",
    "check_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing according to train/test proportions\n",
    "# Xt_train, Xt_test, Xi_train, Xi_test = train_test_split(X_txt, X_img, train_size = train_split, test_size = test_split, random_state = randseed)\n",
    "# Xi_train, Xi_test, y_train, y_test = train_test_split(X_img, y, train_size = train_split, test_size = test_split, random_state = randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 400, 400, 3) (59, 15, 142) (59, 1) (59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_img.shape, X_txt.shape, y.shape, y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">>> Epoch: 1, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 1.014, acc: 0.250] || [F-Dis loss: 4.067, acc: 0.062] || [Gen loss: 0.602, acc: 0.000]\n",
      "Epoch: 1 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.694\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.663\n",
      "Epoch:  1 elapsed time: 36.36 [s]\n",
      ">>> Epoch: 2, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.493, acc: 0.500] || [F-Dis loss: 1.063, acc: 0.062] || [Gen loss: 0.687, acc: 0.000]\n",
      "Epoch:  2 elapsed time: 25.03 [s]\n",
      ">>> Epoch: 3, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.787, acc: 0.250] || [F-Dis loss: 0.842, acc: 0.062] || [Gen loss: 0.675, acc: 0.000]\n",
      "Epoch:  3 elapsed time: 25.49 [s]\n",
      ">>> Epoch: 4, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.588, acc: 0.688] || [F-Dis loss: 0.736, acc: 0.062] || [Gen loss: 0.648, acc: 0.000]\n",
      "Epoch:  4 elapsed time: 27.96 [s]\n",
      ">>> Epoch: 5, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.424, acc: 0.812] || [F-Dis loss: 0.573, acc: 0.062] || [Gen loss: 0.596, acc: 0.000]\n",
      "Epoch:  5 elapsed time: 29.70 [s]\n",
      ">>> Epoch: 6, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.278, acc: 0.875] || [F-Dis loss: 0.583, acc: 0.062] || [Gen loss: 0.540, acc: 0.000]\n",
      "Epoch:  6 elapsed time: 26.05 [s]\n",
      ">>> Epoch: 7, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.194, acc: 0.812] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.363, acc: 0.000]\n",
      "Epoch:  7 elapsed time: 25.29 [s]\n",
      ">>> Epoch: 8, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.054, acc: 0.938] || [F-Dis loss: 0.364, acc: 0.062] || [Gen loss: 0.283, acc: 0.000]\n",
      "Epoch:  8 elapsed time: 23.45 [s]\n",
      ">>> Epoch: 9, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.047, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 0.196, acc: 0.000]\n",
      "Epoch:  9 elapsed time: 22.37 [s]\n",
      ">>> Epoch: 10, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.540, acc: 0.062] || [Gen loss: 0.271, acc: 0.000]\n",
      "Epoch: 10 elapsed time: 23.76 [s]\n",
      ">>> Epoch: 11, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.132, acc: 0.875] || [F-Dis loss: 0.655, acc: 0.062] || [Gen loss: 0.263, acc: 0.000]\n",
      "Epoch: 11 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.634\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.162\n",
      "Epoch: 11 elapsed time: 26.08 [s]\n",
      ">>> Epoch: 12, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.336, acc: 0.500] || [F-Dis loss: 0.447, acc: 0.062] || [Gen loss: 0.165, acc: 0.000]\n",
      "Epoch: 12 elapsed time: 24.48 [s]\n",
      ">>> Epoch: 13, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.585, acc: 0.062] || [Gen loss: 0.226, acc: 0.000]\n",
      "Epoch: 13 elapsed time: 23.59 [s]\n",
      ">>> Epoch: 14, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.607, acc: 0.062] || [Gen loss: 0.276, acc: 0.000]\n",
      "Epoch: 14 elapsed time: 24.18 [s]\n",
      ">>> Epoch: 15, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 0.225, acc: 0.000]\n",
      "Epoch: 15 elapsed time: 25.44 [s]\n",
      ">>> Epoch: 16, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.400, acc: 0.062] || [Gen loss: 0.101, acc: 0.000]\n",
      "Epoch: 16 elapsed time: 29.16 [s]\n",
      ">>> Epoch: 17, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.047, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.107, acc: 0.000]\n",
      "Epoch: 17 elapsed time: 29.19 [s]\n",
      ">>> Epoch: 18, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 0.228, acc: 0.000]\n",
      "Epoch: 18 elapsed time: 30.28 [s]\n",
      ">>> Epoch: 19, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.321, acc: 0.000]\n",
      "Epoch: 19 elapsed time: 34.46 [s]\n",
      ">>> Epoch: 20, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.376, acc: 0.062] || [Gen loss: 0.112, acc: 0.000]\n",
      "Epoch: 20 elapsed time: 36.98 [s]\n",
      ">>> Epoch: 21, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.007, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 0.323, acc: 0.000]\n",
      "Epoch: 21 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.592\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.064\n",
      "Epoch: 21 elapsed time: 40.40 [s]\n",
      ">>> Epoch: 22, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.311, acc: 0.500] || [F-Dis loss: 0.456, acc: 0.062] || [Gen loss: 0.167, acc: 0.000]\n",
      "Epoch: 22 elapsed time: 38.34 [s]\n",
      ">>> Epoch: 23, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.441, acc: 0.062] || [Gen loss: 0.328, acc: 0.000]\n",
      "Epoch: 23 elapsed time: 25.08 [s]\n",
      ">>> Epoch: 24, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.500, acc: 0.062] || [Gen loss: 0.087, acc: 0.000]\n",
      "Epoch: 24 elapsed time: 25.29 [s]\n",
      ">>> Epoch: 25, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.489, acc: 0.062] || [Gen loss: 0.201, acc: 0.000]\n",
      "Epoch: 25 elapsed time: 25.37 [s]\n",
      ">>> Epoch: 26, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.361, acc: 0.062] || [Gen loss: 0.315, acc: 0.000]\n",
      "Epoch: 26 elapsed time: 23.74 [s]\n",
      ">>> Epoch: 27, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.493, acc: 0.062] || [Gen loss: 0.184, acc: 0.000]\n",
      "Epoch: 27 elapsed time: 25.17 [s]\n",
      ">>> Epoch: 28, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.431, acc: 0.062] || [Gen loss: 0.156, acc: 0.000]\n",
      "Epoch: 28 elapsed time: 26.89 [s]\n",
      ">>> Epoch: 29, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.424, acc: 0.062] || [Gen loss: 0.291, acc: 0.000]\n",
      "Epoch: 29 elapsed time: 26.46 [s]\n",
      ">>> Epoch: 30, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.465, acc: 0.062] || [Gen loss: 0.428, acc: 0.000]\n",
      "Epoch: 30 elapsed time: 28.18 [s]\n",
      ">>> Epoch: 31, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.181, acc: 0.000]\n",
      "Epoch: 31 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.548\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.055\n",
      "Epoch: 31 elapsed time: 34.05 [s]\n",
      ">>> Epoch: 32, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.282, acc: 0.500] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.233, acc: 0.000]\n",
      "Epoch: 32 elapsed time: 32.25 [s]\n",
      ">>> Epoch: 33, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 0.393, acc: 0.000]\n",
      "Epoch: 33 elapsed time: 29.97 [s]\n",
      ">>> Epoch: 34, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.313, acc: 0.000]\n",
      "Epoch: 34 elapsed time: 29.62 [s]\n",
      ">>> Epoch: 35, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.379, acc: 0.062] || [Gen loss: 0.276, acc: 0.000]\n",
      "Epoch: 35 elapsed time: 33.46 [s]\n",
      ">>> Epoch: 36, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 0.356, acc: 0.000]\n",
      "Epoch: 36 elapsed time: 31.11 [s]\n",
      ">>> Epoch: 37, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.455, acc: 0.062] || [Gen loss: 0.155, acc: 0.000]\n",
      "Epoch: 37 elapsed time: 31.00 [s]\n",
      ">>> Epoch: 38, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.157, acc: 0.000]\n",
      "Epoch: 38 elapsed time: 26.11 [s]\n",
      ">>> Epoch: 39, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.444, acc: 0.062] || [Gen loss: 0.103, acc: 0.000]\n",
      "Epoch: 39 elapsed time: 24.76 [s]\n",
      ">>> Epoch: 40, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.409, acc: 0.062] || [Gen loss: 0.254, acc: 0.000]\n",
      "Epoch: 40 elapsed time: 24.47 [s]\n",
      ">>> Epoch: 41, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.413, acc: 0.062] || [Gen loss: 0.206, acc: 0.000]\n",
      "Epoch: 41 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.516\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.036\n",
      "Epoch: 41 elapsed time: 27.87 [s]\n",
      ">>> Epoch: 42, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.276, acc: 0.500] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.251, acc: 0.000]\n",
      "Epoch: 42 elapsed time: 35.95 [s]\n",
      ">>> Epoch: 43, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.188, acc: 0.000]\n",
      "Epoch: 43 elapsed time: 29.95 [s]\n",
      ">>> Epoch: 44, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.360, acc: 0.062] || [Gen loss: 0.210, acc: 0.000]\n",
      "Epoch: 44 elapsed time: 27.54 [s]\n",
      ">>> Epoch: 45, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 0.087, acc: 0.000]\n",
      "Epoch: 45 elapsed time: 24.86 [s]\n",
      ">>> Epoch: 46, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.431, acc: 0.062] || [Gen loss: 0.247, acc: 0.000]\n",
      "Epoch: 46 elapsed time: 24.72 [s]\n",
      ">>> Epoch: 47, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 0.496, acc: 0.000]\n",
      "Epoch: 47 elapsed time: 23.92 [s]\n",
      ">>> Epoch: 48, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 0.172, acc: 0.000]\n",
      "Epoch: 48 elapsed time: 23.73 [s]\n",
      ">>> Epoch: 49, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.472, acc: 0.062] || [Gen loss: 0.245, acc: 0.000]\n",
      "Epoch: 49 elapsed time: 24.40 [s]\n",
      ">>> Epoch: 50, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 0.140, acc: 0.000]\n",
      "Epoch: 50 elapsed time: 24.41 [s]\n",
      ">>> Epoch: 51, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.294, acc: 0.000]\n",
      "Epoch: 51 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.514\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.035\n",
      "Epoch: 51 elapsed time: 32.22 [s]\n",
      ">>> Epoch: 52, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.275, acc: 0.500] || [F-Dis loss: 0.481, acc: 0.062] || [Gen loss: 0.140, acc: 0.000]\n",
      "Epoch: 52 elapsed time: 25.51 [s]\n",
      ">>> Epoch: 53, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.179, acc: 0.000]\n",
      "Epoch: 53 elapsed time: 24.84 [s]\n",
      ">>> Epoch: 54, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.464, acc: 0.062] || [Gen loss: 0.211, acc: 0.000]\n",
      "Epoch: 54 elapsed time: 24.80 [s]\n",
      ">>> Epoch: 55, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch: 55 elapsed time: 25.23 [s]\n",
      ">>> Epoch: 56, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 0.205, acc: 0.000]\n",
      "Epoch: 56 elapsed time: 26.84 [s]\n",
      ">>> Epoch: 57, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.444, acc: 0.062] || [Gen loss: 0.370, acc: 0.000]\n",
      "Epoch: 57 elapsed time: 25.68 [s]\n",
      ">>> Epoch: 58, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.238, acc: 0.000]\n",
      "Epoch: 58 elapsed time: 25.22 [s]\n",
      ">>> Epoch: 59, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.434, acc: 0.062] || [Gen loss: 0.145, acc: 0.000]\n",
      "Epoch: 59 elapsed time: 26.65 [s]\n",
      ">>> Epoch: 60, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.167, acc: 0.000]\n",
      "Epoch: 60 elapsed time: 24.43 [s]\n",
      ">>> Epoch: 61, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.398, acc: 0.062] || [Gen loss: 0.214, acc: 0.000]\n",
      "Epoch: 61 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.517\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.033\n",
      "Epoch: 61 elapsed time: 25.66 [s]\n",
      ">>> Epoch: 62, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.268, acc: 0.500] || [F-Dis loss: 0.447, acc: 0.062] || [Gen loss: 0.258, acc: 0.000]\n",
      "Epoch: 62 elapsed time: 22.50 [s]\n",
      ">>> Epoch: 63, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 0.426, acc: 0.000]\n",
      "Epoch: 63 elapsed time: 23.64 [s]\n",
      ">>> Epoch: 64, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 0.366, acc: 0.000]\n",
      "Epoch: 64 elapsed time: 25.08 [s]\n",
      ">>> Epoch: 65, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 0.281, acc: 0.000]\n",
      "Epoch: 65 elapsed time: 29.12 [s]\n",
      ">>> Epoch: 66, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.459, acc: 0.062] || [Gen loss: 0.240, acc: 0.000]\n",
      "Epoch: 66 elapsed time: 21.83 [s]\n",
      ">>> Epoch: 67, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.129, acc: 0.000]\n",
      "Epoch: 67 elapsed time: 23.19 [s]\n",
      ">>> Epoch: 68, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.386, acc: 0.000]\n",
      "Epoch: 68 elapsed time: 22.54 [s]\n",
      ">>> Epoch: 69, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.273, acc: 0.000]\n",
      "Epoch: 69 elapsed time: 21.95 [s]\n",
      ">>> Epoch: 70, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.447, acc: 0.062] || [Gen loss: 0.301, acc: 0.000]\n",
      "Epoch: 70 elapsed time: 24.85 [s]\n",
      ">>> Epoch: 71, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.444, acc: 0.062] || [Gen loss: 0.205, acc: 0.000]\n",
      "Epoch: 71 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.492\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.033\n",
      "Epoch: 71 elapsed time: 27.47 [s]\n",
      ">>> Epoch: 72, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.256, acc: 0.500] || [F-Dis loss: 0.449, acc: 0.062] || [Gen loss: 0.214, acc: 0.000]\n",
      "Epoch: 72 elapsed time: 23.62 [s]\n",
      ">>> Epoch: 73, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 0.244, acc: 0.000]\n",
      "Epoch: 73 elapsed time: 23.42 [s]\n",
      ">>> Epoch: 74, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 0.381, acc: 0.000]\n",
      "Epoch: 74 elapsed time: 22.75 [s]\n",
      ">>> Epoch: 75, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.447, acc: 0.062] || [Gen loss: 0.501, acc: 0.000]\n",
      "Epoch: 75 elapsed time: 22.96 [s]\n",
      ">>> Epoch: 76, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.052, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.248, acc: 0.000]\n",
      "Epoch: 76 elapsed time: 22.74 [s]\n",
      ">>> Epoch: 77, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.320, acc: 0.000]\n",
      "Epoch: 77 elapsed time: 21.15 [s]\n",
      ">>> Epoch: 78, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.354, acc: 0.062] || [Gen loss: 0.397, acc: 0.000]\n",
      "Epoch: 78 elapsed time: 21.27 [s]\n",
      ">>> Epoch: 79, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.481, acc: 0.062] || [Gen loss: 0.296, acc: 0.000]\n",
      "Epoch: 79 elapsed time: 21.73 [s]\n",
      ">>> Epoch: 80, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.280, acc: 0.000]\n",
      "Epoch: 80 elapsed time: 21.03 [s]\n",
      ">>> Epoch: 81, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.341, acc: 0.062] || [Gen loss: 0.300, acc: 0.000]\n",
      "Epoch: 81 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.459\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.036\n",
      "Epoch: 81 elapsed time: 23.88 [s]\n",
      ">>> Epoch: 82, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.235, acc: 0.500] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 0.101, acc: 0.000]\n",
      "Epoch: 82 elapsed time: 21.76 [s]\n",
      ">>> Epoch: 83, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.319, acc: 0.000]\n",
      "Epoch: 83 elapsed time: 20.79 [s]\n",
      ">>> Epoch: 84, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 0.108, acc: 0.000]\n",
      "Epoch: 84 elapsed time: 21.03 [s]\n",
      ">>> Epoch: 85, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.008, acc: 0.938] || [F-Dis loss: 0.382, acc: 0.062] || [Gen loss: 0.186, acc: 0.000]\n",
      "Epoch: 85 elapsed time: 20.99 [s]\n",
      ">>> Epoch: 86, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 0.193, acc: 0.000]\n",
      "Epoch: 86 elapsed time: 21.44 [s]\n",
      ">>> Epoch: 87, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.006, acc: 0.000]\n",
      "Epoch: 87 elapsed time: 20.95 [s]\n",
      ">>> Epoch: 88, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.098, acc: 0.000]\n",
      "Epoch: 88 elapsed time: 21.01 [s]\n",
      ">>> Epoch: 89, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.466, acc: 0.000]\n",
      "Epoch: 89 elapsed time: 21.07 [s]\n",
      ">>> Epoch: 90, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.388, acc: 0.062] || [Gen loss: 0.342, acc: 0.000]\n",
      "Epoch: 90 elapsed time: 21.15 [s]\n",
      ">>> Epoch: 91, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.189, acc: 0.000]\n",
      "Epoch: 91 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.436\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.031\n",
      "Epoch: 91 elapsed time: 23.71 [s]\n",
      ">>> Epoch: 92, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.232, acc: 0.500] || [F-Dis loss: 0.382, acc: 0.062] || [Gen loss: 0.166, acc: 0.000]\n",
      "Epoch: 92 elapsed time: 22.07 [s]\n",
      ">>> Epoch: 93, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.368, acc: 0.062] || [Gen loss: 0.175, acc: 0.000]\n",
      "Epoch: 93 elapsed time: 21.57 [s]\n",
      ">>> Epoch: 94, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.375, acc: 0.062] || [Gen loss: 0.185, acc: 0.000]\n",
      "Epoch: 94 elapsed time: 21.53 [s]\n",
      ">>> Epoch: 95, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.415, acc: 0.062] || [Gen loss: 0.102, acc: 0.000]\n",
      "Epoch: 95 elapsed time: 21.50 [s]\n",
      ">>> Epoch: 96, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.243, acc: 0.000]\n",
      "Epoch: 96 elapsed time: 21.64 [s]\n",
      ">>> Epoch: 97, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.007, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 0.299, acc: 0.000]\n",
      "Epoch: 97 elapsed time: 21.51 [s]\n",
      ">>> Epoch: 98, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.451, acc: 0.062] || [Gen loss: 0.229, acc: 0.000]\n",
      "Epoch: 98 elapsed time: 21.43 [s]\n",
      ">>> Epoch: 99, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.449, acc: 0.062] || [Gen loss: 0.153, acc: 0.000]\n",
      "Epoch: 99 elapsed time: 21.23 [s]\n",
      ">>> Epoch: 100, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.384, acc: 0.062] || [Gen loss: 0.339, acc: 0.000]\n",
      "Epoch:100 elapsed time: 21.34 [s]\n",
      ">>> Epoch: 101, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.363, acc: 0.062] || [Gen loss: 0.245, acc: 0.000]\n",
      "Epoch: 101 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.408\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.031\n",
      "Epoch:101 elapsed time: 28.36 [s]\n",
      ">>> Epoch: 102, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.212, acc: 0.500] || [F-Dis loss: 0.476, acc: 0.062] || [Gen loss: 0.071, acc: 0.000]\n",
      "Epoch:102 elapsed time: 21.77 [s]\n",
      ">>> Epoch: 103, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.476, acc: 0.062] || [Gen loss: 0.307, acc: 0.000]\n",
      "Epoch:103 elapsed time: 21.56 [s]\n",
      ">>> Epoch: 104, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.405, acc: 0.062] || [Gen loss: 0.176, acc: 0.000]\n",
      "Epoch:104 elapsed time: 21.95 [s]\n",
      ">>> Epoch: 105, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.467, acc: 0.000]\n",
      "Epoch:105 elapsed time: 22.93 [s]\n",
      ">>> Epoch: 106, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.242, acc: 0.000]\n",
      "Epoch:106 elapsed time: 22.74 [s]\n",
      ">>> Epoch: 107, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.220, acc: 0.000]\n",
      "Epoch:107 elapsed time: 22.51 [s]\n",
      ">>> Epoch: 108, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.466, acc: 0.062] || [Gen loss: 0.340, acc: 0.000]\n",
      "Epoch:108 elapsed time: 22.30 [s]\n",
      ">>> Epoch: 109, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 0.232, acc: 0.000]\n",
      "Epoch:109 elapsed time: 22.13 [s]\n",
      ">>> Epoch: 110, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 0.130, acc: 0.000]\n",
      "Epoch:110 elapsed time: 22.46 [s]\n",
      ">>> Epoch: 111, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.345, acc: 0.062] || [Gen loss: 0.458, acc: 0.000]\n",
      "Epoch: 111 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.448\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.026\n",
      "Epoch:111 elapsed time: 25.39 [s]\n",
      ">>> Epoch: 112, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.229, acc: 0.500] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 0.246, acc: 0.000]\n",
      "Epoch:112 elapsed time: 22.62 [s]\n",
      ">>> Epoch: 113, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 0.105, acc: 0.000]\n",
      "Epoch:113 elapsed time: 22.25 [s]\n",
      ">>> Epoch: 114, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.410, acc: 0.062] || [Gen loss: 0.191, acc: 0.000]\n",
      "Epoch:114 elapsed time: 22.22 [s]\n",
      ">>> Epoch: 115, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.203, acc: 0.000]\n",
      "Epoch:115 elapsed time: 22.61 [s]\n",
      ">>> Epoch: 116, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.316, acc: 0.000]\n",
      "Epoch:116 elapsed time: 22.40 [s]\n",
      ">>> Epoch: 117, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 0.244, acc: 0.000]\n",
      "Epoch:117 elapsed time: 22.20 [s]\n",
      ">>> Epoch: 118, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.415, acc: 0.062] || [Gen loss: 0.253, acc: 0.000]\n",
      "Epoch:118 elapsed time: 23.02 [s]\n",
      ">>> Epoch: 119, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.398, acc: 0.062] || [Gen loss: 0.175, acc: 0.000]\n",
      "Epoch:119 elapsed time: 22.48 [s]\n",
      ">>> Epoch: 120, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.049, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 0.223, acc: 0.000]\n",
      "Epoch:120 elapsed time: 23.19 [s]\n",
      ">>> Epoch: 121, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.502, acc: 0.062] || [Gen loss: 0.127, acc: 0.000]\n",
      "Epoch: 121 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.485\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.030\n",
      "Epoch:121 elapsed time: 25.82 [s]\n",
      ">>> Epoch: 122, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.261, acc: 0.500] || [F-Dis loss: 0.468, acc: 0.062] || [Gen loss: 0.163, acc: 0.000]\n",
      "Epoch:122 elapsed time: 23.27 [s]\n",
      ">>> Epoch: 123, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.034, acc: 0.000]\n",
      "Epoch:123 elapsed time: 22.40 [s]\n",
      ">>> Epoch: 124, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 0.498, acc: 0.000]\n",
      "Epoch:124 elapsed time: 22.63 [s]\n",
      ">>> Epoch: 125, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.212, acc: 0.000]\n",
      "Epoch:125 elapsed time: 22.25 [s]\n",
      ">>> Epoch: 126, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.315, acc: 0.000]\n",
      "Epoch:126 elapsed time: 22.54 [s]\n",
      ">>> Epoch: 127, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.430, acc: 0.062] || [Gen loss: 0.294, acc: 0.000]\n",
      "Epoch:127 elapsed time: 22.61 [s]\n",
      ">>> Epoch: 128, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.413, acc: 0.062] || [Gen loss: 0.083, acc: 0.000]\n",
      "Epoch:128 elapsed time: 22.36 [s]\n",
      ">>> Epoch: 129, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 0.228, acc: 0.000]\n",
      "Epoch:129 elapsed time: 22.45 [s]\n",
      ">>> Epoch: 130, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.441, acc: 0.062] || [Gen loss: 0.349, acc: 0.000]\n",
      "Epoch:130 elapsed time: 22.45 [s]\n",
      ">>> Epoch: 131, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.165, acc: 0.000]\n",
      "Epoch: 131 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.496\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.032\n",
      "Epoch:131 elapsed time: 25.52 [s]\n",
      ">>> Epoch: 132, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.260, acc: 0.500] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 0.065, acc: 0.000]\n",
      "Epoch:132 elapsed time: 22.63 [s]\n",
      ">>> Epoch: 133, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.470, acc: 0.062] || [Gen loss: 0.239, acc: 0.000]\n",
      "Epoch:133 elapsed time: 22.82 [s]\n",
      ">>> Epoch: 134, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.485, acc: 0.062] || [Gen loss: 0.145, acc: 0.000]\n",
      "Epoch:134 elapsed time: 22.40 [s]\n",
      ">>> Epoch: 135, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.368, acc: 0.062] || [Gen loss: 0.016, acc: 0.000]\n",
      "Epoch:135 elapsed time: 22.59 [s]\n",
      ">>> Epoch: 136, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 0.328, acc: 0.000]\n",
      "Epoch:136 elapsed time: 22.51 [s]\n",
      ">>> Epoch: 137, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.462, acc: 0.062] || [Gen loss: 0.311, acc: 0.000]\n",
      "Epoch:137 elapsed time: 22.92 [s]\n",
      ">>> Epoch: 138, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.233, acc: 0.000]\n",
      "Epoch:138 elapsed time: 21.75 [s]\n",
      ">>> Epoch: 139, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.388, acc: 0.062] || [Gen loss: 0.442, acc: 0.000]\n",
      "Epoch:139 elapsed time: 21.10 [s]\n",
      ">>> Epoch: 140, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.434, acc: 0.062] || [Gen loss: 0.202, acc: 0.000]\n",
      "Epoch:140 elapsed time: 21.00 [s]\n",
      ">>> Epoch: 141, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.078, acc: 0.000]\n",
      "Epoch: 141 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.433\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.033\n",
      "Epoch:141 elapsed time: 24.78 [s]\n",
      ">>> Epoch: 142, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.226, acc: 0.500] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.126, acc: 0.000]\n",
      "Epoch:142 elapsed time: 21.10 [s]\n",
      ">>> Epoch: 143, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 0.505, acc: 0.000]\n",
      "Epoch:143 elapsed time: 21.12 [s]\n",
      ">>> Epoch: 144, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 0.201, acc: 0.000]\n",
      "Epoch:144 elapsed time: 21.59 [s]\n",
      ">>> Epoch: 145, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.482, acc: 0.062] || [Gen loss: 0.175, acc: 0.000]\n",
      "Epoch:145 elapsed time: 20.71 [s]\n",
      ">>> Epoch: 146, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 0.419, acc: 0.000]\n",
      "Epoch:146 elapsed time: 20.85 [s]\n",
      ">>> Epoch: 147, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 0.070, acc: 0.000]\n",
      "Epoch:147 elapsed time: 20.95 [s]\n",
      ">>> Epoch: 148, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 0.176, acc: 0.000]\n",
      "Epoch:148 elapsed time: 20.93 [s]\n",
      ">>> Epoch: 149, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.348, acc: 0.062] || [Gen loss: 0.398, acc: 0.000]\n",
      "Epoch:149 elapsed time: 20.83 [s]\n",
      ">>> Epoch: 150, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.466, acc: 0.062] || [Gen loss: 0.221, acc: 0.000]\n",
      "Epoch:150 elapsed time: 21.32 [s]\n",
      ">>> Epoch: 151, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.259, acc: 0.000]\n",
      "Epoch: 151 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.446\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.032\n",
      "Epoch:151 elapsed time: 28.02 [s]\n",
      ">>> Epoch: 152, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.244, acc: 0.500] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 0.100, acc: 0.000]\n",
      "Epoch:152 elapsed time: 21.63 [s]\n",
      ">>> Epoch: 153, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 0.232, acc: 0.000]\n",
      "Epoch:153 elapsed time: 20.80 [s]\n",
      ">>> Epoch: 154, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.242, acc: 0.000]\n",
      "Epoch:154 elapsed time: 21.31 [s]\n",
      ">>> Epoch: 155, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.444, acc: 0.062] || [Gen loss: 0.247, acc: 0.000]\n",
      "Epoch:155 elapsed time: 21.27 [s]\n",
      ">>> Epoch: 156, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.419, acc: 0.062] || [Gen loss: 0.075, acc: 0.000]\n",
      "Epoch:156 elapsed time: 20.98 [s]\n",
      ">>> Epoch: 157, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 0.146, acc: 0.000]\n",
      "Epoch:157 elapsed time: 21.22 [s]\n",
      ">>> Epoch: 158, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.108, acc: 0.000]\n",
      "Epoch:158 elapsed time: 21.16 [s]\n",
      ">>> Epoch: 159, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.413, acc: 0.062] || [Gen loss: 0.304, acc: 0.000]\n",
      "Epoch:159 elapsed time: 21.00 [s]\n",
      ">>> Epoch: 160, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.187, acc: 0.000]\n",
      "Epoch:160 elapsed time: 21.43 [s]\n",
      ">>> Epoch: 161, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.206, acc: 0.000]\n",
      "Epoch: 161 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.465\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.029\n",
      "Epoch:161 elapsed time: 23.43 [s]\n",
      ">>> Epoch: 162, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.244, acc: 0.500] || [F-Dis loss: 0.379, acc: 0.062] || [Gen loss: 0.417, acc: 0.000]\n",
      "Epoch:162 elapsed time: 21.37 [s]\n",
      ">>> Epoch: 163, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.221, acc: 0.000]\n",
      "Epoch:163 elapsed time: 21.01 [s]\n",
      ">>> Epoch: 164, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.431, acc: 0.062] || [Gen loss: 0.319, acc: 0.000]\n",
      "Epoch:164 elapsed time: 21.00 [s]\n",
      ">>> Epoch: 165, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 0.231, acc: 0.000]\n",
      "Epoch:165 elapsed time: 20.86 [s]\n",
      ">>> Epoch: 166, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 0.310, acc: 0.000]\n",
      "Epoch:166 elapsed time: 21.22 [s]\n",
      ">>> Epoch: 167, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.281, acc: 0.000]\n",
      "Epoch:167 elapsed time: 21.40 [s]\n",
      ">>> Epoch: 168, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.175, acc: 0.000]\n",
      "Epoch:168 elapsed time: 20.85 [s]\n",
      ">>> Epoch: 169, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.349, acc: 0.000]\n",
      "Epoch:169 elapsed time: 21.46 [s]\n",
      ">>> Epoch: 170, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.228, acc: 0.000]\n",
      "Epoch:170 elapsed time: 21.38 [s]\n",
      ">>> Epoch: 171, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 0.011, acc: 0.000]\n",
      "Epoch: 171 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.465\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.025\n",
      "Epoch:171 elapsed time: 23.60 [s]\n",
      ">>> Epoch: 172, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.238, acc: 0.500] || [F-Dis loss: 0.414, acc: 0.062] || [Gen loss: 0.314, acc: 0.000]\n",
      "Epoch:172 elapsed time: 21.85 [s]\n",
      ">>> Epoch: 173, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.280, acc: 0.000]\n",
      "Epoch:173 elapsed time: 21.11 [s]\n",
      ">>> Epoch: 174, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.429, acc: 0.062] || [Gen loss: 0.340, acc: 0.000]\n",
      "Epoch:174 elapsed time: 21.59 [s]\n",
      ">>> Epoch: 175, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 0.243, acc: 0.000]\n",
      "Epoch:175 elapsed time: 20.96 [s]\n",
      ">>> Epoch: 176, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 0.118, acc: 0.000]\n",
      "Epoch:176 elapsed time: 21.24 [s]\n",
      ">>> Epoch: 177, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 0.356, acc: 0.000]\n",
      "Epoch:177 elapsed time: 21.49 [s]\n",
      ">>> Epoch: 178, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.356, acc: 0.062] || [Gen loss: 0.299, acc: 0.000]\n",
      "Epoch:178 elapsed time: 21.28 [s]\n",
      ">>> Epoch: 179, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.539, acc: 0.062] || [Gen loss: 0.176, acc: 0.000]\n",
      "Epoch:179 elapsed time: 21.33 [s]\n",
      ">>> Epoch: 180, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 0.242, acc: 0.000]\n",
      "Epoch:180 elapsed time: 21.24 [s]\n",
      ">>> Epoch: 181, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 0.382, acc: 0.000]\n",
      "Epoch: 181 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.405\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.022\n",
      "Epoch:181 elapsed time: 25.20 [s]\n",
      ">>> Epoch: 182, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.219, acc: 0.500] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.122, acc: 0.000]\n",
      "Epoch:182 elapsed time: 24.88 [s]\n",
      ">>> Epoch: 183, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 0.328, acc: 0.000]\n",
      "Epoch:183 elapsed time: 21.36 [s]\n",
      ">>> Epoch: 184, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.444, acc: 0.062] || [Gen loss: 0.191, acc: 0.000]\n",
      "Epoch:184 elapsed time: 21.48 [s]\n",
      ">>> Epoch: 185, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.370, acc: 0.062] || [Gen loss: 0.096, acc: 0.000]\n",
      "Epoch:185 elapsed time: 21.17 [s]\n",
      ">>> Epoch: 186, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.048, acc: 0.938] || [F-Dis loss: 0.546, acc: 0.062] || [Gen loss: 0.301, acc: 0.000]\n",
      "Epoch:186 elapsed time: 21.79 [s]\n",
      ">>> Epoch: 187, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 0.443, acc: 0.000]\n",
      "Epoch:187 elapsed time: 21.09 [s]\n",
      ">>> Epoch: 188, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.405, acc: 0.062] || [Gen loss: 0.354, acc: 0.000]\n",
      "Epoch:188 elapsed time: 21.71 [s]\n",
      ">>> Epoch: 189, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.294, acc: 0.000]\n",
      "Epoch:189 elapsed time: 21.17 [s]\n",
      ">>> Epoch: 190, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.177, acc: 0.000]\n",
      "Epoch:190 elapsed time: 21.61 [s]\n",
      ">>> Epoch: 191, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.430, acc: 0.062] || [Gen loss: 0.294, acc: 0.000]\n",
      "Epoch: 191 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.440\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.021\n",
      "Epoch:191 elapsed time: 24.00 [s]\n",
      ">>> Epoch: 192, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.245, acc: 0.500] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 0.102, acc: 0.000]\n",
      "Epoch:192 elapsed time: 21.48 [s]\n",
      ">>> Epoch: 193, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.343, acc: 0.062] || [Gen loss: 0.355, acc: 0.000]\n",
      "Epoch:193 elapsed time: 21.49 [s]\n",
      ">>> Epoch: 194, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.299, acc: 0.000]\n",
      "Epoch:194 elapsed time: 21.79 [s]\n",
      ">>> Epoch: 195, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.465, acc: 0.062] || [Gen loss: 0.192, acc: 0.000]\n",
      "Epoch:195 elapsed time: 21.57 [s]\n",
      ">>> Epoch: 196, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.376, acc: 0.062] || [Gen loss: 0.290, acc: 0.000]\n",
      "Epoch:196 elapsed time: 21.44 [s]\n",
      ">>> Epoch: 197, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.478, acc: 0.062] || [Gen loss: 0.486, acc: 0.000]\n",
      "Epoch:197 elapsed time: 21.48 [s]\n",
      ">>> Epoch: 198, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.196, acc: 0.000]\n",
      "Epoch:198 elapsed time: 21.37 [s]\n",
      ">>> Epoch: 199, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.430, acc: 0.062] || [Gen loss: 0.482, acc: 0.000]\n",
      "Epoch:199 elapsed time: 21.44 [s]\n",
      ">>> Epoch: 200, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.284, acc: 0.000]\n",
      "Epoch:200 elapsed time: 21.14 [s]\n",
      ">>> Epoch: 201, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.309, acc: 0.000]\n",
      "Epoch: 201 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.448\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.025\n",
      "Epoch:201 elapsed time: 28.08 [s]\n",
      ">>> Epoch: 202, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.233, acc: 0.500] || [F-Dis loss: 0.488, acc: 0.062] || [Gen loss: 0.314, acc: 0.000]\n",
      "Epoch:202 elapsed time: 22.09 [s]\n",
      ">>> Epoch: 203, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.231, acc: 0.000]\n",
      "Epoch:203 elapsed time: 23.38 [s]\n",
      ">>> Epoch: 204, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 0.394, acc: 0.000]\n",
      "Epoch:204 elapsed time: 21.55 [s]\n",
      ">>> Epoch: 205, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 0.412, acc: 0.000]\n",
      "Epoch:205 elapsed time: 21.39 [s]\n",
      ">>> Epoch: 206, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.429, acc: 0.062] || [Gen loss: 0.124, acc: 0.000]\n",
      "Epoch:206 elapsed time: 21.37 [s]\n",
      ">>> Epoch: 207, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 0.199, acc: 0.000]\n",
      "Epoch:207 elapsed time: 21.50 [s]\n",
      ">>> Epoch: 208, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.460, acc: 0.062] || [Gen loss: 0.351, acc: 0.000]\n",
      "Epoch:208 elapsed time: 21.47 [s]\n",
      ">>> Epoch: 209, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.414, acc: 0.062] || [Gen loss: 0.272, acc: 0.000]\n",
      "Epoch:209 elapsed time: 21.77 [s]\n",
      ">>> Epoch: 210, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 0.189, acc: 0.000]\n",
      "Epoch:210 elapsed time: 21.34 [s]\n",
      ">>> Epoch: 211, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.429, acc: 0.062] || [Gen loss: 0.433, acc: 0.000]\n",
      "Epoch: 211 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.463\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.037\n",
      "Epoch:211 elapsed time: 24.14 [s]\n",
      ">>> Epoch: 212, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.246, acc: 0.500] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 0.462, acc: 0.000]\n",
      "Epoch:212 elapsed time: 21.75 [s]\n",
      ">>> Epoch: 213, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.446, acc: 0.062] || [Gen loss: 0.265, acc: 0.000]\n",
      "Epoch:213 elapsed time: 21.96 [s]\n",
      ">>> Epoch: 214, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 0.139, acc: 0.000]\n",
      "Epoch:214 elapsed time: 21.48 [s]\n",
      ">>> Epoch: 215, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.178, acc: 0.000]\n",
      "Epoch:215 elapsed time: 20.94 [s]\n",
      ">>> Epoch: 216, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.351, acc: 0.062] || [Gen loss: 0.184, acc: 0.000]\n",
      "Epoch:216 elapsed time: 20.95 [s]\n",
      ">>> Epoch: 217, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.408, acc: 0.000]\n",
      "Epoch:217 elapsed time: 21.21 [s]\n",
      ">>> Epoch: 218, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.540, acc: 0.000]\n",
      "Epoch:218 elapsed time: 21.97 [s]\n",
      ">>> Epoch: 219, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.438, acc: 0.000]\n",
      "Epoch:219 elapsed time: 21.05 [s]\n",
      ">>> Epoch: 220, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.195, acc: 0.000]\n",
      "Epoch:220 elapsed time: 21.36 [s]\n",
      ">>> Epoch: 221, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.441, acc: 0.062] || [Gen loss: 0.515, acc: 0.000]\n",
      "Epoch: 221 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.400\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.033\n",
      "Epoch:221 elapsed time: 23.59 [s]\n",
      ">>> Epoch: 222, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.221, acc: 0.500] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 0.276, acc: 0.000]\n",
      "Epoch:222 elapsed time: 22.08 [s]\n",
      ">>> Epoch: 223, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.336, acc: 0.062] || [Gen loss: 0.276, acc: 0.000]\n",
      "Epoch:223 elapsed time: 21.31 [s]\n",
      ">>> Epoch: 224, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.051, acc: 0.938] || [F-Dis loss: 0.468, acc: 0.062] || [Gen loss: 0.085, acc: 0.000]\n",
      "Epoch:224 elapsed time: 21.63 [s]\n",
      ">>> Epoch: 225, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.247, acc: 0.000]\n",
      "Epoch:225 elapsed time: 21.18 [s]\n",
      ">>> Epoch: 226, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.415, acc: 0.062] || [Gen loss: 0.199, acc: 0.000]\n",
      "Epoch:226 elapsed time: 21.41 [s]\n",
      ">>> Epoch: 227, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.226, acc: 0.000]\n",
      "Epoch:227 elapsed time: 20.88 [s]\n",
      ">>> Epoch: 228, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 0.394, acc: 0.000]\n",
      "Epoch:228 elapsed time: 21.04 [s]\n",
      ">>> Epoch: 229, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.473, acc: 0.062] || [Gen loss: 0.112, acc: 0.000]\n",
      "Epoch:229 elapsed time: 21.81 [s]\n",
      ">>> Epoch: 230, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.105, acc: 0.000]\n",
      "Epoch:230 elapsed time: 22.08 [s]\n",
      ">>> Epoch: 231, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.341, acc: 0.062] || [Gen loss: 0.312, acc: 0.000]\n",
      "Epoch: 231 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.425\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.029\n",
      "Epoch:231 elapsed time: 24.83 [s]\n",
      ">>> Epoch: 232, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.230, acc: 0.500] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.353, acc: 0.000]\n",
      "Epoch:232 elapsed time: 22.09 [s]\n",
      ">>> Epoch: 233, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.357, acc: 0.062] || [Gen loss: 0.182, acc: 0.000]\n",
      "Epoch:233 elapsed time: 22.34 [s]\n",
      ">>> Epoch: 234, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.006, acc: 0.938] || [F-Dis loss: 0.518, acc: 0.062] || [Gen loss: 0.239, acc: 0.000]\n",
      "Epoch:234 elapsed time: 21.98 [s]\n",
      ">>> Epoch: 235, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.480, acc: 0.062] || [Gen loss: 0.433, acc: 0.000]\n",
      "Epoch:235 elapsed time: 22.46 [s]\n",
      ">>> Epoch: 236, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.385, acc: 0.000]\n",
      "Epoch:236 elapsed time: 21.89 [s]\n",
      ">>> Epoch: 237, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 0.148, acc: 0.000]\n",
      "Epoch:237 elapsed time: 21.11 [s]\n",
      ">>> Epoch: 238, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.280, acc: 0.000]\n",
      "Epoch:238 elapsed time: 21.68 [s]\n",
      ">>> Epoch: 239, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 0.403, acc: 0.000]\n",
      "Epoch:239 elapsed time: 20.97 [s]\n",
      ">>> Epoch: 240, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.371, acc: 0.062] || [Gen loss: 0.359, acc: 0.000]\n",
      "Epoch:240 elapsed time: 21.44 [s]\n",
      ">>> Epoch: 241, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.325, acc: 0.000]\n",
      "Epoch: 241 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 16 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.437\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.028\n",
      "Epoch:241 elapsed time: 26.11 [s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(cgen_model, cdis_model, gan_model, X_img, X_txt, y, y_labels, epo, bs, check_epochs, fn_config)\n",
    "# train_good(gen_model, dis_model, gan_model, X_img, X_txt, y, epo, bs, check_epochs, fn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}