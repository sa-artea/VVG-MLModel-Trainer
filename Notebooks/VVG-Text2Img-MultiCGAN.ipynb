{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  },
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "useful links:\n",
    "\n",
    "- Data Preparation for Variable Length Input Sequences, URL: https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "- Masking and padding with Keras, URL: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "- Step-by-step understanding LSTM Autoencoder layers, URL: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352XX, \n",
    "- Understanding input_shape parameter in LSTM with Keras, URL: https://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\n",
    "- tf.convert_to_tensor, URL: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n",
    "- ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int) in Python, URL: https://datascience.stackexchange.com/questions/82440/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type\n",
    "- How to Identify and Diagnose GAN Failure Modes, URL: https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/\n",
    "- How to Develop a GAN for Generating MNIST Handwritten Digits\n",
    ", URL: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "- How to Visualize a Deep Learning Neural Network Model in Keras\n",
    ", URL: https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- Tips for Training Stable Generative Adversarial Networks\n",
    ", URL: https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- How to Configure Image Data Augmentation in Keras, URL: https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "import datetime\n",
    "import copy\n",
    "import gc\n",
    "from statistics import mean\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# natural language processing packages\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading nlkt data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# sample handling sklearn package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "# # Keras + Tensorflow ML libraries\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers\n",
    "\n",
    "# preprocessing and processing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# shapping layers\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# basic layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# data processing layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "\n",
    "# recurrent and convolutional layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "\n",
    "# activarion function\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# optimization loss functions\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import SGD # OJO!\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam # OJO!\n",
    "from tensorflow.keras.optimizers import Adadelta # OJO!\n",
    "from tensorflow.keras.optimizers import Adagrad # OJO!\n",
    "\n",
    "# image augmentation and processing\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ==============================="
   ]
  },
  {
   "source": [
    "# FUNCTION DEFINITION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the image from file with cv2\n",
    "def read_img(img_fpn):\n",
    "    ans = cv2.imread(img_fpn, cv2.IMREAD_UNCHANGED)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction to scale the image and reduce cv2\n",
    "def scale_img(img, scale_pct):\n",
    "\n",
    "    width = int(img.shape[1]*scale_pct/100)\n",
    "    height = int(img.shape[0]*scale_pct/100)\n",
    "    dim = (width, height)\n",
    "    # resize image\n",
    "    ans = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img.astype(\"float32\")/float(rangev)\n",
    "    \n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = (img.astype(\"float32\")-rangev)/rangev\n",
    "    # ans = pd.Series(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pad the image in the center\n",
    "def pad_img(img, h, w, img_type):\n",
    "    #  in case when you have odd number\n",
    "    ans = None\n",
    "    top_pad = np.floor((h - img.shape[0]) / 2).astype(np.uint8) # floor\n",
    "    bottom_pad = np.ceil((h - img.shape[0]) / 2).astype(np.uint8)\n",
    "    right_pad = np.ceil((w - img.shape[1]) / 2).astype(np.uint8)\n",
    "    left_pad = np.floor((w - img.shape[1]) / 2).astype(np.uint8) # floor\n",
    "    # print((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "    if img_type == \"rgb\":\n",
    "        ans = np.copy(np.pad(img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode=\"constant\", constant_values=0.0))   \n",
    "    if img_type == \"bw\":\n",
    "        ans = np.copy(np.pad(img, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), mode=\"constant\", constant_values=0))\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shape(src_df, img_col, shape_col):\n",
    "\n",
    "    ans = src_df\n",
    "    src_col = list(ans[img_col])\n",
    "    tgt_col = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for data in src_col:\n",
    "        tshape = data.shape\n",
    "        tgt_col.append(tshape)\n",
    "\n",
    "    ans[shape_col] = tgt_col\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to padd the images in the dataset, needs the shape, the type of image and the src + tgt columns of the frame to work with\n",
    "def padding_images(src_df, src_col, tgt_col, max_shape, img_type):\n",
    "    # ans = src_df\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "    max_x, max_y = max_shape[0], max_shape[1]\n",
    "\n",
    "    for timg in src_images:        \n",
    "        pimg = pad_img(timg, max_y, max_x, img_type)\n",
    "        tgt_images.append(pimg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the images in in memory\n",
    "def get_images(rootf, src_df, src_col, tgt_col, scale_pct):\n",
    "    ans = src_df\n",
    "    src_files = list(ans[src_col])\n",
    "    tgt_files = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for tfile in src_files:\n",
    "        tfpn = os.path.join(rootf, tfile)\n",
    "        timg = read_img(tfpn)\n",
    "        timg = scale_img(timg, scale_pct)\n",
    "        tgt_files.append(timg)\n",
    "\n",
    "    ans[tgt_col] = tgt_files\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize the images in the dataset, it has 2 options\n",
    "def standarize_images(src_df, src_col, tgt_col, img_type, std_opt):\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "\n",
    "    for timg in src_images:\n",
    "        # pcolor image\n",
    "        if img_type == \"rgb\":\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # b&w image\n",
    "        if img_type == \"rb\":\n",
    "            timg = np.asarray(timg) #, dtype=\"uint8\")\n",
    "            timg = timg[:,:,np.newaxis]\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # std_opt affect the standarization results\n",
    "        # result 0.0 < std_timg < 1.0\n",
    "        # result -1.0 < std_timg < 1.0\n",
    "        std_timg = std_img(timg, 0, 255, std_opt)\n",
    "        tgt_images.append(std_timg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function than rotates the original image to create a new example\n",
    "def syth_rgb_img(data):\n",
    "\n",
    "    samples = expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(rotation_range=90)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"uint8\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the max shape in the image dataset\n",
    "def get_mshape(shape_data, imgt):\n",
    "\n",
    "    max_x, max_y, max_ch = 0, 0, 0\n",
    "    shape_data = list(shape_data)\n",
    "    ans = None\n",
    "\n",
    "    if imgt == \"rgb\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty, tch = tshape[0], tshape[1], tshape[2]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            if tch > max_ch:\n",
    "                max_ch = tch\n",
    "            \n",
    "        ans = (max_x, max_y, max_ch)\n",
    "    \n",
    "    elif imgt == \"bw\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty = tshape[0], tshape[1]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            \n",
    "        ans = (max_x, max_y)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the pandas df data into usable word dense vector representation, YOU NEED IT FOR THE CSV to be useful!\n",
    "def format_dvector(work_corpus):\n",
    "\n",
    "    ans = list()\n",
    "    for dvector in work_corpus:\n",
    "        dvector = eval(dvector)\n",
    "        dvector = np.asarray(dvector)\n",
    "        ans.append(dvector)\n",
    "    ans = np.asarray(ans, dtype=\"object\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct to concatenate all label columns into one for a single y in ML training, returns a list\n",
    "def concat_labels(row, cname):\n",
    "\n",
    "    ans = list()\n",
    "    for c in cname:\n",
    "        r = row[c]\n",
    "        r = eval(r)\n",
    "        ans = ans + r\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the ML model\n",
    "def save_model(model, m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    # print(fpn)\n",
    "    model.save(fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the ML model\n",
    "def load_model(m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    model = keras.models.load_model(fpn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cast dataframe and avoid problems with keras\n",
    "def cast_batch(data):\n",
    "\n",
    "    cast_data = list()\n",
    "\n",
    "    if len(data) >= 2:\n",
    "\n",
    "        for d in data:\n",
    "            d = np.asarray(d).astype(\"float32\")\n",
    "            cast_data.append(d)\n",
    "\n",
    "    return cast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select real elements to train the discriminator\n",
    "def gen_real_samples(data, sample_size, half_batch):\n",
    "\n",
    "    real_data = list()\n",
    "    rand_index = np.random.randint(0, sample_size, size=half_batch)\n",
    "\n",
    "    # need at leas X, y\n",
    "    # posible combinations are:\n",
    "    # X_img/X_txt, y\n",
    "    # X_img/X_txt, X_labels, y\n",
    "    # X_img, X_txt, X_labels, y\n",
    "    if len(data) >= 2:\n",
    "        # selectinc the columns in the dataset\n",
    "        for d in data:\n",
    "            # td_real = d[rand_index]\n",
    "            td_real = copy.deepcopy(d[rand_index])\n",
    "            real_data.append(td_real)\n",
    "\n",
    "    # casting data\n",
    "    real_data = cast_batch(real_data)\n",
    "\n",
    "    return real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create fake elements to train the discriminator\n",
    "def gen_fake_samples(gen_model, dataset_shape, half_batch):\n",
    "\n",
    "    # fake data\n",
    "    fake_data = None\n",
    "    # conditional labels for the gan model\n",
    "    conditional = dataset_shape.get(\"conditioned\")\n",
    "    # configuratin keys for the generator\n",
    "    latent_dims = dataset_shape.get(\"latent_dims\")\n",
    "    cat_shape = dataset_shape.get(\"cat_shape\")\n",
    "    label_shape = dataset_shape.get(\"label_shape\")\n",
    "    data_cols = dataset_shape.get(\"data_cols\")\n",
    "\n",
    "    # generator config according to the dataset\n",
    "    # X:images -> y:Real/Fake\n",
    "    if data_cols == 2:\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # random generated image from the model\n",
    "        Xi_fake = gen_model.predict(latent_space)\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, y_fake)\n",
    "\n",
    "    # X_img, X_labels(classification), y (fake/real)\n",
    "    elif (conditional == True) and data_cols == 3:\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_fake = gen_fake_labels(label_shape, half_batch)\n",
    "\n",
    "        # random generated image from the model\n",
    "        Xi_fake = gen_model.predict([latent_space, Xl_fake])\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, Xl_fake, y_fake)\n",
    "\n",
    "    elif (conditional == False) and data_cols == 3:\n",
    "        \n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # random generated image + text from the model\n",
    "        Xi_fake, Xt_fake = gen_model.predict(latent_space)\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, Xt_fake, y_fake)\n",
    "\n",
    "    # X_img(rgb), X_txt(text), X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 4:\n",
    "\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_fake = gen_fake_labels(label_shape, half_batch)\n",
    "\n",
    "        # random generated image from the model\n",
    "        Xi_fake, Xt_fake = gen_model.predict([latent_space, Xl_fake])\n",
    "        # fake samples \n",
    "        fake_data = (Xi_fake, Xt_fake, Xl_fake, y_fake)\n",
    "\n",
    "    # casting data type\n",
    "    fake_data = cast_batch(fake_data)\n",
    "    \n",
    "    return fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create inputs to updalte the GAN generator\n",
    "def gen_latent_data(dataset_shape, batch_size):\n",
    "\n",
    "    # latent data\n",
    "    latent_data = None\n",
    "\n",
    "    # conditional labels for the gan model\n",
    "    conditional = dataset_shape.get(\"conditioned\")\n",
    "    # configuratin keys for the generator\n",
    "    latent_dims = dataset_shape.get(\"latent_dims\")\n",
    "    cat_shape = dataset_shape.get(\"cat_shape\")\n",
    "    label_shape = dataset_shape.get(\"label_shape\")\n",
    "    data_cols = dataset_shape.get(\"data_cols\")\n",
    "\n",
    "    # generator config according to the dataset\n",
    "    # X:images -> y:Real/Fake\n",
    "    if data_cols == 2:\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # fake samples\n",
    "        latent_data = (latent_space, y_gen)\n",
    "\n",
    "    # X_img, X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 3 and (conditional == True):\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_gen = gen_fake_labels(label_shape, batch_size)\n",
    "        # gen samples\n",
    "        latent_data = (latent_space, Xl_gen, y_gen)\n",
    "\n",
    "    elif data_cols == 3 and (conditional == False):\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # fake samples\n",
    "        latent_data = (latent_space, y_gen)\n",
    "\n",
    "    # X_img(rgb), X_txt(text), X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 4:\n",
    "        # random textual latent space \n",
    "        latent_space = gen_latent_space(latent_dims, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_gen = gen_fake_labels(label_shape, batch_size)\n",
    "        # gen samples\n",
    "        latent_data = (latent_space, Xl_gen, y_gen)\n",
    "\n",
    "    return latent_data\n",
    "# latent_gen = gen_latent_txt(latent_shape, batch_size)\n",
    "# create inverted category for the fake noisy text\n",
    "# y_gen = get_fake_positive(cat_shape[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate random/latent text for the GAN generator\n",
    "def gen_latent_space(latent_dims, n_samples):\n",
    "\n",
    "    ans = None\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # noise = np.random.normal(0.0, 1.0, size=latent_shape)\n",
    "        # noise = np.random.normal(0.0, 1.0, size=latent_dims)\n",
    "        # noise = np.random.normal(0.5, 0.25, size=latent_shape)\n",
    "        # noise = np.random.uniform(low=0.0, high=1.0, size=latent_shape)\n",
    "        # noise = np.random.randn(latent_shape[0], latent_shape[1])\n",
    "        noise = np.random.randn(latent_dims)\n",
    "        if ans is None:\n",
    "            txt = np.expand_dims(noise, axis=0)\n",
    "            ans = txt\n",
    "        else:\n",
    "            txt = np.expand_dims(noise, axis=0)\n",
    "            ans = np.concatenate((ans, txt), axis=0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfunction to smooth the fake positives\n",
    "def smooth_positives(y):\n",
    "\treturn y - 0.3 + (np.random.random(y.shape)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to smooth the fake negatives\n",
    "def smooth_negatives(y):\n",
    "\treturn y + np.random.random(y.shape)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to smooth the data labels\n",
    "def smooth_labels(y):\n",
    "    # label smoothing formula\n",
    "    # alpha: 0.0 -> original distribution, 1.0 uniform distribution\n",
    "    # K: number of label classes\n",
    "    # y_ls = (1 - alpha) * y_hot + alpha / K\n",
    "    alpha = 0.3\n",
    "    K = y[0].shape[0]\n",
    "    ans = (1-alpha)*y + alpha/K\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake true categories for the generator\n",
    "def gen_fake_posclass(cat_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_shape[0])\n",
    "    ans = np.ones(sz)\n",
    "    # smoothing fakes\n",
    "    ans = smooth_positives(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake negative category to train the GAN\n",
    "def gen_fake_negclass(cat_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_shape[0])\n",
    "    ans = np.zeros(sz)\n",
    "    ans = smooth_negatives(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate fake labels to train the GAN\n",
    "def gen_fake_labels(label_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, label_shape[0])\n",
    "    ans = np.random.randint(0,1, size=sz)\n",
    "    ans = smooth_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create text similar to the original one with 5% of noise\n",
    "def syth_text(data, nptc=0.05):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetizing a noisy std image from real data\n",
    "def syth_std_img(data):\n",
    "\n",
    "    samples = np.expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=10)\n",
    "    # datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True, vertical_flip=True)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"float32\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create new categories with some noise, default 5%\n",
    "def syth_categories(data, nptc=0.05):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to artificially span a batch with some noise and alterations by an specific number\n",
    "# TODO fix because this is an old version with no flexibility\n",
    "def expand_samples(data, synth_batch):\n",
    "\n",
    "    X_txt = data[0]\n",
    "    X_img = data[1]\n",
    "    y = data[2]\n",
    "    labels = data[3]\n",
    "\n",
    "    # creating the exapnded batch response\n",
    "    Xe_txt, Xe_img, ye, lbe = None, None, None, None\n",
    "\n",
    "    # iterating in the original batch\n",
    "    for Xtt, Xit, yt, lb in zip(X_txt, X_img, y, labels):\n",
    "\n",
    "        # temporal synth minibatch per original image\n",
    "        synth_Xt, synth_Xi, synth_y, synth_lb = None, None, None, None\n",
    "\n",
    "        # synthetizing artificial data for the batch\n",
    "        for i in range(synth_batch):\n",
    "\n",
    "            # generating first element\n",
    "            if (synth_Xt is None) and (synth_Xi is None) and (synth_y is None) and (synth_lb is None):\n",
    "                # gen text\n",
    "                gen_Xt = copy.deepcopy(Xtt)\n",
    "                gen_Xt = np.array(gen_Xt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = gen_Xt\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = gen_Xi\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = gen_yt\n",
    "\n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = gen_lb\n",
    "\n",
    "            # generatin the rest of the elements\n",
    "            else:\n",
    "                # gen text\n",
    "                gen_Xt = syth_text(Xtt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = np.concatenate((synth_Xt, gen_Xt), axis=0)\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = np.concatenate((synth_Xi, gen_Xi), axis=0)\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = np.concatenate((synth_y, gen_yt), axis=0)\n",
    "        \n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = np.concatenate((synth_lb, gen_lb), axis=0)\n",
    "\n",
    "        # adding the first part to the training batch\n",
    "        if (Xe_txt is None) and (Xe_img is None) and (ye is None) and (lbe is None):\n",
    "            # adding text\n",
    "            Xe_txt = synth_Xt\n",
    "            # adding images\n",
    "            Xe_img = synth_Xi\n",
    "            # adding categories\n",
    "            ye = synth_y\n",
    "            # adding labels\n",
    "            lbe = synth_lb\n",
    "\n",
    "        # adding the rest of the batch\n",
    "        else:\n",
    "            # adding text\n",
    "            Xe_txt = np.concatenate((Xe_txt, synth_Xt), axis=0)\n",
    "            # adding images\n",
    "            Xe_img = np.concatenate((Xe_img, synth_Xi), axis=0)\n",
    "            # adding category\n",
    "            ye = np.concatenate((ye, synth_y), axis=0)\n",
    "            # adding labels\n",
    "            lbe = np.concatenate((lbe, synth_lb), axis=0)\n",
    "\n",
    "    Xe_txt, Xe_img, ye, lbe = cast_batch(Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    e_data = (Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    return e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drift_labels(Xt_real, Xi_real, y_real, Xt_fake, Xi_fake, y_fake, batch_size, drift_pct):\n",
    "def drift_labels(real_data, fake_data, batch_size, drift_pct):\n",
    "\n",
    "    # setting the size for the drift labels\n",
    "    drift_size = int(math.ceil(drift_pct*batch_size))\n",
    "    # random index for drift elements!!!\n",
    "    rand_drifters = np.random.choice(batch_size, size=drift_size, replace=False)\n",
    "    # print(\"batch size\", batch_size, \"\\nrandom choise to change\", drift_size, \"\\n\", rand_drifters)\n",
    "\n",
    "    # if the dataset has at leas X, y... NEED TO PASS A GOOD ORDER\n",
    "    if (len(real_data) and len(fake_data)) >= 2:\n",
    "\n",
    "        # iterating over the random choose index\n",
    "        for drift in rand_drifters:\n",
    "\n",
    "            # taking one real + fake column at a time\n",
    "            # X_img/txt, y\n",
    "            # X_img/txt, X_labels, y\n",
    "            # X_img, X_txt, X_labels, y\n",
    "            for real_col, fake_col in zip(real_data, fake_data):\n",
    "                # copying real data in temporal var\n",
    "                temp_drift = copy.deepcopy(real_col[drift])\n",
    "                # replacing real with fakes\n",
    "                real_col[drift] = copy.deepcopy(fake_col[drift])\n",
    "                # updating fakes with temporal original\n",
    "                fake_col[drift] = temp_drift\n",
    "\n",
    "    return real_data, fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def inv_std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = img+rangev\n",
    "        ans = ans*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes the ideas array, shape and configuration to render them into human understandable lenguage\n",
    "# it select n number of ideas and plot them, for images, for text and for both\n",
    "def plot_ideas(ideas, train_cfg, test_cfg):\n",
    "\n",
    "    # get the index of random ideas in the set\n",
    "    ideas_size = test_cfg.get(\"batch_size\")\n",
    "    gen_samples = test_cfg.get(\"gen_sample_size\")\n",
    "    data_cols = train_cfg.get(\"data_cols\")\n",
    "\n",
    "    # choosing non repeated ideas in the set\n",
    "    rand_index = np.random.choice(ideas_size, size=gen_samples*gen_samples, replace=False)\n",
    "    # print(rand_index)\n",
    "    # print(\"ojo!!!!\", len(ideas))\n",
    "    # if the ideas are images or text\n",
    "    if len(ideas) == 1:\n",
    "        # print(\"data_cols:\", data_cols)\n",
    "        data = ideas[0]\n",
    "        current_shape = data[0].shape\n",
    "        # print(\"idea current_shape:\", ideas.shape)\n",
    "        # print(\"idea current_shape:\", current_shape)\n",
    "\n",
    "        if current_shape == train_cfg.get(\"img_shape\"):\n",
    "            render_painting(data, rand_index, train_cfg, test_cfg)\n",
    "\n",
    "        elif current_shape == train_cfg.get(\"txt_shape\"):\n",
    "            render_wordcloud(data, rand_index, train_cfg, test_cfg)\n",
    "\n",
    "    # if the ideas are images + text\n",
    "    elif len(ideas) == 2:\n",
    "        data_img = ideas[0]\n",
    "        data_txt = ideas[1]\n",
    "        render_painting(data_img, rand_index, train_cfg, test_cfg)\n",
    "        render_wordcloud(data_txt, rand_index, train_cfg, test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the selected ideas and transform them into pytlot objects\n",
    "def render_painting(ideas, rand_index, train_cfg, test_cfg):\n",
    "\n",
    "    # get important data for iterating\n",
    "    n_sample = test_cfg.get(\"gen_sample_size\")\n",
    "    report_fp_name = test_cfg.get(\"report_fn_path\")\n",
    "    epoch = test_cfg.get(\"current_epoch\")\n",
    "\n",
    "    # prep the figure\n",
    "    fig, ax = plt.subplots(n_sample, n_sample, figsize=(20,20))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n_sample*n_sample):\n",
    "        # define subplot\n",
    "        plt.subplot(n_sample, n_sample, 1+i)\n",
    "\n",
    "        # getting the images from sample\n",
    "        rand_i = rand_index[i]\n",
    "        gimg = ideas[rand_i]\n",
    "        gimg = inv_std_img(gimg, 0, 255, \"ctr\")\n",
    "\n",
    "        # turn off axis\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(gimg) #, interpolation=\"nearest\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"GENERATED PAINTINGS\", fontsize=50)\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-Gen-img-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, \"img\", plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the selected ideas and translate them into pytplot objects\n",
    "def render_wordcloud(ideas, rand_index, train_cfg, test_cfg):\n",
    "\n",
    "    # get important data for iterating\n",
    "    n_sample = test_cfg.get(\"n_samples\")\n",
    "    lexicon = train_cfg.get(\"bow_lexicon\")\n",
    "    tfidf_tokens = train_cfg.get(\"tfidf_lexicon\")\n",
    "    # get important data for iterating\n",
    "    n_sample = test_cfg.get(\"gen_sample_size\")\n",
    "    report_fp_name = test_cfg.get(\"report_fn_path\")\n",
    "    epoch = test_cfg.get(\"current_epoch\")\n",
    "    default = {\"no\":1, \"words\":1}\n",
    "\n",
    "    # prep the figure\n",
    "    fig, ax = plt.subplots(n_sample,n_sample, figsize=(20,20))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n_sample*n_sample):\n",
    "        # define subplot\n",
    "        plt.subplot(n_sample, n_sample, 1+i)\n",
    "        \n",
    "        # getting the images from sample\n",
    "        rand_i = rand_index[i]\n",
    "        gtxt = ideas[rand_i]\n",
    "        gtxt = translate_from_lexicon(gtxt, tfidf_tokens, lexicon)\n",
    "\n",
    "        wordcloud = WordCloud(max_font_size=100,\n",
    "                                min_font_size=10,\n",
    "                                max_words=100,\n",
    "                                min_word_length=1,\n",
    "                                relative_scaling = 0.5,\n",
    "                                width=600, height=400,\n",
    "                                background_color=\"white\",\n",
    "                                random_state=42)\n",
    "        if len(gtxt) == 0:\n",
    "            gtxt = default\n",
    "        \n",
    "        wordcloud.generate_from_frequencies(frequencies=gtxt)\n",
    "        # plt.figure()\n",
    "\n",
    "        # turn off axis\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\") #, interpolation=\"nearest\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"GENERATED WORDS\", fontsize=50)\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-Gen-txt-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, \"txt\", plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads the model known lexicon into the a dictionary for the world cloud to translate\n",
    "def load_lexicon(lexicon_fp):\n",
    "\n",
    "    lexicon = gensim.corpora.Dictionary.load(lexicon_fp)\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the idtf dense word vector representacion and translate it to human lenguage using the kown lexicon\n",
    "def translate_from_lexicon(tfidf_corpus, tfidf_dict, lexicon):\n",
    "\n",
    "    wordcloud = dict()\n",
    "\n",
    "    bow_corpus = tfidf2bow(tfidf_corpus, tfidf_dict)\n",
    "    wordcloud = bow2words(bow_corpus, lexicon)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate from tfidf token representation to bow representation\n",
    "def tfidf2bow(tfidf_corpus, tfidf_dict):\n",
    "\n",
    "    bows = dict()\n",
    "    tfidf_corpus = np.asarray(tfidf_corpus, dtype=\"float32\")\n",
    "    # print(type(tfidf_corpus))\n",
    "\n",
    "    for tfidf_doc in tfidf_corpus:\n",
    "\n",
    "        for tfidf_token in tfidf_doc:\n",
    "\n",
    "            bows = get_similars(tfidf_token, bows, tfidf_dict)\n",
    "\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stablish if the tfidf representation of a token is similar to the one in the tfidf dictionar\n",
    "def get_similars(tfidf_token, bows, tfidf_dict):\n",
    "\n",
    "    ans = bows\n",
    "    ans = isclose_in(tfidf_token, bows, tfidf_dict)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function return the similar values of the tfidf value with a token id and a count\n",
    "def isclose_in(token, token_dict, cmp_tokens, tol=0.0001):\n",
    "\n",
    "    for tcmp in cmp_tokens:\n",
    "        for key, value in tcmp.items():\n",
    "\n",
    "            if math.isclose(token, value, rel_tol=tol) and (key not in token_dict.keys()):\n",
    "                token_dict.update({key:1})\n",
    "            \n",
    "            elif math.isclose(token, value, rel_tol=tol) and (key in token_dict.keys()):\n",
    "                count = token_dict[key]\n",
    "                count = count + 1\n",
    "                token_dict.update({key:count})\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow2words(bow_txt, lexicon):\n",
    "\n",
    "    words = dict()\n",
    "\n",
    "    for key, value in bow_txt.items():\n",
    "        token = lexicon.get(key)\n",
    "        td = {token:value}\n",
    "        # word = id2token.get(key)\n",
    "        # td = {word:value}\n",
    "        words.update(td)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the generated images within a training epoch\n",
    "def plot_gen_images(examples, epoch, report_fp_name, n_sample):\n",
    "\n",
    "    # get important data for iterating\n",
    "    example_size = examples.shape[0]\n",
    "    og_shape = examples[0].shape\n",
    "    rand_img = np.random.choice(example_size, size=n_sample*n_sample, replace=False) \n",
    "    # (0, example_size, size=n_sample*n_sample)\n",
    "\n",
    "    # prep the figure\n",
    "    fig, ax = plt.subplots(n_sample,n_sample, figsize=(20,20))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n_sample*n_sample):\n",
    "        # define subplot\n",
    "        plt.subplot(n_sample, n_sample, 1+i)\n",
    "\n",
    "        # getting the images from sample\n",
    "        rand_i = rand_img[i]\n",
    "        gimg = examples[rand_i]\n",
    "        gimg = inv_std_img(gimg, 0, 255, \"ctr\")\n",
    "\n",
    "        # turn off axis\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(gimg) #, interpolation=\"nearest\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"GENERATED PAINTINGS\", fontsize=50)\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-Gen-img-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, \"img\", plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_metrics(disr_hist, disf_hist, gan_hist, report_fp_name, epoch):\n",
    "\n",
    "    # reporting results\n",
    "    disr_hist = np.array(disr_hist)\n",
    "    disf_hist = np.array(disf_hist)\n",
    "    gan_hist = np.array(gan_hist)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # loss\n",
    "    ax1.plot(disr_hist[:,1], \"royalblue\", label=\"Loss: R-Dis\")\n",
    "    ax1.plot(disf_hist[:,1], \"crimson\", label=\"Loss: F-Dis\")\n",
    "    ax1.plot(gan_hist[:,1], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "    # ax1.plot(gan_hist[:], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "\n",
    "    # acc_\n",
    "    ax2.plot(disr_hist[:,0], \"royalblue\", label=\"Acc: R-Dis\")\n",
    "    ax2.plot(disf_hist[:,0], \"crimson\", label=\"Acc: F-Dis\")\n",
    "    ax2.plot(gan_hist[:,0], \"blueviolet\", label=\"Acc: GAN/Gen\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"LEARNING BEHAVIOR\", fontsize=20)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax1.set(xlabel = \"Epoch [cycle]\", ylabel = \"Loss\")\n",
    "    ax2.set(xlabel = \"Epoch [cycle]\", ylabel = \"Acc\")\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-learn-curve-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, \"learn\", plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the loss and accuracy avg in multiple batchs of an epoch\n",
    "def epoch_avg(log):\n",
    "    loss, acc = None, None\n",
    "\n",
    "    # if acc and loss are present to avg\n",
    "    if type(log[0]) is list:\n",
    "        if len(log) > 0:\n",
    "\n",
    "            acc_list = list()\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                ta = l[0]\n",
    "                tl = l[1]\n",
    "\n",
    "                acc_list.append(ta)\n",
    "                loss_list.append(tl)\n",
    "\n",
    "            loss, acc = mean(loss_list), mean(acc_list)\n",
    "        return loss, acc\n",
    "    \n",
    "    else:\n",
    "        # if only loss is present\n",
    "        if len(log) > 0:\n",
    "\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                loss_list.append(l)\n",
    "\n",
    "            loss = mean(loss_list)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model, needs the dirpath, the name and the datetime to save\n",
    "def export_model(model, models_fp_name, filename, datetime):\n",
    "\n",
    "    ss = True\n",
    "    sln = True\n",
    "    fext = \"png\"\n",
    "    fpn = filename + \"-\" + datetime\n",
    "    fpn = filename + \".\" + fext\n",
    "    fpn = os.path.join(models_fp_name, fpn)\n",
    "    plot_model(model, to_file=fpn, show_shapes=ss, show_layer_names=sln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format data to save in file\n",
    "def format_metrics(disr_history, disf_history, gan_history):\n",
    "\n",
    "    headers, data = None, None\n",
    "\n",
    "    disr_hist = np.array(disr_history)\n",
    "    disf_hist = np.array(disf_history)\n",
    "    gan_hist = np.array(gan_history)\n",
    "\n",
    "    # formating file headers\n",
    "    headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\", \"gen_gan_acc\"]\n",
    "    # headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\",] # \"gen_gan_acc\"]\n",
    "\n",
    "    # formating fake discriminator train data\n",
    "    drhl = disr_hist[:,1]\n",
    "    drha = disr_hist[:,0]\n",
    "\n",
    "    # formating real discrimintator train data\n",
    "    dfhl = disf_hist[:,1]\n",
    "    dfha = disf_hist[:,0]\n",
    "\n",
    "    # formating gan/gen train data\n",
    "    # gghl = gan_hist[:]# .flatten()\n",
    "    gghl = gan_hist[:,1]\n",
    "    ggha = gan_hist[:,0]\n",
    "\n",
    "    # adding all formatted data into list\n",
    "    data = np.column_stack((drhl, drha, dfhl, dfha, gghl, ggha))\n",
    "    # data = np.column_stack((drhl, drha, dfhl, dfha, gghl)) #, ggha))\n",
    "\n",
    "    return data, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write data in csv file\n",
    "def write_metrics(data, headers, report_fn_path, filename):\n",
    "\n",
    "    # print(report_fn_path, filename)\n",
    "    fpn = filename + \"-train-history.csv\"\n",
    "    fpn = os.path.join(report_fn_path, fpn)\n",
    "\n",
    "    history_df = pd.DataFrame(data, columns=headers)\n",
    "    tdata = history_df.to_csv(\n",
    "                            fpn,\n",
    "                            sep=\",\",\n",
    "                            index=False,\n",
    "                            encoding=\"utf-8\",\n",
    "                            mode=\"w\",\n",
    "                            quoting=csv.QUOTE_ALL\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to safe the loss/acc logs in training for the gan/gen/dis models\n",
    "def save_metrics(disr_history, disf_history, gan_history, report_fn_path, filename):\n",
    "\n",
    "    data, headers = format_metrics(disr_history, disf_history, gan_history)\n",
    "    write_metrics(data, headers, report_fn_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to know the time between epochs or batchs it return the new time for a new calculation\n",
    "def lapse_time(last_time, epoch):\n",
    "\n",
    "    now_time = datetime.datetime.now()\n",
    "    deltatime = now_time - last_time\n",
    "    deltatime = deltatime.total_seconds()\n",
    "    deltatime = \"%.2f\" % deltatime\n",
    "    msg = \"Epoch:%3d \" % int(epoch+1)\n",
    "    msg = msg + \"elapsed time: \" + str(deltatime) + \" [s]\"\n",
    "    print(msg)\n",
    "    return now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test the model while training\n",
    "def test_model(gen_model, dis_model, data, data_shape, train_cfg, test_cfg): \n",
    "\n",
    "    dataset_size = test_cfg.get(\"dataset_size\")\n",
    "    batch_size = test_cfg.get(\"batch_size\")\n",
    "    synth_batch = test_cfg.get(\"synth_batch\")\n",
    "    epoch = int(test_cfg.get(\"current_epoch\"))\n",
    "    report_fn_path = test_cfg.get(\"report_fn_path\")\n",
    "    gen_samples = test_cfg.get(\"gen_sample_size\") \n",
    "    balance_batch = test_cfg.get(\"balance_batch\")\n",
    "    split_batch = int(batch_size/2)\n",
    "\n",
    "    # select real txt2img for discrimintator\n",
    "    real_data = gen_real_samples(data, dataset_size, batch_size)\n",
    "\n",
    "    # create false txt for txt2img for generator\n",
    "    fake_data = gen_fake_samples(gen_model, data_shape, batch_size)\n",
    "\n",
    "    # expand the training sample for the discriminator\n",
    "    if synth_batch > 1:\n",
    "        real_data = expand_samples(real_data, synth_batch)\n",
    "        fake_data = expand_samples(fake_data, synth_batch)\n",
    "\n",
    "    # balance training samples for the discriminator\n",
    "    if balance_batch == True:\n",
    "        real_data = balance_samples(real_data)\n",
    "        fake_data = balance_samples(fake_data)\n",
    "        \n",
    "    # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "    # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "\n",
    "    # plotting gen ideas\n",
    "    # ideas = (fake_data[0], fake_data[1])\n",
    "    # plot_ideas(ideas, train_cfg, test_cfg)\n",
    "\n",
    "    # test metrics\n",
    "    test_real, test_fake = None, None\n",
    "\n",
    "    # 1 output, img or txt\n",
    "    if len(data) == 2:\n",
    "        ideas = (fake_data[0],)\n",
    "        print(len(ideas))\n",
    "        plot_ideas(ideas, train_cfg, test_cfg)\n",
    "        test_real, test_fake = test_gan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    # 2 output, img + txt and labels conditioned\n",
    "    elif len(data) == 3 and data_shape.get(\"conditioned\") == True:\n",
    "        ideas = (fake_data[0],)\n",
    "        plot_ideas(ideas, train_cfg, test_cfg)\n",
    "        test_real, test_fake = test_cgan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    # 2 output, img + txt unconditioned\n",
    "    elif len(data) == 3 and data_shape.get(\"conditioned\") == False:\n",
    "        ideas = (fake_data[0], fake_data[1])\n",
    "        plot_ideas(ideas, train_cfg, test_cfg)\n",
    "        test_real, test_fake = test_multi_gan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    # 2 outputs, img + txt and label conditioned\n",
    "    elif len(data) == 4:\n",
    "        ideas = (fake_data[0], fake_data[1])\n",
    "        plot_ideas(ideas, train_cfg, test_cfg)\n",
    "        test_real, test_fake = test_multi_cgan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    # summarize discriminator performance\n",
    "    print(\"Batch Size %d -> Samples: Fake: %d & Real: %d\" % (batch_size*synth_batch, split_batch, split_batch))\n",
    "    print(\">>> Test Fake -> Acc: %.3f || Loss: %.3f\" % (test_fake[1], test_fake[0]))\n",
    "    print(\">>> Test Real -> Acc: %.3f || Loss: %.3f\" % (test_real[1], test_real[0]))\n",
    "    # print(\">>> Test Gen -> Acc: %.3f || Loss: %.3f\" % (test_cgen[1], test_cgen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special function to train the GAN\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "# def train(gen_model, dis_model, gan_model, X_img, X_txt, y, labels, epochs, batch_size, save_intervas, fn_config):\n",
    "def training_model(gen_model, dis_model, gan_model, data, train_cfg): # epochs, batch_size, save_intervas, fn_config\n",
    "\n",
    "    # sample size\n",
    "    dataset_size = train_cfg.get(\"dataset_size\")\n",
    "\n",
    "    # data shape for the generator\n",
    "    data_shape = {\n",
    "        \"latent_dims\": train_cfg.get(\"latent_dims\"),\n",
    "        \"cat_shape\": train_cfg.get(\"cat_shape\"),\n",
    "        \"txt_shape\": train_cfg.get(\"txt_shape\"),\n",
    "        \"label_shape\": train_cfg.get(\"label_shape\"),\n",
    "        \"conditioned\": train_cfg.get(\"conditioned\"),\n",
    "        \"data_cols\": train_cfg.get(\"data_cols\"),\n",
    "        }\n",
    "    # print(data_shape)\n",
    "\n",
    "    # augmentation factor\n",
    "    synth_batch = train_cfg.get(\"synth_batch\")\n",
    "    balance_batch = train_cfg.get(\"balance_batch\")\n",
    "    n = train_cfg.get(\"gen_sample_size\")\n",
    "    # trained_epochs = train_cfg.get(\"trained_epochs\")\n",
    "    epochs = train_cfg.get(\"max_epochs\")\n",
    "    # epochs = trained_epochs + max_epochs\n",
    "    batch_size = train_cfg.get(\"batch_size\")\n",
    "    half_batch = int(batch_size/2)\n",
    "    batch_per_epoch = int(dataset_size/batch_size)\n",
    "    # fake/real batch division\n",
    "    real_batch = int((batch_size*synth_batch)/2)\n",
    "\n",
    "    # train config\n",
    "    model_fn_path = train_cfg.get(\"models_fn_path\")\n",
    "    report_fn_path = train_cfg.get(\"report_fn_path\")\n",
    "    dis_model_name = train_cfg.get(\"dis_model_name\")\n",
    "    gen_model_name = train_cfg.get(\"gen_model_name\")\n",
    "    gan_model_name = train_cfg.get(\"gan_model_name\")\n",
    "    check_intervas = train_cfg.get(\"check_epochs\")\n",
    "    save_intervas = train_cfg.get(\"save_epochs\")\n",
    "    max_models = train_cfg.get(\"max_models\")\n",
    "    pretrain = train_cfg.get(\"pretrained\")\n",
    "    learning_history = train_cfg.get(\"learning_history\")\n",
    "\n",
    "\t# prepare lists for storing stats each epoch\n",
    "    disf_hist, disr_hist, gan_hist = list(), list(), list()\n",
    "\n",
    "    # if is pretrained\n",
    "    # if learning_history != None:\n",
    "    #     disf_hist, disr_hist, gan_hist = learning_history[0], learning_history[1], learning_history[2]\n",
    "\n",
    "    train_time = None\n",
    "\n",
    "    # train dict config\n",
    "    test_cfg = {\n",
    "        \"report_fn_path\": report_fn_path,\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"synth_batch\": synth_batch,\n",
    "        \"gen_sample_size\": train_cfg.get(\"gen_sample_size\"),\n",
    "        \"current_epoch\": None,\n",
    "    }\n",
    "    # ep = trained_epochs\n",
    "    # iterating in training epochs:\n",
    "    for ep in range(epochs+1):\n",
    "        # epoch logs\n",
    "        ep_disf_hist, ep_disr_hist, ep_gan_hist = list(), list(), list()\n",
    "        train_time = datetime.datetime.now()\n",
    "\n",
    "        # iterating over training batchs\n",
    "        for batch in range(batch_per_epoch):\n",
    "\n",
    "            # select real txt2img for discrimintator\n",
    "            real_data = gen_real_samples(data, dataset_size, half_batch)\n",
    "            # create false txt for txt2img for generator\n",
    "            fake_data = gen_fake_samples(gen_model, data_shape, half_batch)\n",
    "\n",
    "            # expand the training sample for the discriminator\n",
    "            if synth_batch > 1:\n",
    "                real_data = expand_samples(real_data, synth_batch)\n",
    "                fake_data = expand_samples(fake_data, synth_batch)\n",
    "\n",
    "            # balance training samples for the discriminator\n",
    "            if balance_batch == True:\n",
    "                real_data = balance_samples(real_data)\n",
    "                fake_data = balance_samples(fake_data)\n",
    "\n",
    "            # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "            # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "            # print(real_data[0].shape, fake_data[0].shape)\n",
    "            # print(real_data[1].shape, fake_data[1].shape)\n",
    "            # drift labels to confuse the model\n",
    "            real_data, fake_data = drift_labels(real_data, fake_data, half_batch, 0.05)\n",
    "\n",
    "            # TODO transfor this in 1 function train_model()...\n",
    "            dhf, dhr, gh = None, None, None\n",
    "\n",
    "            if len(data) == 2:\n",
    "                dhr, dhf, gh = train_gan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 3 and data_shape.get(\"conditioned\") == True:\n",
    "                dhr, dhf, gh = train_cgan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 3 and data_shape.get(\"conditioned\") == False:\n",
    "                # TODO need to implement this function!!!\n",
    "                dhr, dhf, gh = train_multi_gan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 4:\n",
    "                dhr, dhf, gh = train_multi_cgan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            # epoch log\n",
    "            ep_disr_hist.append(dhr)\n",
    "            ep_disf_hist.append(dhf)\n",
    "            ep_gan_hist.append(gh)\n",
    "\n",
    "\t\t\t# print('>%d, %d/%d, dis_=%.3f, gen=%.3f' % (ep+1, batch+1, bat_per_epo, dis_history, gen_history))\n",
    "            log_msg = \">>> Epoch: %d, B/Ep: %d/%d, Batch S: %d\" %(ep+1, batch+1, batch_per_epoch, batch_size*synth_batch)\n",
    "            log_msg = \"%s -> [R-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhr[0], dhr[1])\n",
    "            log_msg = \"%s || [F-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhf[0], dhf[1])\n",
    "            log_msg = \"%s || [Gen loss: %.3f, acc: %.3f]\" % (log_msg, gh[0], gh[1])\n",
    "            print(log_msg)\n",
    "\n",
    "        # record history for epoch\n",
    "        disr_hist.append(epoch_avg(ep_disr_hist))\n",
    "        disf_hist.append(epoch_avg(ep_disf_hist))\n",
    "        gan_hist.append(epoch_avg(ep_gan_hist))\n",
    "        test_cfg[\"current_epoch\"] = ep\n",
    "        \n",
    "\t\t# evaluate the model performance sometimes\n",
    "        if (ep) % check_intervas == 0:\n",
    "            print(\"Epoch:\", ep+1, \"Testing model training process...\")\n",
    "            \n",
    "            # test_model(gen_model, dis_model, data, data_shape, test_cfg) #, synth_batch)\n",
    "            test_model(gen_model, dis_model, data, data_shape, train_cfg, test_cfg)\n",
    "            print(\"Ploting results\")\n",
    "            plot_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, ep)\n",
    "            save_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, gan_model_name)\n",
    "\n",
    "\t\t# saving the model sometimes\n",
    "        if (ep) % save_intervas == 0:\n",
    "            print(\"Epoch:\", ep+1, \"Saving the training progress...\")\n",
    "            save_models(dis_model, gen_model, gan_model, train_cfg, test_cfg)\n",
    "            clear_models(train_cfg, test_cfg)\n",
    "        \n",
    "        train_time = lapse_time(train_time, ep)\n",
    "        # ep = ep + 1\n",
    "\n",
    "    # updating training epochs\n",
    "    # train_cfg[\"trained_epochs\"] = epochs\n",
    "    # train_cfg[\"learning_history\"] = learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(dis_model, gen_model, gan_model, train_cfg, test_cfg):\n",
    "\n",
    "    ep = test_cfg.get(\"current_epoch\") # = ep\n",
    "    epoch_sufix = \"-epoch%d\" % int(ep)\n",
    "\n",
    "    model_fn_path = train_cfg.get(\"models_fn_path\")\n",
    "    dis_model_name = train_cfg.get(\"dis_model_name\")\n",
    "    gen_model_name = train_cfg.get(\"gen_model_name\")\n",
    "    gan_model_name = train_cfg.get(\"gan_model_name\")\n",
    "\n",
    "    # epoch_sufix = \"-last\"\n",
    "    epoch_sufix = str(epoch_sufix)\n",
    "    dis_mn = dis_model_name + epoch_sufix\n",
    "    gen_mn = gen_model_name + epoch_sufix\n",
    "    gan_mn = gan_model_name + epoch_sufix\n",
    "\n",
    "    dis_path = os.path.join(model_fn_path, \"Dis\")\n",
    "    gen_path = os.path.join(model_fn_path, \"Gen\")\n",
    "    gan_path = os.path.join(model_fn_path, \"GAN\")\n",
    "\n",
    "    save_model(dis_model, dis_path, dis_mn)\n",
    "    save_model(gen_model, gen_path, gen_mn)\n",
    "    save_model(gan_model, gan_path, gan_mn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_models(train_cfg, test_cfg):\n",
    "\n",
    "    # epoch_sufix = \"-epoch%d\" % int(ep)\n",
    "    model_fn_path = train_cfg.get(\"models_fn_path\")\n",
    "    dis_path = os.path.join(model_fn_path, \"Dis\")\n",
    "    gen_path = os.path.join(model_fn_path, \"Gen\")\n",
    "    gan_path = os.path.join(model_fn_path, \"GAN\")\n",
    "    max_files = train_cfg.get(\"max_save_models\")\n",
    "\n",
    "    list_path = (dis_path, gen_path, gan_path)\n",
    "    rmv_path = list()\n",
    "\n",
    "    for path in list_path:\n",
    "\n",
    "        files = os.listdir(path)\n",
    "        filepaths = list()\n",
    "\n",
    "        for f in files:\n",
    "            fp = os.path.join(path, f)\n",
    "            filepaths.append(fp)\n",
    "\n",
    "        # print(filepaths)\n",
    "        filepaths.sort(key=os.path.getctime, reverse=True)\n",
    "        # print(\"files!!!!\", filepaths)\n",
    "\n",
    "        if len(filepaths) > max_files:\n",
    "\n",
    "            for del_file in filepaths[max_files:]:\n",
    "                os.remove(del_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape):\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    y_real = real_data[1]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    y_fake = fake_data[1]\n",
    "\n",
    "    # train for real samples batch\n",
    "    dhr = dis_model.train_on_batch(Xi_real, y_real)\n",
    "    # train for fake samples batch\n",
    "    dhf = dis_model.train_on_batch(Xi_fake, y_fake)\n",
    "\n",
    "    # prepare text and inverted categories from the latent space as input for the generator\n",
    "    latent_gen, y_gen = gen_latent_data(dataset_shape, batch_size)\n",
    "\n",
    "    # update the generator via the discriminator's error\n",
    "    gh = gan_model.train_on_batch(latent_gen, y_gen)\n",
    "\n",
    "    return dhr, dhf, gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape):\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    yl_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    yl_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "\n",
    "    # train for real samples batch\n",
    "    dhr = dis_model.train_on_batch([Xi_real, yl_real], y_real)\n",
    "    # train for fake samples batch\n",
    "    dhf = dis_model.train_on_batch([Xi_fake, yl_fake], y_fake)\n",
    "\n",
    "    # prepare text and inverted categories from the latent space as input for the generator\n",
    "    latent_gen, yl_gen, y_gen = gen_latent_data(dataset_shape, batch_size)\n",
    "\n",
    "    # update the generator via the discriminator's error\n",
    "    gh = gan_model.train_on_batch([latent_gen, yl_gen], y_gen)\n",
    "\n",
    "    return dhr, dhf, gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_cgan(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape):\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    Xt_real = real_data[1]\n",
    "    Xl_real = real_data[2]\n",
    "    y_real = real_data[3]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    Xt_fake = fake_data[1]\n",
    "    Xl_fake = fake_data[2]\n",
    "    y_fake = fake_data[3]\n",
    "\n",
    "    # train for real samples batch\n",
    "    dhr = dis_model.train_on_batch([Xi_real, Xt_real, Xl_real], y_real)\n",
    "    # train for fake samples batch\n",
    "    dhf = dis_model.train_on_batch([Xi_fake, Xt_fake, Xl_fake], y_fake)\n",
    "\n",
    "    # prepare text and inverted categories from the latent space as input for the generator\n",
    "    latent_gen, yl_gen, y_gen = gen_latent_data(dataset_shape, batch_size)\n",
    "\n",
    "    # update the generator via the discriminator's error\n",
    "    gh = gan_model.train_on_batch([latent_gen, yl_gen], y_gen)\n",
    "\n",
    "    return dhr, dhf, gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(dis_model, real_data, fake_data, batch_size):\n",
    "    \n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, batch_size, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    y_real = real_data[1]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    y_fake = fake_data[1]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate(Xi_real, y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate(Xi_fake, y_fake, verbose=0)\n",
    "\n",
    "    return test_real, test_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cgan(dis_model, real_data, fake_data, batch_size):\n",
    "    \n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, batch_size, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    Xl_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    Xl_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate([Xi_real, Xl_real], y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate([Xi_fake, Xl_fake], y_fake, verbose=0)\n",
    "\n",
    "    return test_real, test_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_cgan(dis_model, real_data, fake_data, batch_size):\n",
    "\n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, batch_size, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    Xt_real = real_data[1]\n",
    "    Xl_real = real_data[2]\n",
    "    y_real = real_data[3]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    Xt_fake = fake_data[1]\n",
    "    Xl_fake = fake_data[2]\n",
    "    y_fake = fake_data[3]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate([Xi_real, Xt_real, Xl_real], y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate([Xi_fake, Xt_fake, Xl_fake], y_fake, verbose=0)\n",
    "\n",
    "    return test_real, test_fake"
   ]
  },
  {
   "source": [
    "# EXEC SCRIPT\n",
    "\n",
    "## Dataset prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== working files ===\n\n std-VVG-Gallery-Text-Data-Small.csv \n std-VVG-Gallery-Img-Data-Small.csv \n Small \n rgb \n Validation-GAN-Text-Data-Small.csv \n VVG-Gallery-Text-Data-Small.dict\n"
     ]
    }
   ],
   "source": [
    "# variable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with predictions txt data\n",
    "imagef = \"Img\"\n",
    "\n",
    "# report subfolder\n",
    "reportf = \"Reports\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "trainf = \"Train\"\n",
    "testf = \"Test\"\n",
    "\n",
    "# subfolder for model IO\n",
    "modelf = \"Models\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "imgf = \"jpg\"\n",
    "lexf = \"dict\"\n",
    "\n",
    "rgb_sufix = \"rgb\"\n",
    "bw_sufix = \"bw\"\n",
    "\n",
    "# standard sufix\n",
    "stdprefix = \"std-\"\n",
    "\n",
    "# ml model useful data\n",
    "mltprefix = \"ml-\"\n",
    "\n",
    "# report names\n",
    "# timestamp = datetime.date.today().strftime(\"%d-%b-%Y\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "sample_sufix = \"Small\"\n",
    "# sample_sufix = \"Large\"\n",
    "# sample_sufix = \"Paintings\"\n",
    "imgf_sufix = \"Img-Data-\"\n",
    "text_sufix = \"Text-Data-\"\n",
    "\n",
    "# std-VVG-Gallery-Text-Data-Paintings\n",
    "gallery_prefix = \"VVG-Gallery-\"\n",
    "\n",
    "# dataframe file name\n",
    "text_fn = stdprefix + gallery_prefix + text_sufix + sample_sufix + \".\" + fext\n",
    "imgf_fn = stdprefix + gallery_prefix + imgf_sufix + sample_sufix + \".\" + fext\n",
    "valt_fn = \"Validation-GAN-\" + text_sufix + sample_sufix + \".\" + fext\n",
    "lexicon_fn = gallery_prefix + text_sufix + sample_sufix + \".\" + lexf\n",
    "\n",
    "# model names\n",
    "dis_model_name = \"VVG-Text2Img-CDiscriminator\"\n",
    "gen_model_name = \"VVG-Text2Img-CGenerator\"\n",
    "gan_model_name = \"VVG-Text2Img-CGAN\"\n",
    "\n",
    "# to continue training after stoping script\n",
    "continue_training = True\n",
    "\n",
    "# ramdom seed\n",
    "randseed = 42\n",
    "\n",
    "# sample distribution train vs test sample size\n",
    "train_split = 0.80\n",
    "test_split = 1.0 - train_split\n",
    "\n",
    "# regex to know that column Im interested in\n",
    "keeper_regex = r\"(^ID$)|(^std_)\"\n",
    "\n",
    "imgt = rgb_sufix\n",
    "# imgt = bw_sufix\n",
    "\n",
    "# woring values for code\n",
    "work_txtf, work_imgf, work_sufix, work_imgt, work_lex = text_fn, imgf_fn, sample_sufix, imgt, lexicon_fn\n",
    "\n",
    "print(\"=== working files ===\")\n",
    "print(\"\\n\", work_txtf, \"\\n\", work_imgf, \"\\n\", work_sufix, \"\\n\", work_imgt, \"\\n\", valt_fn, \"\\n\", work_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\n"
     ]
    }
   ],
   "source": [
    "root_folder = os.getcwd()\n",
    "root_folder = os.path.split(root_folder)[0]\n",
    "root_folder = os.path.normpath(root_folder)\n",
    "print(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Text-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Img-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\VVG-Gallery-Text-Data-Small.dict True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Test\\Validation-GAN-Text-Data-Small.csv False\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Models True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Reports True\n"
     ]
    }
   ],
   "source": [
    "# variable reading\n",
    "# dataframe filepath for texttual data\n",
    "text_fn_path = os.path.join(root_folder, dataf, trainf, work_txtf)\n",
    "print(text_fn_path, os.path.exists(text_fn_path))\n",
    "\n",
    "# dataframe filepath for img data\n",
    "img_fn_path = os.path.join(root_folder, dataf, trainf, work_imgf)\n",
    "print(img_fn_path, os.path.exists(img_fn_path))\n",
    "\n",
    "# dictionary filepath for the GAN data\n",
    "lex_fn_path = os.path.join(root_folder, dataf, trainf, work_lex)\n",
    "print(lex_fn_path, os.path.exists(lex_fn_path))\n",
    "\n",
    "# dataframe filepath for GAN data\n",
    "val_fn_path = os.path.join(root_folder, dataf, testf, valt_fn)\n",
    "print(val_fn_path, os.path.exists(val_fn_path))\n",
    "\n",
    "# filepath for the models\n",
    "model_fn_path = os.path.join(root_folder, dataf, modelf)\n",
    "print(model_fn_path, os.path.exists(model_fn_path))\n",
    "\n",
    "# filepath for the reports\n",
    "report_fn_path = os.path.join(root_folder, dataf, reportf)\n",
    "print(report_fn_path, os.path.exists(report_fn_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rading training data\n",
    "# loading textual file\n",
    "text_df = pd.read_csv(\n",
    "                text_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "text_cols = text_df.columns.values\n",
    "\n",
    "# loading image file\n",
    "img_df = pd.read_csv(\n",
    "                img_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "img_cols = img_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ID', 'F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "idx_cols = list()\n",
    "\n",
    "for tcol in text_cols:\n",
    "    if tcol in img_cols:\n",
    "        idx_cols.append(tcol)\n",
    "print(idx_cols)\n",
    "\n",
    "source_df = pd.merge(text_df, img_df, how=\"inner\", on=idx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns (total 22 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   ID                     59 non-null     object\n 1   F-number               59 non-null     object\n 2   JH-number              59 non-null     object\n 3   creator-date           59 non-null     object\n 4   creator-place          59 non-null     object\n 5   Dimensions             59 non-null     object\n 6   details                59 non-null     object\n 7   MUS_TEXT               59 non-null     object\n 8   std_cat_creator-date   59 non-null     object\n 9   std_cat_creator-place  59 non-null     object\n 10  std_cat_Dimensions     59 non-null     object\n 11  std_cat_details        59 non-null     object\n 12  clr_tokens             59 non-null     object\n 13  lemmas                 59 non-null     object\n 14  bows_tokens            59 non-null     object\n 15  idxs_tokens            59 non-null     object\n 16  tfidf_tokens           59 non-null     object\n 17  std_dvec_tokens        59 non-null     object\n 18  rgb_img                59 non-null     object\n 19  bw_img                 59 non-null     object\n 20  rgb_shape              59 non-null     object\n 21  bw_shape               59 non-null     object\ndtypes: object(22)\nmemory usage: 10.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking everything is allrigth\n",
    "img_df = None\n",
    "text_df = None\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = source_df.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and loading images into df\n",
    "# working variables\n",
    "src_col = work_imgt + \"_img\"\n",
    "tgt_col = work_imgt + \"_img\" + \"_data\"\n",
    "work_shape = work_imgt + \"_shape\"\n",
    "scale = 16 # !!! 50->400pix, 64->512pix, 32->256pix 16->128pix\n",
    "print(src_col, tgt_col)\n",
    "source_df = get_images(root_folder, source_df, src_col, tgt_col, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update image shape\n",
    "source_df = update_shape(source_df, tgt_col, work_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_shape\n(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# searching the biggest shape in the image files\n",
    "print(work_shape)\n",
    "shape_data = source_df[work_shape]\n",
    "max_shape = get_mshape(shape_data, work_imgt)\n",
    "print(max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img_data pad_cnn_rgb_img_data rgb\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max shape of the images in gallery\n",
    "pad_prefix = \"pad_\"\n",
    "conv_prefix = \"cnn_\"\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = pad_prefix + conv_prefix + src_col\n",
    "\n",
    "print(src_col, tgt_col, work_imgt)\n",
    "source_df = padding_images(source_df, src_col, tgt_col, max_shape, work_imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing regular images...\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and stadarizing images into df\n",
    "# working variables\n",
    "print(\"standarizing regular images...\")\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "\n",
    "# source_df = standarize_images(source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing padded images...\npad_cnn_rgb_img_data std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "print(\"standarizing padded images...\")\n",
    "src_col = pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "print(src_col, tgt_col)\n",
    "\n",
    "# std_opt = \"std\"\n",
    "std_opt = \"ctr\"\n",
    "source_df = standarize_images(source_df, src_col, tgt_col, work_imgt, std_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, s0004V1962r to d1125S2005\nData columns (total 24 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   F-number                  59 non-null     object\n 1   JH-number                 59 non-null     object\n 2   creator-date              59 non-null     object\n 3   creator-place             59 non-null     object\n 4   Dimensions                59 non-null     object\n 5   details                   59 non-null     object\n 6   MUS_TEXT                  59 non-null     object\n 7   std_cat_creator-date      59 non-null     object\n 8   std_cat_creator-place     59 non-null     object\n 9   std_cat_Dimensions        59 non-null     object\n 10  std_cat_details           59 non-null     object\n 11  clr_tokens                59 non-null     object\n 12  lemmas                    59 non-null     object\n 13  bows_tokens               59 non-null     object\n 14  idxs_tokens               59 non-null     object\n 15  tfidf_tokens              59 non-null     object\n 16  std_dvec_tokens           59 non-null     object\n 17  rgb_img                   59 non-null     object\n 18  bw_img                    59 non-null     object\n 19  rgb_shape                 59 non-null     object\n 20  bw_shape                  59 non-null     object\n 21  rgb_img_data              59 non-null     object\n 22  pad_cnn_rgb_img_data      59 non-null     object\n 23  std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(24)\nmemory usage: 11.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# shuffle the DataFrame rows\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "# cleaning memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find a name of column names according to a regex\n",
    "def get_keeper_cols(col_names, search_regex):\n",
    "    ans = [i for i in col_names if re.search(search_regex, i)]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the disperse columns in the df\n",
    "def get_disperse_categories(src_df, keep_cols, max_dis, check_cols, ignore_col):\n",
    "\n",
    "    ans = list()\n",
    "\n",
    "    max_dis = 2\n",
    "    tcount = 0\n",
    "\n",
    "    while tcount < max_dis:\n",
    "        for label_col in keep_columns:\n",
    "\n",
    "            if label_col != ignore_col:\n",
    "\n",
    "                label_count = src_df[label_col].value_counts(normalize=False)\n",
    "\n",
    "                if tcount < label_count.shape[0] and (check_cols in label_col):\n",
    "                    tcount = label_count.shape[0]\n",
    "                    ans.append(label_col)\n",
    "                # print(\"count values of\", label_col, \":=\", label_count.shape)#.__dict__)\n",
    "        tcount = tcount + 1\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the disperse columns from the interesting ones\n",
    "def remove_disperse_categories(keep_columns, too_disperse):\n",
    "    for too in too_disperse:\n",
    "        keep_columns.remove(too)\n",
    "    return keep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_corpus(train_df, dvector_col, pad_prefix):\n",
    "    # getting the corpus dense vectors\n",
    "    work_corpus = np.asarray(train_df[dvector_col], dtype=\"object\")\n",
    "\n",
    "    # converting list of list to array of array\n",
    "    print(\"Original txt shape\", work_corpus.shape)\n",
    "\n",
    "    # padding the representation\n",
    "    work_corpus = pad_sequences(work_corpus, dtype='object', padding=\"post\")\n",
    "    # print(\"Padded txt shape\", work_corpus.shape)\n",
    "\n",
    "    # creating the new column and saving padded data\n",
    "    padded_col_dvector = pad_prefix + dvector_col\n",
    "\n",
    "    # print(padded_col)\n",
    "    train_df[padded_col_dvector] = list(work_corpus)\n",
    "    print(\"Padded txt shape\", work_corpus.shape)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_categories(train_df, cat_cols, tgt_col):\n",
    "\n",
    "    labels_data = train_df[cat_cols]\n",
    "    labels_concat = list()\n",
    "\n",
    "    # concatenating all category labels from dataframe\n",
    "    for index, row in labels_data.iterrows():\n",
    "        row = concat_labels(row, labels_cols)\n",
    "        labels_concat.append(row)\n",
    "\n",
    "    # print(len(labels_concat[0]), type(labels_concat[0]))\n",
    "    # updating dataframe\n",
    "    tcat_label_col = \"std_cat_labels\"\n",
    "    train_df[tgt_col] = labels_concat\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to adjust the textual data for the LSTM layers in the model\n",
    "def format_corpus(corpus, timesteps, features):\n",
    "\n",
    "    # preparation for reshape lstm model\n",
    "    corpus = temporalize(corpus, timesteps)\n",
    "    print(corpus.shape)\n",
    "\n",
    "    corpus = corpus.reshape((corpus.shape[0], timesteps, features))\n",
    "    print(corpus.shape)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ original input/interested columns ------\n['F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'MUS_TEXT', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'clr_tokens', 'lemmas', 'bows_tokens', 'idxs_tokens', 'tfidf_tokens', 'std_dvec_tokens', 'rgb_img', 'bw_img', 'rgb_shape', 'bw_shape', 'rgb_img_data', 'pad_cnn_rgb_img_data', 'std_pad_cnn_rgb_img_data']\n\n\n------ Interesting columns ------\n['std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# selecting data to train\n",
    "# want to keep the columns starting with STD_\n",
    "keep_columns = list(source_df.columns)\n",
    "print(\"------ original input/interested columns ------\")\n",
    "print(keep_columns)\n",
    "\n",
    "# create the columns Im interesting in\n",
    "keep_columns = get_keeper_cols(keep_columns, keeper_regex)\n",
    "# keep_columns = [i for i in df_columns if re.search(keeper_regex, i)]\n",
    "\n",
    "print(\"\\n\\n------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-date', 'std_cat_Dimensions']\n"
     ]
    }
   ],
   "source": [
    "too_disperse = get_disperse_categories(source_df, keep_columns, 2, \"std_cat_\", \"std_pad_cnn_rgb_img_data\")\n",
    "print(too_disperse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ Interesting columns ------\n['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# creating the training dataframe\n",
    "keep_columns = remove_disperse_categories(keep_columns, too_disperse)\n",
    "# keep_columns.remove(\"ID\")\n",
    "print(\"------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving idtfd encoding to translate bow\n",
    "tfidf_tokens = source_df[\"tfidf_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training dataframe\n",
    "train_df = pd.DataFrame(source_df, columns=keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the stuff\n",
    "train_df = train_df.sample(frac = 1)\n",
    "source_df = None\n",
    "df_columns = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, s0061V1962 to s0199V1962\nData columns (total 4 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(4)\nmemory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Padded image column in dataframe:  std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "pad_regex = u\"^std_pad_\"\n",
    "padimg_col = get_keeper_cols(df_columns, pad_regex)\n",
    "padimg_col = padimg_col[0]\n",
    "print(\"Padded image column in dataframe: \", str(padimg_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dense vector column in dataframe:  std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "dvec_regex = u\"^std_dvec\"\n",
    "dvector_col = get_keeper_cols(df_columns, dvec_regex)\n",
    "dvector_col = dvector_col[0]\n",
    "print(\"Dense vector column in dataframe: \", str(dvector_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix column data type\n",
    "work_corpus = train_df[dvector_col]\n",
    "work_corpus = format_dvector(work_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing type in dataframe\n",
    "train_df[dvector_col] = work_corpus\n",
    "work_corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original txt shape (59,)\nPadded txt shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max length of text corpus\n",
    "pad_prefix = \"pad_\"\n",
    "recurrent_prefix = \"lstm_\"\n",
    "\n",
    "train_df = padding_corpus(train_df, dvector_col, pad_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_img_col = \"std_\" + work_imgt + \"_img\" + \"_data\"\n",
    "padded_img_col = \"std_\" + pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "padded_col_dvector = pad_prefix + dvector_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\nClassifier trainable labels in dataframe:  ['std_cat_creator-place', 'std_cat_details']\ncategories heat column: std_cat_labels\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "cat_regex = u\"^std_cat_\"\n",
    "labels_cols = get_keeper_cols(keep_columns, cat_regex)\n",
    "print(\"Classifier trainable labels in dataframe: \", str(labels_cols))\n",
    "\n",
    "# updating dataframe with hot/concatenated categories\n",
    "tcat_label_col = \"std_cat_labels\"\n",
    "print(\"categories heat column:\", tcat_label_col)\n",
    "train_df = heat_categories(train_df, labels_cols, tcat_label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\nTrainable labels columns in dataframe:  ['std_cat_creator-place', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "labels_cols = [i for i in keep_columns if re.search(u\"^std_cat_\", i)]\n",
    "print(\"Trainable labels columns in dataframe: \", str(labels_cols))\n",
    "\n",
    "labels_data = train_df[labels_cols]\n",
    "labels_concat = list()\n",
    "\n",
    "# concatenating all category labels from dataframe\n",
    "for index, row in labels_data.iterrows():\n",
    "    row = concat_labels(row, labels_cols)\n",
    "    labels_concat.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pad_std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "text_lstm_col = padded_col_dvector\n",
    "print(text_lstm_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "working_img_col = padded_img_col\n",
    "# working_img_col = regular_img_col\n",
    "print(working_img_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, s0061V1962 to s0199V1962\nData columns (total 6 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\n 4   pad_std_dvec_tokens       59 non-null     object\n 5   std_cat_labels            59 non-null     object\ndtypes: object(6)\nmemory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 (128, 128, 3)\n",
      "final X_img shape (59, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "og_shape = train_df[working_img_col][0].shape# y[0].shape\n",
    "X_img_len = train_df[working_img_col].shape[0] #y.shape[0]\n",
    "print(X_img_len, og_shape)\n",
    "\n",
    "X_img = None\n",
    "\n",
    "for img in train_df[working_img_col]:\n",
    "\n",
    "    if X_img is None:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = img\n",
    "    else:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = np.concatenate((X_img, img), axis=0)\n",
    "\n",
    "print(\"final X_img shape\", X_img.shape)\n",
    "# y.shape = (1899, 800, 800, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_img[0]))\n",
    "print(type(X_img[0][0]))\n",
    "print(X_img[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_img.shape) == 3:\n",
    "    X_img = X_img[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 16)\n"
     ]
    }
   ],
   "source": [
    "# y = train_df[working_img_col]\n",
    "# y = np.expand_dims(y, axis=0)\n",
    "y_labels = np.asarray([np.asarray(j, dtype=\"object\") for j in train_df[tcat_label_col]], dtype=\"object\")\n",
    "print(\"y shape\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.ones((y_labels.shape[0],1)).astype(\"float32\")\n",
    "print(\"y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y classification category\n<class 'numpy.ndarray'>\n<class 'numpy.float32'>\n(1,)\ny labels category\n<class 'numpy.ndarray'>\n<class 'float'>\n(16,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y classification category\")\n",
    "print(type(y[0]))\n",
    "print(type(y[0][0]))\n",
    "print(y[1].shape)\n",
    "\n",
    "print(\"y labels category\")\n",
    "print(type(y_labels[0]))\n",
    "print(type(y_labels[0][0]))\n",
    "print(y_labels[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "final X_lstm shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "X_txt = np.asarray([np.asarray(i, dtype=\"object\") for i in train_df[text_lstm_col]], dtype=\"object\")\n",
    "# X = np.array(train_df[text_lstm_col]).astype(\"object\")\n",
    "# X = train_df[text_lstm_col]\n",
    "print(\"final X_lstm shape\", X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'float'>\n(142,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_txt[0]))\n",
    "print(type(X_txt[0][0]))\n",
    "print(X_txt[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 142\n"
     ]
    }
   ],
   "source": [
    "# timestep is the memory of what i read, this is the longest sentence I can remember in the short term\n",
    "# neet to look for the best option, in small the max is 15\n",
    "timesteps = 15\n",
    "\n",
    "# features is the max length in the corpus, after padding!!!!\n",
    "features = X_txt[0].shape[0]\n",
    "print(timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 1, 142)\n(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "X_txt = format_corpus(X_txt, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "diff_txt = y.shape[0] - X_txt.shape[0]\n",
    "print(diff_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "Xa = X_txt[-diff_txt:]\n",
    "X_txt = np.append(X_txt, Xa, axis=0)\n",
    "print(X_txt.shape)\n",
    "Xa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n(59, 128, 128, 3)\n(59, 1)\n(59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)\n",
    "print(X_img.shape)\n",
    "print(y.shape)\n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n(128, 128, 3)\n(1,)\n(16,)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt[0].shape)\n",
    "print(X_img[0].shape)\n",
    "print(y[0].shape)\n",
    "print(y_labels[0].shape)\n",
    "txt_og_shape = X_txt[0].shape\n",
    "img_og_shape = X_img[0].shape\n",
    "cat_og_shape = y[0].shape\n",
    "lab_og_shape = y_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xt = X_txt # np.array(X).astype(\"object\")\n",
    "# Xi = X_img\n",
    "# yt = y # np.array(y).astype(\"object\")\n",
    "# # ya = y[0:timesteps]\n",
    "# train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# ML Model Definition\n",
    "\n",
    "## Image GAN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional generator for images\n",
    "def create_img_generator(latent_dims, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    latent_features = model_cfg.get(\"latent_features\")\n",
    "    latent_filters = model_cfg.get(\"latent_filters\")\n",
    "    latent_dense = latent_features*latent_features*latent_filters\n",
    "    # latent_input = latent_shape[0]*latent_shape[1]\n",
    "    latent_n = model_cfg.get(\"latent_img_size\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    # latent img shape\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    rs = model_cfg.get(\"return_sequences\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_dims, name=\"ImgGenIn\")\n",
    "\n",
    "    # dense layer\n",
    "    lyr1 = Dense(latent_dense, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgGenDense_1\")(in_latent)\n",
    "    \n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr2 = Reshape(latent_img_shape, name=\"ImgGenReshape_2\")(lyr1)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr3 = Conv2DTranspose(int(filters), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_3\")(lyr2)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr4 = BatchNormalization(name=\"ImgGenBN_4\",\n",
    "                                epsilon=batchep)(lyr3)\n",
    "    lyr5 = Dropout(hid_ldrop, name=\"ImgGenDrop_5\")(lyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr6 = Conv2DTranspose(int(filters/2), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgGenBN_7\",\n",
    "                                epsilon=batchep)(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgGenDrop_8\")(lyr7)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=out_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_9\")(lyr8)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr10 = BatchNormalization(name=\"ImgGenBN_10\",\n",
    "                                epsilon=batchep)(lyr9)\n",
    "    lyr11 = Dropout(hid_ldrop, name=\"ImgGenDrop_11\")(lyr10)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr12 = Conv2DTranspose(int(filters/8), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_123\")(lyr11)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr13 = BatchNormalization(name=\"ImgGenBN_13\",\n",
    "                                epsilon=batchep)(lyr12)\n",
    "    lyr14 = Dropout(hid_ldrop, name=\"ImgGenDrop_14\")(lyr13)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr15 = Conv2DTranspose(int(filters/16), kernel_size=out_ksize, \n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=out_stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_15\")(lyr14)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr16 = BatchNormalization(name=\"ImgGenBN_16\",\n",
    "                                epsilon=batchep)(lyr15)\n",
    "    lyr17 = Dropout(hid_ldrop, name=\"ImgGenDrop_17\")(lyr16)\n",
    "\n",
    "    # # transpose conv2D layer\n",
    "    # lyr18 = Conv2DTranspose(int(filters/32), kernel_size=ksize, \n",
    "    #                         kernel_initializer=initializer,\n",
    "    #                         strides=stsize, activation=hid_lyr_act, \n",
    "    #                         padding=pad, name=\"ImgGenConv2D_18\")(lyr17)\n",
    "\n",
    "    # # batch normalization + drop layers to avoid overfit\n",
    "    # lyr19 = BatchNormalization(name=\"ImgGenBN_19\",\n",
    "    #                             epsilon=batchep)(lyr18)\n",
    "    # lyr20 = Dropout(hid_ldrop, name=\"ImgGenDrop_20\")(lyr19)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize,\n",
    "                        kernel_initializer=initializer,\n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgGenOut\")(lyr17)#(lyr20)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_latent, outputs=out_img)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional discriminator for images\n",
    "def create_img_discriminator(img_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # input layer config, image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"DisImgIn\")\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer 64 filters\n",
    "    lyr1 = Conv2D(int(in_filters/64), kernel_size=in_ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgDisConv2D_1\")(in_img)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr2 = BatchNormalization(name=\"ImgDisBN_2\",\n",
    "                                epsilon=batchep)(lyr1)\n",
    "    lyr3 = Dropout(hid_ldrop, name=\"ImgDisDrop_3\")(lyr2)\n",
    "\n",
    "    # intermediate conv layer 128 filters\n",
    "    lyr4 = Conv2D(int(in_filters/32), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_4\")(lyr3)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr5 = BatchNormalization(name=\"ImgDisBN_5\",\n",
    "                                epsilon=batchep)(lyr4)\n",
    "    lyr6 = Dropout(hid_ldrop, name=\"ImgDisDrop_6\")(lyr5)\n",
    "\n",
    "    # intermediate conv layer 256 filters\n",
    "    sp_stsize = (1,1)\n",
    "    lyr7 = Conv2D(int(in_filters/16), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=sp_stsize, name=\"ImgDisConv2D_7\")(lyr6)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr8 = BatchNormalization(name=\"ImgDisBN_8\",\n",
    "                                epsilon=batchep)(lyr7)\n",
    "    lyr9 = Dropout(hid_ldrop, name=\"ImgDisDrop_9\")(lyr8)\n",
    "\n",
    "    # intermediate conv layer 512 filters\n",
    "    lyr10 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_10\")(lyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr11 = BatchNormalization(name=\"ImgDisBN_11\",\n",
    "                                epsilon=batchep)(lyr10)\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgDisDrop_12\")(lyr11)\n",
    "\n",
    "    # intermediate conv layer 1024 filters\n",
    "    lyr13 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_13\")(lyr12)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr14 = BatchNormalization(name=\"ImgDisBN_14\",\n",
    "                                epsilon=batchep)(lyr13)\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"ImgDisDrop_15\")(lyr14)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr16 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_16\")(lyr15)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr17 = BatchNormalization(name=\"ImgDisBN_17\",\n",
    "                                epsilon=batchep)(lyr16)\n",
    "    lyr18 = Dropout(hid_ldrop, name=\"ImgDisDrop_18\")(lyr17)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr19 = Flatten(name=\"ImgDisFlat_19\")(lyr18)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr20 = Dense(int(mid_disn), activation=hid_cls_act, name=\"ImgDisDense_20\")(lyr19)\n",
    "    lyr21 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"ImgDisDense_21\")(lyr20)\n",
    "    # drop layer\n",
    "    lyr22 = Dropout(hid_ldrop, name=\"ImgDisDrop_22\")(lyr21)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr23 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"ImgDisDense_23\")(lyr22)\n",
    "    lyr24 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"ImgDisDense_24\")(lyr23)\n",
    "    # drop layer\n",
    "    lyr25 = Dropout(hid_ldrop, name=\"ImgDisDrop_25\")(lyr24)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr26 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"ImgDisDense_26\")(lyr25)\n",
    "    lyr27 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"ImgDisDense_27\")(lyr26)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"ImgDisOut\")(lyr27)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_img, outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "\t# make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "\t# get noise and label inputs from generator model\n",
    "    gen_noise = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model(gen_output)\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model(gen_noise, gan_output)\n",
    "    # compile model\n",
    "    model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # model.compile(loss=ls, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Text GAN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM generator for text\n",
    "def create_txt_generator(latent_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # hidden layer config\n",
    "    latent_n = model_cfg.get(\"mid_gen_neurons\")\n",
    "    latent_reshape = model_cfg.get(\"latent_lstm_reshape\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # output layer condig\n",
    "    txt_shape = model_cfg.get(\"output_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_shape, name=\"TxtGenIn\")\n",
    "\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=latent_shape, \n",
    "                    name = \"TxtGenMask_1\")(in_latent) # concat1\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr2 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    input_shape=latent_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtGenLSTM_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"TxtGenBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"TxtGenDrop_4\")(lyr3)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr5 = Flatten(name=\"TxtGenFlat_5\")(lyr4)\n",
    "\n",
    "    # dense layer\n",
    "    lyr6 = Dense(latent_n, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"TxtGenDense_6\")(lyr5)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr7 = Reshape(latent_reshape, name=\"TxtGenReshape_7\")(lyr6)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr8 = BatchNormalization(name=\"TxtGenBN_8\")(lyr7)\n",
    "    lyr9 = Dropout(hid_ldrop, name=\"TxtGenDrop_9\")(lyr8)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr10 = LSTM(int(lstm_units/4), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_10\")(lyr9)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr11 = LSTM(int(lstm_units/2), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_11\")(lyr10)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr12 = BatchNormalization(name=\"TxtGenBN_12\")(lyr11)\n",
    "    lyr13 = Dropout(hid_ldrop, name=\"TxtGenDrop_13\")(lyr12)\n",
    "\n",
    "    # output layer, dense time sequential layer.\n",
    "    lyr14 = LSTM(lstm_units, activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenDrop_14\")(lyr13)\n",
    "\n",
    "    out_txt = TimeDistributed(Dense(txt_shape, activation=out_lyr_act), name = \"GenTxtOut\")(lyr14)\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=in_latent, outputs=out_txt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM discriminator for text\n",
    "def create_txt_discriminator(txt_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # hidden layer config\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_txt = Input(shape=txt_shape, name=\"DisTxtIn\")\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=txt_shape, \n",
    "                    name = \"TxtDisMask_1\")(in_txt) # concat1\n",
    "\n",
    "    # input LSTM layer\n",
    "    lyr2 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    input_shape=txt_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtDisLSTM_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"TxtDisBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"TxtDisDrop_4\")(lyr3)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr5 = LSTM(int(lstm_units/2), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtDisLSTM_5\")(lyr4)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr6 = LSTM(int(lstm_units/4), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtDisLSTM_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"TxtDisBN_7\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"TxtDisDrop_8\")(lyr7)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr9 = Flatten(name=\"TxtDisFlat_9\")(lyr8)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr10 = Dense(int(mid_disn), activation=hid_cls_act, name=\"TxtDisDense_10\")(lyr9)\n",
    "    lyr11 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"TxtDisDense_11\")(lyr10)\n",
    "    # drop layer\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"TxtDisDrop_12\")(lyr11)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr13 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"TxtDisDense_13\")(lyr12)\n",
    "    lyr14 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"TxtDisDense_14\")(lyr13)\n",
    "    # drop layer\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"TxtDisDrop_15\")(lyr14)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr16 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"TxtDisDense_16\")(lyr15)\n",
    "    lyr17 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"TxtDisDense_17\")(lyr16)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"TxtDisOut\")(lyr17)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_txt, outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model(gen_output)\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model(gen_noise, gan_output)\n",
    "    # compile model\n",
    "    model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Conditional Img GAN: CGAN-img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional generator for images\n",
    "def create_img_cgenerator(latent_dims, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "\n",
    "    # def of the latent space size for the input\n",
    "    latent_features = model_cfg.get(\"latent_features\")\n",
    "    latent_filters = model_cfg.get(\"latent_filters\")\n",
    "    latent_dense = latent_features*latent_features*latent_filters\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "    latent_img_size = model_cfg.get(\"latent_img_size\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    # CONDITIONAL LABELS LAYERS\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"ImgCGenLblIn\")\n",
    "\n",
    "    # dense layer\n",
    "    con3 = Dense(latent_img_size, activation=lbl_ly_actf, name=\"ImgCGenLblDense_3\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con4 = BatchNormalization(name=\"ImgCGenLblBN_4\",\n",
    "                                epsilon=batchep)(con3)\n",
    "    con5 = Dropout(hid_ldrop, name=\"ImgCGenLblDrop_5\")(con4)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    out_con = Reshape(latent_img_shape, name=\"ImgCGenLblOut\")(con5)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_dims, name=\"ImgCGenIn\")\n",
    "\n",
    "    # dense layer\n",
    "    lyr1 = Dense(latent_dense, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgCGenDense_1\")(in_latent)\n",
    "    \n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr2 = Reshape(latent_img_shape, name=\"ImgCGenReshape_2\")(lyr1)\n",
    "\n",
    "    # concat generator layer + labels layer\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"ImgCGenConcat\")([lyr2, out_con])\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr3 = Conv2DTranspose(int(filters), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_3\")(lbl_concat)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr4 = BatchNormalization(name=\"ImgCGenBN_4\",\n",
    "                                epsilon=batchep)(lyr3)\n",
    "    lyr5 = Dropout(hid_ldrop, name=\"ImgCGenDrop_5\")(lyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr6 = Conv2DTranspose(int(filters/2), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgCGenBN_7\",\n",
    "                                epsilon=batchep)(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgCGenDrop_8\")(lyr7)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=out_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_9\")(lyr8)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr10 = BatchNormalization(name=\"ImgCGenBN_10\",\n",
    "                                epsilon=batchep)(lyr9)\n",
    "    lyr11 = Dropout(hid_ldrop, name=\"ImgCGenDrop_11\")(lyr10)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr12 = Conv2DTranspose(int(filters/8), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_123\")(lyr11)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr13 = BatchNormalization(name=\"ImgCGenBN_13\",\n",
    "                                epsilon=batchep)(lyr12)\n",
    "    lyr14 = Dropout(hid_ldrop, name=\"ImgCGenDrop_14\")(lyr13)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr15 = Conv2DTranspose(int(filters/16), kernel_size=out_ksize, \n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=out_stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_15\")(lyr14)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr16 = BatchNormalization(name=\"ImgCGenBN_16\",\n",
    "                                epsilon=batchep)(lyr15)\n",
    "    lyr17 = Dropout(hid_ldrop, name=\"ImgCGenDrop_17\")(lyr16)\n",
    "\n",
    "    # # transpose conv2D layer\n",
    "    # lyr18 = Conv2DTranspose(int(filters/32), kernel_size=ksize, \n",
    "    #                         kernel_initializer=initializer,\n",
    "    #                         strides=stsize, activation=hid_lyr_act, \n",
    "    #                         padding=pad, name=\"ImgCGenConv2D_18\")(lyr17)\n",
    "\n",
    "    # # batch normalization + drop layers to avoid overfit\n",
    "    # lyr19 = BatchNormalization(name=\"ImgCGenBN_19\",\n",
    "    #                             epsilon=batchep)(lyr18)\n",
    "    # lyr20 = Dropout(hid_ldrop, name=\"ImgCGenDrop_20\")(lyr19)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize,\n",
    "                        kernel_initializer=initializer,\n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgCGenOut\")(lyr16)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_latent, in_labels], outputs=out_img)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional discriminator for images\n",
    "def create_img_cdiscriminator(img_shape, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    lbl_filters = model_cfg.get(\"labels_filters\")\n",
    "    lbl_ksize = model_cfg.get(\"labels_kernel_size\")\n",
    "    lbl_stsize = model_cfg.get(\"labels_stride\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    latent_img_size = model_cfg.get(\"latent_img_size\")\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "\n",
    "    # input layer config, image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    # CONDITIONAL LABELS LAYERS\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"ImgCDisLblIn\")\n",
    "\n",
    "    # dense layer\n",
    "    con1 = Dense(latent_img_size, activation=lbl_ly_actf, name=\"ImgCDisLblDense_2\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con2 = BatchNormalization(name=\"ImgCDisLblBN_4\",\n",
    "                                epsilon=batchep)(con1)\n",
    "    con3 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_5\")(con2)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    con4 = Reshape(latent_img_shape, name=\"ImgCDisReshape_6\")(con3)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    con5 = Conv2DTranspose(int(filters/2), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf,\n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_5\")(con4)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con6 = BatchNormalization(name=\"ImgCDisLblBN_6\",\n",
    "                                epsilon=batchep)(con5)\n",
    "    con7 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_7\")(con6)\n",
    "\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    con8 = Conv2DTranspose(int(filters/4), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf,\n",
    "                            padding=pad, name=\"ImgCDisLblDrop_8\")(con7)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con9 = BatchNormalization(name=\"ImgCDisLblBN_9\",\n",
    "                                epsilon=batchep)(con8)\n",
    "    con10 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_10\")(con9)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    con11 = Conv2DTranspose(int(filters/8), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf, \n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_11\")(con10)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con12 = BatchNormalization(name=\"ImgCDisLblBN_12\",\n",
    "                                epsilon=batchep)(con11)\n",
    "    con13 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_13\")(con12)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    con14 = Conv2DTranspose(int(filters/16), kernel_size=lbl_ksize, \n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf, \n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_14\")(con13)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    con15 = BatchNormalization(name=\"ImgCDisLblBN_15\",\n",
    "                                epsilon=batchep)(con14)\n",
    "    con16 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_16\")(con15)\n",
    "\n",
    "    # # transpose conv2D layer\n",
    "    # con17 = Conv2DTranspose(int(filters/32), kernel_size=ksize, \n",
    "    #                         kernel_initializer=initializer,\n",
    "    #                         strides=stsize, activation=hid_lyr_act, \n",
    "    #                         padding=pad, name=\"ImgCDisLblConv2D_17\")(con16)\n",
    "\n",
    "    # # batch normalization + drop layers to avoid overfit\n",
    "    # con18 = BatchNormalization(name=\"ImgCDisLblBN_18\",\n",
    "    #                             epsilon=batchep)(con17)\n",
    "    # con19 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_19\")(con18)\n",
    "\n",
    "    # output layer\n",
    "    con_img = Conv2D(img_shape[2], kernel_size=(3,3),\n",
    "                    kernel_initializer=initializer,\n",
    "                    strides=(1,1), activation=lbl_ly_actf, \n",
    "                    padding=pad, input_shape=img_shape, \n",
    "                    name=\"ImgCDisLblOut\")(con16)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"CDisImgIn\")\n",
    "\n",
    "    # concatenate in img + labels layer\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"ImgCDisConcat\")([in_img, con_img])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer 64 filters\n",
    "    lyr1 = Conv2D(int(in_filters/64), kernel_size=in_ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgCDisConv2D_1\")(lbl_concat)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr2 = BatchNormalization(name=\"ImgCDisBN_2\",\n",
    "                                epsilon=batchep)(lyr1)\n",
    "    lyr3 = Dropout(hid_ldrop, name=\"ImgCDisDrop_3\")(lyr2)\n",
    "\n",
    "    # intermediate conv layer 128 filters\n",
    "    lyr4 = Conv2D(int(in_filters/32), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_4\")(lyr3)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr5 = BatchNormalization(name=\"ImgCDisBN_5\",\n",
    "                                epsilon=batchep)(lyr4)\n",
    "    lyr6 = Dropout(hid_ldrop, name=\"ImgCDisDrop_6\")(lyr5)\n",
    "\n",
    "    # intermediate conv layer 256 filters\n",
    "    sp_stsize = (1,1)\n",
    "    lyr7 = Conv2D(int(in_filters/16), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=sp_stsize, name=\"ImgCDisConv2D_7\")(lyr6)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr8 = BatchNormalization(name=\"ImgCDisBN_8\",\n",
    "                                epsilon=batchep)(lyr7)\n",
    "    lyr9 = Dropout(hid_ldrop, name=\"ImgCDisDrop_9\")(lyr8)\n",
    "\n",
    "    # intermediate conv layer 512 filters\n",
    "    lyr10 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_10\")(lyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr11 = BatchNormalization(name=\"ImgCDisBN_11\",\n",
    "                                epsilon=batchep)(lyr10)\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgCDisDrop_12\")(lyr11)\n",
    "\n",
    "    # intermediate conv layer 1024 filters\n",
    "    lyr13 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_13\")(lyr12)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr14 = BatchNormalization(name=\"ImgCDisBN_14\",\n",
    "                                epsilon=batchep)(lyr13)\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"ImgCDisDrop_15\")(lyr14)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr16 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_16\")(lyr15)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr17 = BatchNormalization(name=\"ImgCDisBN_17\",\n",
    "                                epsilon=batchep)(lyr16)\n",
    "    lyr18 = Dropout(hid_ldrop, name=\"ImgCDisDrop_18\")(lyr17)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr19 = Flatten(name=\"ImgCDisFlat_19\")(lyr18)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr20 = Dense(int(mid_disn), activation=hid_cls_act, name=\"ImgCDisDense_20\")(lyr19)\n",
    "    lyr21 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"ImgCDisDense_21\")(lyr20)\n",
    "    # drop layer\n",
    "    lyr22 = Dropout(hid_ldrop, name=\"ImgCDisDrop_22\")(lyr21)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr23 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"ImgCDisDense_23\")(lyr22)\n",
    "    lyr24 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"ImgCDisDense_24\")(lyr23)\n",
    "    # drop layer\n",
    "    lyr25 = Dropout(hid_ldrop, name=\"ImgCDisDrop_25\")(lyr24)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr26 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"ImgCDisDense_26\")(lyr25)\n",
    "    lyr27 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"ImgCDisDense_27\")(lyr26)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"ImgCDisOut\")(lyr27)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_img, in_labels], outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_cgan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_output, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "source": [
    "## Multi GAN txt2img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Conv discriminator for image and text\n",
    "# TODO need to implement this\n",
    "def create_multi_discriminator(img_shape, txt_shape, model_cfg):\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=[in_img, in_txt], outputs=out_cls)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to implement this\n",
    "def create_multi_generator(img_shape, txt_shape, model_cfg):\n",
    "\n",
    "    # model definition\n",
    "    gen_model = Model(inputs=in_latent, outputs=[out_img, out_txt])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to implement this\n",
    "def create_multi_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    cdis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = cgen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = cgen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_output, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1], metrics=gan_cfg[2])\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "source": [
    "## Multi CGAN txt2img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cgenerator(latent_dims, img_shape, txt_shape, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    # print(\"=======================\\n\",model_cfg, \"=====================\")\n",
    "    memory = model_cfg.get(\"memory\")\n",
    "    features = model_cfg.get(\"features\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    latent_features = model_cfg.get(\"latent_features\")\n",
    "    latent_filters = model_cfg.get(\"latent_filters\")\n",
    "    latent_img_dense = latent_features*latent_features*latent_filters\n",
    "    latent_txt_dense = memory*features\n",
    "    latent_img_size = model_cfg.get(\"latent_img_size\")\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "    latent_txt_size = model_cfg.get(\"latent_txt_size\")\n",
    "    latent_txt_shape = model_cfg.get(\"latent_txt_shape\")\n",
    "    # latent_ntxt = model_cfg.get(\"mid_gen_neurons\")\n",
    "\n",
    "\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    rs = model_cfg.get(\"return_sequences\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "    txt_shape = model_cfg.get(\"output_neurons\")\n",
    "    out_rs = model_cfg.get(\"output_return_sequences\")\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    ############################## OJO LABELS START ##################################\n",
    "\n",
    "    # CONDITIONAL LABELS LAYERS\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"MultiCGenLblIn\")\n",
    "\n",
    "    # image conditional layers\n",
    "    # dense layer\n",
    "    icon1 = Dense(latent_img_size, activation=lbl_ly_actf, name=\"MultiImgCGenLblDense_3\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon2 = BatchNormalization(name=\"MultiImgCGenLblBN_4\",\n",
    "                                epsilon=batchep)(icon1)\n",
    "    icon3 = Dropout(hid_ldrop, name=\"MultiImgCGenLblDrop_5\")(icon2)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    iout_con = Reshape(latent_img_shape, name=\"MultiImgCGenLblOut\")(icon3)\n",
    "\n",
    "    # text conditional layers\n",
    "    # dense layer\n",
    "    tcon1 = Dense(latent_txt_size, activation=lbl_ly_actf, name=\"MultiTxtCGenLblDense_3\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tcon2 = BatchNormalization(name=\"MultiTxtCGenLblBN_4\",\n",
    "                                epsilon=batchep)(tcon1)\n",
    "    tcon3 = Dropout(hid_ldrop, name=\"MultiTxtCGenLblDrop_5\")(tcon2)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    tout_con = Reshape(latent_txt_shape, name=\"MultiTxtCGenLblOut\")(tcon3)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_dims, name=\"ImgMultiCGenIn\")\n",
    "\n",
    "    # dense layer for rgb image\n",
    "    lyr1 = Dense(latent_img_dense, \n",
    "                activation=in_lyr_act, \n",
    "                name=\"ImgMultiGenDense_1\")(in_latent)\n",
    "\n",
    "    # dense layer for text data\n",
    "    lyr2 = Dense(latent_txt_dense, \n",
    "                activation=in_lyr_act, \n",
    "                name=\"TxtMultiGenDense_2\")(in_latent)\n",
    "    \n",
    "    # RGB IMAGE GENERATOR\n",
    "    #  reshape layer 1D-> 2D (rbg image)\n",
    "    ilyr2 = Reshape(latent_img_shape, name=\"ImgMultiCGenReshape_2\")(lyr1)\n",
    "\n",
    "    # concat generator layer + labels layer\n",
    "    ilbl_concat = Concatenate(axis=-1, name=\"ImgMultiCGenConcat\")([ilyr2, iout_con])\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr3 = Conv2DTranspose(int(filters), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgMultiCGenConv2D_3\")(ilbl_concat)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr4 = BatchNormalization(name=\"ImgMultiGenBN_4\",\n",
    "                                epsilon=batchep)(ilyr3)\n",
    "    ilyr5 = Dropout(hid_ldrop, name=\"ImgMultiCGenDrop_5\")(ilyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr6 = Conv2DTranspose(int(filters/2), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgMultiCGenConv2D_6\")(ilyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr7 = BatchNormalization(name=\"ImgMultiCGenBN_7\",\n",
    "                                epsilon=batchep)(ilyr6)\n",
    "    ilyr8 = Dropout(hid_ldrop, name=\"ImgMultiCGenDrop_8\")(ilyr7)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgMultiCGenConv2D_9\")(ilyr8)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr10 = BatchNormalization(name=\"ImgMultiCGenBN_10\",\n",
    "                                epsilon=batchep)(ilyr9)\n",
    "    ilyr11 = Dropout(hid_ldrop, name=\"ImgMultiCGenDrop_11\")(ilyr10)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr12 = Conv2DTranspose(int(filters/8), kernel_size=ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgMultiCGenConv2D_123\")(ilyr11)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr13 = BatchNormalization(name=\"ImgMultiCGenBN_13\",\n",
    "                                epsilon=batchep)(ilyr12)\n",
    "    ilyr14 = Dropout(hid_ldrop, name=\"ImgMultiCGenDrop_14\")(ilyr13)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr15 = Conv2DTranspose(int(filters/16), kernel_size=out_ksize, \n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=out_stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgMultiCGenConv2D_15\")(ilyr14)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr16 = BatchNormalization(name=\"ImgMultiCGenBN_16\",\n",
    "                                epsilon=batchep)(ilyr15)\n",
    "    ilyr17 = Dropout(hid_ldrop, name=\"ImgMultiCGenDrop_17\")(ilyr16)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize,\n",
    "                        kernel_initializer=initializer,\n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgMultiCGenOut\")(ilyr17)\n",
    "\n",
    "    # TEXT DATA GENERATOR\n",
    "    # reshape layer 1D-> 2D (descriptive txt)\n",
    "    tlyr3 = Reshape(latent_txt_shape, name=\"TxtMultiCGenReshape_3\")(lyr2)\n",
    "\n",
    "    # concat generator layer + labels layer\n",
    "    tlbl_concat = Concatenate(axis=-1, name=\"TxtMultiCGenConcat\")([tlyr3, tout_con])\n",
    "\n",
    "    # masking input text\n",
    "    tlyr4 = Masking(mask_value=mval, input_shape=mem_shape, \n",
    "                    name = \"TxtMultiCGenMask_4\")(tlbl_concat)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tlyr5 = LSTM(int(lstm_units/4), activation=hid_lyr_act,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtMultiCGenLSTM_5\")(tlyr4)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr6 = BatchNormalization(name=\"TxtMultiCGenBN_6\",\n",
    "                                epsilon=batchep)(tlyr5)\n",
    "    tlyr7 = Dropout(hid_ldrop, name=\"TxtMultiCGenDrop_7\")(tlyr6)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tlyr8 = LSTM(int(lstm_units/2), activation=hid_lyr_act,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtMultiCGenLSTM_8\")(tlyr7)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr9 = BatchNormalization(name=\"TxtMultiCGenBN_9\",\n",
    "                                epsilon=batchep)(tlyr8)\n",
    "    tlyr10 = Dropout(hid_ldrop, name=\"TxtMultiCGenDrop_10\")(tlyr9)\n",
    "\n",
    "    # output layer, dense time sequential layer.\n",
    "    tlyr11 = LSTM(lstm_units, activation=hid_lyr_act,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtMultiCGenLSTM_11\")(tlyr10)\n",
    "\n",
    "    out_txt = TimeDistributed(Dense(txt_shape, activation=out_lyr_act), name = \"TxtMultiCGenOut\")(tlyr11)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_latent, in_labels], outputs=[out_img, out_txt])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Conv conditianal discriminator for text and images\n",
    "def create_multi_cdiscriminator(img_shape, txt_shape, n_labels, model_cfg):\n",
    "\n",
    "    print(\"=======================\\n\", model_cfg, \"\\n\")\n",
    "    # MODEL CONFIG\n",
    "    # config for txt + img conditional labels\n",
    "    memory = model_cfg.get(\"timesteps\")\n",
    "    features = model_cfg.get(\"max_features\")\n",
    "    lbl_neurons = model_cfg.get(\"labels_neurons\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    lbl_filters = model_cfg.get(\"labels_filters\")\n",
    "    lbl_ksize = model_cfg.get(\"labels_kernel_size\")\n",
    "    lbl_stsize = model_cfg.get(\"labels_stride\")\n",
    "    lbl_lstm = model_cfg.get(\"labels_lstm_neurons\")\n",
    "    lbl_rs = model_cfg.get(\"labels_return_sequences\")\n",
    "    dis_img_reshape = model_cfg.get(\"labels_img_reshape\")\n",
    "    dis_txt_reshape = model_cfg.get(\"labels_txt_reshape\")\n",
    "\n",
    "    # latent_img_dense = latent_features*latent_features*latent_filters\n",
    "    # latent_txt_dense = memory*features\n",
    "    latent_img_size = model_cfg.get(\"latent_img_size\")\n",
    "    # latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "    latent_txt_size = model_cfg.get(\"latent_txt_size\")\n",
    "    # latent_txt_shape = model_cfg.get(\"latent_txt_shape\")\n",
    "\n",
    "\n",
    "    # input layer config for image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # input layer config for txt classification\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # encoding hidden layer config for image classification\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "\n",
    "    # encoding hidden layer config for text classification\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # mid classification config\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer config\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "\n",
    "    # kernet initialization config\n",
    "    initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    batchep = 0.00001\n",
    "\n",
    "    # CONDITIONAL LABELS LAYERS\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"ImgCDisLblIn\")\n",
    "\n",
    "    # image conditional layers\n",
    "    # dense layer\n",
    "    icon1 = Dense(latent_img_size, activation=lbl_ly_actf, name=\"ImgCDisLblDense_1\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon2 = BatchNormalization(name=\"ImgCDisLblBN_2\",\n",
    "                                epsilon=batchep)(icon1)\n",
    "    icon3 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_3\")(icon2)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    icon4 = Reshape(dis_img_reshape, name=\"ImgCDisReshape_4\")(icon3)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    icon5 = Conv2DTranspose(int(filters/2), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf,\n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_5\")(icon4)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon6 = BatchNormalization(name=\"ImgCDisLblBN_6\",\n",
    "                                epsilon=batchep)(icon5)\n",
    "    icon7 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_7\")(icon6)\n",
    "\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    icon8 = Conv2DTranspose(int(filters/4), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf,\n",
    "                            padding=pad, name=\"ImgCDisLblDrop_8\")(icon7)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon9 = BatchNormalization(name=\"ImgCDisLblBN_9\",\n",
    "                                epsilon=batchep)(icon8)\n",
    "    icon10 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_10\")(icon9)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    icon11 = Conv2DTranspose(int(filters/8), kernel_size=lbl_ksize,\n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf, \n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_11\")(icon10)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon12 = BatchNormalization(name=\"ImgCDisLblBN_12\",\n",
    "                                epsilon=batchep)(icon11)\n",
    "    icon13 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_13\")(icon12)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    icon14 = Conv2DTranspose(int(filters/16), kernel_size=lbl_ksize, \n",
    "                            kernel_initializer=initializer,\n",
    "                            strides=lbl_stsize, activation=lbl_ly_actf, \n",
    "                            padding=pad, name=\"ImgCDisLblConv2D_14\")(icon13)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    icon15 = BatchNormalization(name=\"ImgCDisLblBN_15\",\n",
    "                                epsilon=batchep)(icon14)\n",
    "    icon16 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_16\")(icon15)\n",
    "\n",
    "    # output layer\n",
    "    icon_out = Conv2D(img_shape[2], kernel_size=(3,3),\n",
    "                    kernel_initializer=initializer,\n",
    "                    strides=(1,1), activation=lbl_ly_actf, \n",
    "                    padding=pad, input_shape=img_shape, \n",
    "                    name=\"ImgMultiCDisLblOut\")(icon16)\n",
    "\n",
    "    # text conditional layers\n",
    "    # dense layer\n",
    "    tcon1 = Dense(latent_txt_size, activation=lbl_ly_actf, name=\"MultiTxtCDisLblDense_1\")(in_labels)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tcon2 = BatchNormalization(name=\"MultiTxtCDisLblBN_2\",\n",
    "                                epsilon=batchep)(tcon1)\n",
    "    tcon3 = Dropout(hid_ldrop, name=\"MultiTxtCDisLblDrop_3\")(tcon2)\n",
    "\n",
    "    # reshape layer 1D-> 2D (descriptive txt)\n",
    "    tcon4 = Reshape(dis_txt_reshape, name=\"MultiTxtCDisReshape_4\")(tcon3)\n",
    "\n",
    "    # TEXT DATA GENERATOR\n",
    "    # masking input text\n",
    "    tcon5 = Masking(mask_value=mval, input_shape=mem_shape, \n",
    "                    name = \"TxtMultiCDisMask_5\")(tcon4)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tcon6 = LSTM(int(lbl_lstm/4), activation=lbl_ly_actf,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=lbl_rs, \n",
    "                    name=\"TxtMultiCDisLSTM_6\")(tcon5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tcon7 = BatchNormalization(name=\"TxtMultiCDisBN_7\",\n",
    "                                epsilon=batchep)(tcon6)\n",
    "    tcon8 = Dropout(hid_ldrop, name=\"TxtMultiCDisDrop_8\")(tcon7)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tcon9 = LSTM(int(lbl_lstm/2), activation=lbl_ly_actf,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=lbl_rs, \n",
    "                    name=\"TxtMultiCDisLSTM_9\")(tcon8)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tcon10 = BatchNormalization(name=\"TxtMultiCGenBN_10\",\n",
    "                                epsilon=batchep)(tcon9)\n",
    "    tcon11 = Dropout(hid_ldrop, name=\"TxtMultiCDisDrop_11\")(tcon10)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tcon12 = LSTM(lbl_lstm, activation=lbl_ly_actf,\n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=lbl_rs, \n",
    "                    name=\"TxtMultiCDisLSTM_12\")(tcon11)\n",
    "\n",
    "    # output layer, dense time sequential layer.\n",
    "    # print(txt_shape, type(txt_shape))\n",
    "    tcon_out = TimeDistributed(Dense(txt_shape[1], activation=lbl_ly_actf), name = \"TxtMultiCDisLblOut\")(tcon12)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # IMAGE DISCRIMINATOR\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"ImgMulitCDisIn\")\n",
    "\n",
    "    # concatenate in img + labels layer\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"ImgMultiCDisConcat\")([in_img, icon_out])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer 64 filters\n",
    "    ilyr1 = Conv2D(int(in_filters/64), kernel_size=in_ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgMultiCDisConv2D_1\")(lbl_concat)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr2 = BatchNormalization(name=\"ImgMultiCDisBN_2\",\n",
    "                                epsilon=batchep)(ilyr1)\n",
    "    ilyr3 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_3\")(ilyr2)\n",
    "\n",
    "    # intermediate conv layer 128 filters\n",
    "    ilyr4 = Conv2D(int(in_filters/32), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgMultiCDisConv2D_4\")(ilyr3)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr5 = BatchNormalization(name=\"ImgMultiCDisBN_5\",\n",
    "                                epsilon=batchep)(ilyr4)\n",
    "    ilyr6 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_6\")(ilyr5)\n",
    "\n",
    "    # intermediate conv layer 256 filters\n",
    "    sp_stsize = (1,1)\n",
    "    ilyr7 = Conv2D(int(in_filters/16), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=sp_stsize, name=\"ImgMultiCDisConv2D_7\")(ilyr6)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr8 = BatchNormalization(name=\"ImgMultiCDisBN_8\",\n",
    "                                epsilon=batchep)(ilyr7)\n",
    "    ilyr9 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_9\")(ilyr8)\n",
    "\n",
    "    # intermediate conv layer 512 filters\n",
    "    ilyr10 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgMultiCDisConv2D_10\")(ilyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr11 = BatchNormalization(name=\"ImgMultiCDisBN_11\",\n",
    "                                epsilon=batchep)(ilyr10)\n",
    "    ilyr12 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_12\")(ilyr11)\n",
    "\n",
    "    # intermediate conv layer 1024 filters\n",
    "    ilyr13 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgMultiCDisConv2D_13\")(ilyr12)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr14 = BatchNormalization(name=\"ImgMultiCDisBN_14\",\n",
    "                                epsilon=batchep)(ilyr13)\n",
    "    ilyr15 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_15\")(ilyr14)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    ilyr16 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    kernel_initializer=initializer,\n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgMultiCDisConv2D_16\")(ilyr15)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr17 = BatchNormalization(name=\"ImgMultiCDisBN_17\",\n",
    "                                epsilon=batchep)(ilyr16)\n",
    "    ilyr18 = Dropout(hid_ldrop, name=\"ImgMultiCDisDrop_18\")(ilyr17)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    ilyr19 = Flatten(name=\"ImgMultiCDisFlat_19\")(ilyr18)\n",
    "\n",
    "    #TXT DISCRIMINATOR\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_txt = Input(shape=txt_shape, name=\"TxtMultiCDisIn\")\n",
    "\n",
    "    # concat txt input with labels conditional\n",
    "    concat_txt = Concatenate(axis=-1, name=\"TxtMultiCDisConcat\")([in_txt, tcon_out])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # masking input text\n",
    "    tlyr1 = Masking(mask_value=mval, input_shape=txt_shape, \n",
    "                    name = \"TxtMultiCDisMask_1\")(concat_txt) # concat1\n",
    "\n",
    "    # input LSTM layer\n",
    "    tlyr2 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    kernel_initializer=initializer,\n",
    "                    input_shape=txt_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtMultiCDisLSTM_2\")(tlyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr3 = BatchNormalization(name=\"TxtMultiCDisBN_3\",\n",
    "                                epsilon=batchep)(tlyr2)\n",
    "    tlyr4 = Dropout(hid_ldrop, name=\"TxtMultiCtDisDrop_4\")(tlyr3)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    tlyr5 = LSTM(int(lstm_units/2), \n",
    "                activation=hid_lyr_act, \n",
    "                kernel_initializer=initializer,\n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtMultiCDisLSTM_5\")(tlyr4)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr6 = BatchNormalization(name=\"TxtMultiCDisBN_6\",\n",
    "                                epsilon=batchep)(tlyr5)\n",
    "    tlyr7 = Dropout(hid_ldrop, name=\"TxtMultiCDisDrop_7\")(tlyr6)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    tlyr8 = LSTM(int(lstm_units/4),\n",
    "                kernel_initializer=initializer,\n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtMultiCDisLSTM_8\")(tlyr7)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr9 = BatchNormalization(name=\"TxtMultiCDisBN_9\",\n",
    "                                epsilon=batchep)(tlyr8)\n",
    "    tlyr10 = Dropout(hid_ldrop, name=\"TxtMultiCDisDrop_10\")(tlyr9)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    tlyr11 = Flatten(name=\"TxtMultiCDisFlat_11\")(tlyr10)\n",
    "\n",
    "    # concat img encoding + txt encoding\n",
    "    # concat_encoding = Concatenate(axis=-1, name=\"MultiCDisDenseConcat\")([ilyr19, tlyr11])\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr1 = Dense(int(mid_disn), activation=hid_cls_act, name=\"MultiCDisDense_1\")(concat_encoding)\n",
    "    lyr2 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"MultiCDisDense_2\")(lyr1)\n",
    "    # drop layer\n",
    "    lyr3 = Dropout(hid_ldrop, name=\"MultiCDisDrop_3\")(lyr2)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr4 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"MultiCDisDense_4\")(lyr3)\n",
    "    lyr5 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"MultiCDisDense_5\")(lyr4)\n",
    "    # drop layer\n",
    "    lyr6 = Dropout(hid_ldrop, name=\"MultiCDisDrop_6\")(lyr5)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr7 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"MultiCDisDense_7\")(lyr6)\n",
    "    lyr8 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"MultiCDisDense_8\")(lyr7)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"MultiCDisOut\")(lyr8)\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=[in_img, in_txt, in_labels], outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cgan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_img, gen_txt = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_img, gen_txt, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fromat lexicon/dictionary to translate for humnas\n",
    "def format_tfidf_tokens(tfidf_tokens):\n",
    "\n",
    "    tfidf_dict = list()\n",
    "\n",
    "    for tfidf in tfidf_tokens:\n",
    "\n",
    "        tfidf = eval(tfidf)\n",
    "        # print(type(tfidf), tfidf)\n",
    "        td = dict(tfidf)\n",
    "        tfidf_dict.append(td)\n",
    "\n",
    "    return tfidf_dict"
   ]
  },
  {
   "source": [
    "## ML Models Configuration\n",
    "### GAN-img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_KERNEL_SIZE = (5,5)#(4,4) #(7,7) # (5,5) #(4,4)\n",
    "REF_FILTERS = 128*8 # *16 -> 256pix, *8->128, \n",
    "REF_LSTM = 400\n",
    "\n",
    "# slow opti functions\n",
    "MIN_DLR = 0.000005\n",
    "MIN_GLR = 0.00002\n",
    "\n",
    "# fast opti functions\n",
    "MAX_DLR = 0.00005\n",
    "MAX_GLR = 0.0002\n",
    "\n",
    "# middle opti functions\n",
    "MID_DLR = 0.00003\n",
    "MID_GLR = 0.0001\n",
    "\n",
    "# working opti functions\n",
    "DIS_OPTI_REF = Adam(learning_rate=MIN_DLR, beta_1=0.50) # oficial learning_Rate=0.000050, beta_1= 0.50\n",
    "GEN_OPTI_REF = Adam(learning_rate=MIN_GLR, beta_1=0.50) # oficial learning_rate0.000200, beta_1=0.50\n",
    "\n",
    "# loss and activation function\n",
    "LOSS_REF = \"binary_crossentropy\"\n",
    "ACC_REF = [\"accuracy\"]\n",
    "ACT_REF = LeakyReLU(alpha=0.2)\n",
    "\n",
    "# common variables for the models\n",
    "# input common vars\n",
    "input_filters = REF_FILTERS\n",
    "input_kernel_size = REF_KERNEL_SIZE\n",
    "input_stride = (2,2)\n",
    "input_padding = \"same\"\n",
    "input_lstm_neurons = REF_LSTM\n",
    "mask_value = 0.0\n",
    "input_return_sequences = True\n",
    "\n",
    "# latent and conditional label common vars\n",
    "# def of the latent space size for the input\n",
    "latent_features = 8 # 5 # model_cfg.get(\"latent_features\")\n",
    "latent_filters = REF_FILTERS # 128 # model_cfg.get(\"latent_filters\")\n",
    "latent_lstm_reshape = X_txt[0].shape\n",
    "memory_shape = X_txt[0].shape\n",
    "memory = memory_shape[0]\n",
    "max_features = memory_shape[1]\n",
    "labels_neurons = timesteps*X_txt.shape[2]\n",
    "latent_img_size = 8*8*REF_FILTERS # 50*50*8 # 32*32*3, # 5*5*128 #\n",
    "latent_img_shape = (8,8,REF_FILTERS) # (50,50,8) # (32,32,3), # (5,5,128) # \n",
    "labels_img_neurons =  8*8*REF_FILTERS # 50*50*3\n",
    "labels_filters = REF_FILTERS\n",
    "labels_kernel_size = REF_KERNEL_SIZE\n",
    "labels_stride = (2,2)\n",
    "labels_reshape = (8,8,REF_FILTERS)\n",
    "labels_lstm_neurons = REF_LSTM\n",
    "labels_return_sequences = True\n",
    "labels_txt_reshape = X_txt[0].shape\n",
    "\n",
    "# hidden common vars\n",
    "lstm_neurons = REF_LSTM\n",
    "filters = REF_FILTERS\n",
    "kernel_size = REF_KERNEL_SIZE\n",
    "stride = (2,2)\n",
    "padding = \"same\"\n",
    "gen_dropout_rate = 0.25\n",
    "mid_txt_gen_neurons = X_txt.shape[1]*X_txt.shape[2]\n",
    "hidden_return_sequences = True\n",
    "dis_dropout_rate = 0.25\n",
    "mid_dis_neurons = 2*2*REF_FILTERS # 50*50*2 # 32*32*3,\n",
    "dense_cls_activation = ACT_REF # \"softmax\"\n",
    "\n",
    "# output common vars\n",
    "output_neurons = X_txt.shape[2]\n",
    "output_txt_shape = X_txt[0].shape\n",
    "output_gen_lyr_activation = \"tanh\" #\"softmax\" #\"tanh\"\n",
    "output_gen_txt_activation = \"softmax\" #\"tanh\"\n",
    "output_return_sequences = True\n",
    "output_filters = X_img[0].shape[2]\n",
    "output_kernel_size = (3,3)\n",
    "output_stride = (1,1)\n",
    "output_img_shape = X_img[0].shape\n",
    "output_dis_neurons = 1\n",
    "output_dis_lyr_activation = \"softmax\" # \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Generator Config:\n {'latent_features': 8, 'latent_filters': 1024, 'mask_value': 0.0, 'return_sequences': True, 'lstm_neurons': 400, 'latent_img_size': 65536, 'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'latent_img_shape': (8, 8, 1024), 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'gen_dropout_rate': 0.25, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (128, 128, 3), 'output_lyr_activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "# img generator config\n",
    "img_gen_cfg = {\n",
    "    \"latent_features\": latent_features,\n",
    "    \"latent_filters\": latent_filters,\n",
    "    \"mask_value\": mask_value,\n",
    "    \"return_sequences\": hidden_return_sequences,\n",
    "    \"lstm_neurons\": lstm_neurons,\n",
    "    \"latent_img_size\": latent_img_size,\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"latent_img_shape\": latent_img_shape,\n",
    "    \"filters\": filters, \n",
    "    \"kernel_size\": kernel_size,\n",
    "    \"stride\": stride,\n",
    "    \"padding\": padding,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"gen_dropout_rate\": gen_dropout_rate,\n",
    "    \"output_filters\": output_filters,\n",
    "    \"output_kernel_size\": output_kernel_size,\n",
    "    \"output_stride\": output_stride,\n",
    "    \"output_padding\": padding,\n",
    "    \"output_shape\": output_img_shape,\n",
    "    \"output_lyr_activation\": output_gen_lyr_activation,\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Generator Config:\\n\", img_gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Discriminator Config:\n {'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'input_filters': 1024, 'input_kernel_size': (5, 5), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'dis_dropout_rate': 0.25, 'mid_dis_neurons': 4096, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'output_dis_neurons': 1, 'output_lyr_activation': 'softmax', 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B8640BAF0>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# img discriminator config\n",
    "img_dis_cfg = {\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"input_filters\": input_filters,\n",
    "    \"input_kernel_size\": input_kernel_size,\n",
    "    \"input_stride\": input_stride,\n",
    "    \"input_padding\": input_padding,\n",
    "    \"filters\": filters,\n",
    "    \"kernel_size\": kernel_size,\n",
    "    \"stride\": stride,\n",
    "    \"padding\": padding,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"dis_dropout_rate\": dis_dropout_rate,\n",
    "    \"mid_dis_neurons\": mid_dis_neurons,\n",
    "    \"dense_cls_activation\": dense_cls_activation,\n",
    "    \"output_dis_neurons\": output_dis_neurons,\n",
    "    \"output_lyr_activation\": output_dis_lyr_activation,\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": DIS_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Discriminator Config:\\n\", img_dis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B87727400>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# img GAN config\n",
    "gan_cfg = {\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": GEN_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Config:\\n\", gan_cfg)"
   ]
  },
  {
   "source": [
    "### GAN-txt definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Generator Config:\n {'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'latent_txt_size': 2130, 'lstm_neurons': 400, 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'hidden_return_sequences': True, 'gen_dropout_rate': 0.25, 'latent_txt_shape': (15, 142), 'memory_shape': (15, 142), 'output_neurons': 142, 'output_shape': (15, 142), 'output_txt_activation': 'softmax', 'output_return_sequences': True}\n"
     ]
    }
   ],
   "source": [
    "# txt generator config\n",
    "txt_gen_cfg = {\n",
    "    \"mask_value\": mask_value,\n",
    "    \"input_return_sequences\": input_return_sequences,\n",
    "    \"input_lstm_neurons\": input_lstm_neurons,\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"latent_txt_size\": mid_txt_gen_neurons,\n",
    "    \"lstm_neurons\": lstm_neurons,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"hidden_return_sequences\": hidden_return_sequences,\n",
    "    \"gen_dropout_rate\": gen_dropout_rate,\n",
    "    \"latent_txt_shape\": latent_lstm_reshape,\n",
    "    \"memory_shape\": memory_shape,\n",
    "    \"output_neurons\": output_neurons,\n",
    "    \"output_shape\": output_txt_shape,\n",
    "    \"output_txt_activation\": output_gen_txt_activation,\n",
    "    \"output_return_sequences\": output_return_sequences,\n",
    "    }\n",
    "\n",
    "print(\"GAN-txt Generator Config:\\n\", txt_gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Discriminator Config:\n {'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'lstm_neurons': 400, 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'hidden_return_sequences': True, 'memory_shape': (15, 142), 'dis_dropout_rate': 0.25, 'latent_txt_size': 2130, 'mid_dis_neurons': 2130, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'output_dis_neurons': 1, 'output_lyr_activation': 'softmax', 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B8640BAF0>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt discriminator config\n",
    "txt_dis_cfg = {\n",
    "    \"mask_value\": mask_value,\n",
    "    \"input_return_sequences\": input_return_sequences,\n",
    "    \"input_lstm_neurons\": input_lstm_neurons,\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"lstm_neurons\": lstm_neurons,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"hidden_return_sequences\": hidden_return_sequences,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"memory_shape\": memory_shape,\n",
    "    \"dis_dropout_rate\": dis_dropout_rate,\n",
    "    \"latent_txt_size\": mid_txt_gen_neurons,\n",
    "    \"mid_dis_neurons\": mid_txt_gen_neurons,\n",
    "    \"dense_cls_activation\": dense_cls_activation,\n",
    "    \"output_dis_neurons\": output_dis_neurons,\n",
    "    \"output_lyr_activation\": output_dis_lyr_activation,\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": DIS_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"GAN-txt Discriminator Config:\\n\", txt_dis_cfg)"
   ]
  },
  {
   "source": [
    "### CGAN-img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Generator Config:\n {'latent_features': 8, 'latent_filters': 1024, 'memory': 15, 'features': 142, 'mask_value': 0.0, 'latent_img_size': 65536, 'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'latent_img_shape': (8, 8, 1024), 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'gen_dropout_rate': 0.25, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (128, 128, 3), 'output_lyr_activation': 'tanh', 'labels_neurons': 2130, 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>}\n"
     ]
    }
   ],
   "source": [
    "img_cgen_cfg = {\n",
    "    \"latent_features\": latent_features,\n",
    "    \"latent_filters\": latent_filters,\n",
    "    \"memory\": memory,\n",
    "    \"features\": max_features,\n",
    "    \"mask_value\": mask_value,\n",
    "    \"latent_img_size\": latent_img_size,\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"latent_img_shape\": latent_img_shape,\n",
    "    \"filters\": filters, \n",
    "    \"kernel_size\": kernel_size,\n",
    "    \"stride\": stride,\n",
    "    \"padding\": padding,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"gen_dropout_rate\": gen_dropout_rate,\n",
    "    \"output_filters\": output_filters,\n",
    "    \"output_kernel_size\": output_kernel_size,\n",
    "    \"output_stride\": output_stride,\n",
    "    \"output_padding\": padding,\n",
    "    \"output_shape\": output_img_shape,\n",
    "    \"output_lyr_activation\": output_gen_lyr_activation,\n",
    "    \"labels_neurons\": labels_neurons,\n",
    "    \"labels_lyr_activation\": ACT_REF,\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Generator Config:\\n\", img_cgen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Generator Config:\n {'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'input_filters': 1024, 'latent_img_size': 65536, 'latent_img_shape': (8, 8, 1024), 'input_kernel_size': (5, 5), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'dis_dropout_rate': 0.25, 'mid_dis_neurons': 4096, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'output_dis_neurons': 1, 'output_lyr_activation': 'softmax', 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'timesteps': 15, 'max_features': 142, 'labels_neurons': 65536, 'labels_filters': 1024, 'labels_kernel_size': (5, 5), 'labels_stride': (2, 2), 'labels_reshape': (8, 8, 1024), 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B8640BAF0>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "img_cdis_cfg = {\n",
    "    \"input_lyr_activation\": ACT_REF,\n",
    "    \"input_filters\": input_filters,\n",
    "    \"latent_img_size\": latent_img_size,\n",
    "    \"latent_img_shape\": latent_img_shape,\n",
    "    \"input_kernel_size\": input_kernel_size,\n",
    "    \"input_stride\": input_stride,\n",
    "    \"input_padding\": padding,\n",
    "    \"filters\": filters,\n",
    "    \"kernel_size\": kernel_size,\n",
    "    \"stride\": stride,\n",
    "    \"padding\": padding,\n",
    "    \"hidden_lyr_activation\": ACT_REF,\n",
    "    \"dis_dropout_rate\": dis_dropout_rate,\n",
    "    \"mid_dis_neurons\":mid_dis_neurons,\n",
    "    \"dense_cls_activation\": dense_cls_activation,\n",
    "    \"output_dis_neurons\": output_dis_neurons,\n",
    "    \"output_lyr_activation\": output_dis_lyr_activation,\n",
    "    \"labels_lyr_activation\": ACT_REF,\n",
    "    \"timesteps\": memory,\n",
    "    \"max_features\": max_features,\n",
    "    \"labels_neurons\": labels_img_neurons,\n",
    "    \"labels_lyr_activation\": ACT_REF,\n",
    "    \"labels_filters\": labels_filters,\n",
    "    \"labels_kernel_size\": labels_kernel_size,\n",
    "    \"labels_stride\": labels_stride,\n",
    "    \"labels_reshape\": labels_reshape,\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": DIS_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Generator Config:\\n\", img_cdis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B87727400>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt GAN config\n",
    "img_cgan_cfg = {\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": GEN_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Config:\\n\", img_cgan_cfg)"
   ]
  },
  {
   "source": [
    "### Multi CGAN definition (txt+img)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGen-txt2img Config:\n {'latent_features': 8, 'latent_filters': 1024, 'memory': 15, 'features': 142, 'mask_value': 0.0, 'latent_img_size': 65536, 'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'latent_img_shape': (8, 8, 1024), 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'gen_dropout_rate': 0.25, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (15, 142), 'output_lyr_activation': 'tanh', 'labels_neurons': 2130, 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'latent_txt_size': 2130, 'lstm_neurons': 400, 'hidden_return_sequences': True, 'latent_txt_shape': (15, 142), 'memory_shape': (15, 142), 'output_neurons': 142, 'output_txt_activation': 'softmax', 'output_return_sequences': True}\n"
     ]
    }
   ],
   "source": [
    "multi_cgen_cfg = dict()\n",
    "multi_cgen_cfg.update(img_cgen_cfg)\n",
    "multi_cgen_cfg.update(txt_gen_cfg)\n",
    "\n",
    "print(\"Multi CGen-txt2img Config:\\n\", multi_cgen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CDis-txt2img Config:\n {'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'input_filters': 1024, 'latent_img_size': 65536, 'latent_img_shape': (8, 8, 1024), 'input_kernel_size': (5, 5), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'dis_dropout_rate': 0.25, 'mid_dis_neurons': 2130, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'output_dis_neurons': 1, 'output_lyr_activation': 'softmax', 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'timesteps': 15, 'max_features': 142, 'labels_neurons': 65536, 'labels_filters': 1024, 'labels_kernel_size': (5, 5), 'labels_stride': (2, 2), 'labels_reshape': (8, 8, 1024), 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B8640BAF0>, 'metrics': ['accuracy'], 'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'lstm_neurons': 400, 'hidden_return_sequences': True, 'memory_shape': (15, 142), 'latent_txt_size': 2130, 'labels_lstm_neurons': 400, 'labels_return_sequences': True, 'labels_img_reshape': (8, 8, 1024), 'labels_txt_reshape': (15, 142), 'output_txt_activation': 'softmax'}\n"
     ]
    }
   ],
   "source": [
    "multi_cdis_cfg = dict()\n",
    "multi_cdis_cfg.update(img_cdis_cfg)\n",
    "multi_cdis_cfg.update(txt_dis_cfg)\n",
    "\n",
    "mcdis_cfg_update = {\n",
    "    \"labels_lstm_neurons\": labels_lstm_neurons,\n",
    "    \"labels_return_sequences\": labels_return_sequences,\n",
    "    \"labels_img_reshape\": labels_reshape,\n",
    "    \"labels_txt_reshape\": labels_txt_reshape,\n",
    "    \"output_txt_activation\": output_gen_txt_activation,\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": DIS_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "multi_cdis_cfg.update(mcdis_cfg_update)\n",
    "\n",
    "print(\"Multi CDis-txt2img Config:\\n\", multi_cdis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B87727400>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt2img CGAN config\n",
    "multi_cgan_cfg = {\n",
    "    \"loss\": LOSS_REF,\n",
    "    \"optimizer\": GEN_OPTI_REF,\n",
    "    \"metrics\": ACC_REF,\n",
    "    }\n",
    "\n",
    "print(\"Multi CGAN-txt2img Config:\\n\", multi_cgan_cfg)"
   ]
  },
  {
   "source": [
    "## ML Model Creation\n",
    "### GAN img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# latent shape\n",
    "latent_dims = 128\n",
    "print(latent_dims)\n",
    "# latent_shape = (int(X_img[0].shape[0]/4), int(X_img[0].shape[1]/4), 3)\n",
    "# latent_shape = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Generator Definition\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nImgGenIn (InputLayer)        [(None, 128)]             0         \n_________________________________________________________________\nImgGenDense_1 (Dense)        (None, 65536)             8454144   \n_________________________________________________________________\nImgGenReshape_2 (Reshape)    (None, 8, 8, 1024)        0         \n_________________________________________________________________\nImgGenConv2D_3 (Conv2DTransp (None, 16, 16, 1024)      26215424  \n_________________________________________________________________\nImgGenBN_4 (BatchNormalizati (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nImgGenDrop_5 (Dropout)       (None, 16, 16, 1024)      0         \n_________________________________________________________________\nImgGenConv2D_6 (Conv2DTransp (None, 32, 32, 512)       13107712  \n_________________________________________________________________\nImgGenBN_7 (BatchNormalizati (None, 32, 32, 512)       2048      \n_________________________________________________________________\nImgGenDrop_8 (Dropout)       (None, 32, 32, 512)       0         \n_________________________________________________________________\nImgGenConv2D_9 (Conv2DTransp (None, 64, 64, 256)       3277056   \n_________________________________________________________________\nImgGenBN_10 (BatchNormalizat (None, 64, 64, 256)       1024      \n_________________________________________________________________\nImgGenDrop_11 (Dropout)      (None, 64, 64, 256)       0         \n_________________________________________________________________\nImgGenConv2D_123 (Conv2DTran (None, 128, 128, 128)     819328    \n_________________________________________________________________\nImgGenBN_13 (BatchNormalizat (None, 128, 128, 128)     512       \n_________________________________________________________________\nImgGenDrop_14 (Dropout)      (None, 128, 128, 128)     0         \n_________________________________________________________________\nImgGenConv2D_15 (Conv2DTrans (None, 128, 128, 64)      73792     \n_________________________________________________________________\nImgGenBN_16 (BatchNormalizat (None, 128, 128, 64)      256       \n_________________________________________________________________\nImgGenDrop_17 (Dropout)      (None, 128, 128, 64)      0         \n_________________________________________________________________\nImgGenOut (Conv2D)           (None, 128, 128, 3)       1731      \n=================================================================\nTotal params: 51,957,123\nTrainable params: 51,953,155\nNon-trainable params: 3,968\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = create_img_generator(latent_dims, img_gen_cfg)\n",
    "print(\"GAN-img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "gen_model.model_name = \"GAN-img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 128, 3)\n",
      "GAN-img Discriminator Definition\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "DisImgIn (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_1 (Conv2D)      (None, 64, 64, 16)        1216      \n",
      "_________________________________________________________________\n",
      "ImgDisBN_2 (BatchNormalizati (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_3 (Dropout)       (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_4 (Conv2D)      (None, 32, 32, 32)        12832     \n",
      "_________________________________________________________________\n",
      "ImgDisBN_5 (BatchNormalizati (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_6 (Dropout)       (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_7 (Conv2D)      (None, 32, 32, 64)        51264     \n",
      "_________________________________________________________________\n",
      "ImgDisBN_8 (BatchNormalizati (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_9 (Dropout)       (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_10 (Conv2D)     (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "ImgDisBN_11 (BatchNormalizat (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_12 (Dropout)      (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_13 (Conv2D)     (None, 8, 8, 256)         819456    \n",
      "_________________________________________________________________\n",
      "ImgDisBN_14 (BatchNormalizat (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_15 (Dropout)      (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "ImgDisConv2D_16 (Conv2D)     (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "ImgDisBN_17 (BatchNormalizat (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_18 (Dropout)      (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "ImgDisFlat_19 (Flatten)      (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "ImgDisDense_20 (Dense)       (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "ImgDisDense_21 (Dense)       (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_22 (Dropout)      (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "ImgDisDense_23 (Dense)       (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "ImgDisDense_24 (Dense)       (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "ImgDisDrop_25 (Dropout)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "ImgDisDense_26 (Dense)       (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "ImgDisDense_27 (Dense)       (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "ImgDisOut (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 49,107,553\n",
      "Trainable params: 49,105,537\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape = X_img[0].shape\n",
    "print(img_shape)\n",
    "\n",
    "# img_shape = (100,100,3)\n",
    "dis_model = create_img_discriminator(img_shape, img_dis_cfg)\n",
    "print(\"GAN-img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "dis_model.model_name = \"GAN-img Discriminator\"\n",
    "\n",
    "# compile model\n",
    "dis_model.compile(loss=img_dis_cfg[\"loss\"], \n",
    "                    optimizer=img_dis_cfg[\"optimizer\"], \n",
    "                    metrics=img_dis_cfg[\"metrics\"])\n",
    "\n",
    "# cdis_model.trainable = False\n",
    "dis_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Model definition\nModel: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nImgGenIn (InputLayer)        [(None, 128)]             0         \n_________________________________________________________________\nImgGenDense_1 (Dense)        (None, 65536)             8454144   \n_________________________________________________________________\nImgGenReshape_2 (Reshape)    (None, 8, 8, 1024)        0         \n_________________________________________________________________\nImgGenConv2D_3 (Conv2DTransp (None, 16, 16, 1024)      26215424  \n_________________________________________________________________\nImgGenBN_4 (BatchNormalizati (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nImgGenDrop_5 (Dropout)       (None, 16, 16, 1024)      0         \n_________________________________________________________________\nImgGenConv2D_6 (Conv2DTransp (None, 32, 32, 512)       13107712  \n_________________________________________________________________\nImgGenBN_7 (BatchNormalizati (None, 32, 32, 512)       2048      \n_________________________________________________________________\nImgGenDrop_8 (Dropout)       (None, 32, 32, 512)       0         \n_________________________________________________________________\nImgGenConv2D_9 (Conv2DTransp (None, 64, 64, 256)       3277056   \n_________________________________________________________________\nImgGenBN_10 (BatchNormalizat (None, 64, 64, 256)       1024      \n_________________________________________________________________\nImgGenDrop_11 (Dropout)      (None, 64, 64, 256)       0         \n_________________________________________________________________\nImgGenConv2D_123 (Conv2DTran (None, 128, 128, 128)     819328    \n_________________________________________________________________\nImgGenBN_13 (BatchNormalizat (None, 128, 128, 128)     512       \n_________________________________________________________________\nImgGenDrop_14 (Dropout)      (None, 128, 128, 128)     0         \n_________________________________________________________________\nImgGenConv2D_15 (Conv2DTrans (None, 128, 128, 64)      73792     \n_________________________________________________________________\nImgGenBN_16 (BatchNormalizat (None, 128, 128, 64)      256       \n_________________________________________________________________\nImgGenDrop_17 (Dropout)      (None, 128, 128, 64)      0         \n_________________________________________________________________\nImgGenOut (Conv2D)           (None, 128, 128, 3)       1731      \n_________________________________________________________________\nmodel_1 (Functional)         (None, 1)                 49107553  \n=================================================================\nTotal params: 101,064,676\nTrainable params: 51,953,155\nNon-trainable params: 49,111,521\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"GAN-img Model definition\")\n",
    "gan_model = create_img_gan(gen_model, dis_model, gan_cfg)\n",
    "gan_model.model_name = \"GAN-img\"\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-21 09:19:13\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(gen_model, model_fn_path, gen_model.model_name, timestamp)\n",
    "export_model(dis_model, model_fn_path, dis_model.model_name, timestamp)\n",
    "export_model(gan_model, model_fn_path, gan_model.model_name, timestamp)"
   ]
  },
  {
   "source": [
    "### GAN txt definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_txt_model = create_txt_generator(latent_dims, txt_gen_cfg)\n",
    "# print(\"GAN-txt Generator Definition\")\n",
    "# # dis_model = Sequential(slim_dis_layers)\n",
    "# gen_txt_model.model_name = \"GAN-txt Generator\"\n",
    "\n",
    "# # DONT compile model\n",
    "# # cdis_model.trainable = False\n",
    "# gen_txt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n"
     ]
    }
   ],
   "source": [
    "txt_shape = X_txt[0].shape\n",
    "print(txt_shape)\n",
    "# dis_txt_model = create_txt_discriminator(txt_shape, txt_dis_cfg)\n",
    "# print(\"GAN-txt Discriminator Definition\")\n",
    "# # dis_model = Sequential(slim_dis_layers)\n",
    "# dis_txt_model.model_name = \"GAN-txt Discriminator\"\n",
    "\n",
    "# # compile model\n",
    "# dis_txt_model.compile(loss=txt_dis_cfg[\"loss\"], \n",
    "#                     optimizer=txt_dis_cfg[\"optimizer\"], \n",
    "#                     metrics=txt_dis_cfg[\"metrics\"])\n",
    "\n",
    "# # cdis_model.trainable = False\n",
    "# dis_txt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Model definition\n"
     ]
    }
   ],
   "source": [
    "print(\"GAN-txt Model definition\")\n",
    "# gan_txt_model = create_img_gan(gen_txt_model, dis_txt_model, gan_cfg)\n",
    "# gan_txt_model.summary()\n",
    "# gan_txt_model.model_name = \"GAN-txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-21 09:19:13\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "# export_model(gen_txt_model, model_fn_path, gen_txt_model.model_name, timestamp)\n",
    "# export_model(dis_txt_model, model_fn_path, dis_txt_model.model_name, timestamp)\n",
    "# export_model(gan_txt_model, model_fn_path, gan_txt_model.model_name, timestamp)"
   ]
  },
  {
   "source": [
    "### CGAN definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n",
      "CGAN-img Generator Definition\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgCGenLblIn (InputLayer)       [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDense_3 (Dense)       (None, 65536)        1114112     ImgCGenLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenIn (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblBN_4 (BatchNormalizat (None, 65536)        262144      ImgCGenLblDense_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDense_1 (Dense)          (None, 65536)        8454144     ImgCGenIn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDrop_5 (Dropout)      (None, 65536)        0           ImgCGenLblBN_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenReshape_2 (Reshape)      (None, 8, 8, 1024)   0           ImgCGenDense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblOut (Reshape)         (None, 8, 8, 1024)   0           ImgCGenLblDrop_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConcat (Concatenate)     (None, 8, 8, 2048)   0           ImgCGenReshape_2[0][0]           \n",
      "                                                                 ImgCGenLblOut[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_3 (Conv2DTranspos (None, 16, 16, 1024) 52429824    ImgCGenConcat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_4 (BatchNormalization (None, 16, 16, 1024) 4096        ImgCGenConv2D_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_5 (Dropout)         (None, 16, 16, 1024) 0           ImgCGenBN_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_6 (Conv2DTranspos (None, 32, 32, 512)  13107712    ImgCGenDrop_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_7 (BatchNormalization (None, 32, 32, 512)  2048        ImgCGenConv2D_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_8 (Dropout)         (None, 32, 32, 512)  0           ImgCGenBN_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_9 (Conv2DTranspos (None, 64, 64, 256)  3277056     ImgCGenDrop_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_10 (BatchNormalizatio (None, 64, 64, 256)  1024        ImgCGenConv2D_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_11 (Dropout)        (None, 64, 64, 256)  0           ImgCGenBN_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_123 (Conv2DTransp (None, 128, 128, 128 819328      ImgCGenDrop_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_13 (BatchNormalizatio (None, 128, 128, 128 512         ImgCGenConv2D_123[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_14 (Dropout)        (None, 128, 128, 128 0           ImgCGenBN_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_15 (Conv2DTranspo (None, 128, 128, 64) 73792       ImgCGenDrop_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_16 (BatchNormalizatio (None, 128, 128, 64) 256         ImgCGenConv2D_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenOut (Conv2D)             (None, 128, 128, 3)  1731        ImgCGenBN_16[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 79,547,779\n",
      "Trainable params: 79,412,739\n",
      "Non-trainable params: 135,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_labels = y_labels[0].shape[0]\n",
    "print(n_labels)\n",
    "cgen_img_model = create_img_cgenerator(latent_dims, n_labels, img_cgen_cfg)\n",
    "print(\"CGAN-img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "cgen_img_model.model_name = \"CGAN-img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "cgen_img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 128, 3)\n",
      "CGAN-img Discriminator Definition\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgCDisLblIn (InputLayer)       [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDense_2 (Dense)       (None, 65536)        1114112     ImgCDisLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_4 (BatchNormalizat (None, 65536)        262144      ImgCDisLblDense_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_5 (Dropout)      (None, 65536)        0           ImgCDisLblBN_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisReshape_6 (Reshape)      (None, 8, 8, 1024)   0           ImgCDisLblDrop_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_5 (Conv2DTrans (None, 16, 16, 512)  13107712    ImgCDisReshape_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_6 (BatchNormalizat (None, 16, 16, 512)  2048        ImgCDisLblConv2D_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_7 (Dropout)      (None, 16, 16, 512)  0           ImgCDisLblBN_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_8 (Conv2DTranspo (None, 32, 32, 256)  3277056     ImgCDisLblDrop_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_9 (BatchNormalizat (None, 32, 32, 256)  1024        ImgCDisLblDrop_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_10 (Dropout)     (None, 32, 32, 256)  0           ImgCDisLblBN_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_11 (Conv2DTran (None, 64, 64, 128)  819328      ImgCDisLblDrop_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_12 (BatchNormaliza (None, 64, 64, 128)  512         ImgCDisLblConv2D_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_13 (Dropout)     (None, 64, 64, 128)  0           ImgCDisLblBN_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_14 (Conv2DTran (None, 128, 128, 64) 204864      ImgCDisLblDrop_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_15 (BatchNormaliza (None, 128, 128, 64) 256         ImgCDisLblConv2D_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_16 (Dropout)     (None, 128, 128, 64) 0           ImgCDisLblBN_15[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "CDisImgIn (InputLayer)          [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblOut (Conv2D)          (None, 128, 128, 3)  1731        ImgCDisLblDrop_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConcat (Concatenate)     (None, 128, 128, 6)  0           CDisImgIn[0][0]                  \n",
      "                                                                 ImgCDisLblOut[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_1 (Conv2D)        (None, 64, 64, 16)   2416        ImgCDisConcat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_2 (BatchNormalization (None, 64, 64, 16)   64          ImgCDisConv2D_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_3 (Dropout)         (None, 64, 64, 16)   0           ImgCDisBN_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_4 (Conv2D)        (None, 32, 32, 32)   12832       ImgCDisDrop_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_5 (BatchNormalization (None, 32, 32, 32)   128         ImgCDisConv2D_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_6 (Dropout)         (None, 32, 32, 32)   0           ImgCDisBN_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_7 (Conv2D)        (None, 32, 32, 64)   51264       ImgCDisDrop_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_8 (BatchNormalization (None, 32, 32, 64)   256         ImgCDisConv2D_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_9 (Dropout)         (None, 32, 32, 64)   0           ImgCDisBN_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_10 (Conv2D)       (None, 16, 16, 128)  204928      ImgCDisDrop_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_11 (BatchNormalizatio (None, 16, 16, 128)  512         ImgCDisConv2D_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_12 (Dropout)        (None, 16, 16, 128)  0           ImgCDisBN_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_13 (Conv2D)       (None, 8, 8, 256)    819456      ImgCDisDrop_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_14 (BatchNormalizatio (None, 8, 8, 256)    1024        ImgCDisConv2D_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_15 (Dropout)        (None, 8, 8, 256)    0           ImgCDisBN_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisConv2D_16 (Conv2D)       (None, 4, 4, 512)    3277312     ImgCDisDrop_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisBN_17 (BatchNormalizatio (None, 4, 4, 512)    2048        ImgCDisConv2D_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_18 (Dropout)        (None, 4, 4, 512)    0           ImgCDisBN_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisFlat_19 (Flatten)        (None, 8192)         0           ImgCDisDrop_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_20 (Dense)         (None, 4096)         33558528    ImgCDisFlat_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_21 (Dense)         (None, 2048)         8390656     ImgCDisDense_20[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_22 (Dropout)        (None, 2048)         0           ImgCDisDense_21[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_23 (Dense)         (None, 1024)         2098176     ImgCDisDrop_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_24 (Dense)         (None, 512)          524800      ImgCDisDense_23[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDrop_25 (Dropout)        (None, 512)          0           ImgCDisDense_24[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_26 (Dense)         (None, 256)          131328      ImgCDisDrop_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisDense_27 (Dense)         (None, 128)          32896       ImgCDisDense_26[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisOut (Dense)              (None, 1)            129         ImgCDisDense_27[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 67,899,540\n",
      "Trainable params: 67,764,532\n",
      "Non-trainable params: 135,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape = X_img[0].shape\n",
    "print(img_shape)\n",
    "cdis_img_model = create_img_cdiscriminator(img_shape, n_labels, img_cdis_cfg)\n",
    "print(\"CGAN-img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "cdis_img_model.model_name = \"CGAN-img Discriminator\"\n",
    "\n",
    "# compile model\n",
    "cdis_img_model.compile(loss=img_cdis_cfg[\"loss\"], \n",
    "                    optimizer=img_cdis_cfg[\"optimizer\"], \n",
    "                    metrics=img_cdis_cfg[\"metrics\"])\n",
    "\n",
    "# cdis_model.trainable = False\n",
    "cdis_img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Model definition\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgCGenLblIn (InputLayer)       [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDense_3 (Dense)       (None, 65536)        1114112     ImgCGenLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenIn (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblBN_4 (BatchNormalizat (None, 65536)        262144      ImgCGenLblDense_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDense_1 (Dense)          (None, 65536)        8454144     ImgCGenIn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDrop_5 (Dropout)      (None, 65536)        0           ImgCGenLblBN_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenReshape_2 (Reshape)      (None, 8, 8, 1024)   0           ImgCGenDense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblOut (Reshape)         (None, 8, 8, 1024)   0           ImgCGenLblDrop_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConcat (Concatenate)     (None, 8, 8, 2048)   0           ImgCGenReshape_2[0][0]           \n",
      "                                                                 ImgCGenLblOut[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_3 (Conv2DTranspos (None, 16, 16, 1024) 52429824    ImgCGenConcat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_4 (BatchNormalization (None, 16, 16, 1024) 4096        ImgCGenConv2D_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_5 (Dropout)         (None, 16, 16, 1024) 0           ImgCGenBN_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_6 (Conv2DTranspos (None, 32, 32, 512)  13107712    ImgCGenDrop_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_7 (BatchNormalization (None, 32, 32, 512)  2048        ImgCGenConv2D_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_8 (Dropout)         (None, 32, 32, 512)  0           ImgCGenBN_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_9 (Conv2DTranspos (None, 64, 64, 256)  3277056     ImgCGenDrop_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_10 (BatchNormalizatio (None, 64, 64, 256)  1024        ImgCGenConv2D_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_11 (Dropout)        (None, 64, 64, 256)  0           ImgCGenBN_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_123 (Conv2DTransp (None, 128, 128, 128 819328      ImgCGenDrop_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_13 (BatchNormalizatio (None, 128, 128, 128 512         ImgCGenConv2D_123[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_14 (Dropout)        (None, 128, 128, 128 0           ImgCGenBN_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_15 (Conv2DTranspo (None, 128, 128, 64) 73792       ImgCGenDrop_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_16 (BatchNormalizatio (None, 128, 128, 64) 256         ImgCGenConv2D_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenOut (Conv2D)             (None, 128, 128, 3)  1731        ImgCGenBN_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Functional)            (None, 1)            67899540    ImgCGenOut[0][0]                 \n",
      "                                                                 ImgCGenLblIn[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 147,447,319\n",
      "Trainable params: 79,412,739\n",
      "Non-trainable params: 68,034,580\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"CGAN-img Model definition\")\n",
    "cgan_img_model = create_img_cgan(cgen_img_model, cdis_img_model, gan_cfg)\n",
    "cgan_img_model.summary()\n",
    "cgan_img_model.model_name = \"CGAN-img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-21 09:19:13\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(cgen_img_model, model_fn_path, cgen_img_model.model_name, timestamp)\n",
    "export_model(cdis_img_model, model_fn_path, cdis_img_model.model_name, timestamp)\n",
    "export_model(cgan_img_model, model_fn_path, cgan_img_model.model_name, timestamp)"
   ]
  },
  {
   "source": [
    "### Multi CGAN-txt&img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Generator Definition\nModel: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nMultiCGenLblIn (InputLayer)     [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nMultiImgCGenLblDense_3 (Dense)  (None, 65536)        1114112     MultiCGenLblIn[0][0]             \n__________________________________________________________________________________________________\nImgMultiCGenIn (InputLayer)     [(None, 128)]        0                                            \n__________________________________________________________________________________________________\nMultiImgCGenLblBN_4 (BatchNorma (None, 65536)        262144      MultiImgCGenLblDense_3[0][0]     \n__________________________________________________________________________________________________\nImgMultiGenDense_1 (Dense)      (None, 65536)        8454144     ImgMultiCGenIn[0][0]             \n__________________________________________________________________________________________________\nMultiImgCGenLblDrop_5 (Dropout) (None, 65536)        0           MultiImgCGenLblBN_4[0][0]        \n__________________________________________________________________________________________________\nImgMultiCGenReshape_2 (Reshape) (None, 8, 8, 1024)   0           ImgMultiGenDense_1[0][0]         \n__________________________________________________________________________________________________\nMultiImgCGenLblOut (Reshape)    (None, 8, 8, 1024)   0           MultiImgCGenLblDrop_5[0][0]      \n__________________________________________________________________________________________________\nImgMultiCGenConcat (Concatenate (None, 8, 8, 2048)   0           ImgMultiCGenReshape_2[0][0]      \n                                                                 MultiImgCGenLblOut[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenConv2D_3 (Conv2DTra (None, 16, 16, 1024) 52429824    ImgMultiCGenConcat[0][0]         \n__________________________________________________________________________________________________\nImgMultiGenBN_4 (BatchNormaliza (None, 16, 16, 1024) 4096        ImgMultiCGenConv2D_3[0][0]       \n__________________________________________________________________________________________________\nImgMultiCGenDrop_5 (Dropout)    (None, 16, 16, 1024) 0           ImgMultiGenBN_4[0][0]            \n__________________________________________________________________________________________________\nMultiTxtCGenLblDense_3 (Dense)  (None, 2130)         36210       MultiCGenLblIn[0][0]             \n__________________________________________________________________________________________________\nImgMultiCGenConv2D_6 (Conv2DTra (None, 32, 32, 512)  13107712    ImgMultiCGenDrop_5[0][0]         \n__________________________________________________________________________________________________\nMultiTxtCGenLblBN_4 (BatchNorma (None, 2130)         8520        MultiTxtCGenLblDense_3[0][0]     \n__________________________________________________________________________________________________\nImgMultiCGenBN_7 (BatchNormaliz (None, 32, 32, 512)  2048        ImgMultiCGenConv2D_6[0][0]       \n__________________________________________________________________________________________________\nTxtMultiGenDense_2 (Dense)      (None, 2130)         274770      ImgMultiCGenIn[0][0]             \n__________________________________________________________________________________________________\nMultiTxtCGenLblDrop_5 (Dropout) (None, 2130)         0           MultiTxtCGenLblBN_4[0][0]        \n__________________________________________________________________________________________________\nImgMultiCGenDrop_8 (Dropout)    (None, 32, 32, 512)  0           ImgMultiCGenBN_7[0][0]           \n__________________________________________________________________________________________________\nTxtMultiCGenReshape_3 (Reshape) (None, 15, 142)      0           TxtMultiGenDense_2[0][0]         \n__________________________________________________________________________________________________\nMultiTxtCGenLblOut (Reshape)    (None, 15, 142)      0           MultiTxtCGenLblDrop_5[0][0]      \n__________________________________________________________________________________________________\nImgMultiCGenConv2D_9 (Conv2DTra (None, 64, 64, 256)  3277056     ImgMultiCGenDrop_8[0][0]         \n__________________________________________________________________________________________________\nTxtMultiCGenConcat (Concatenate (None, 15, 284)      0           TxtMultiCGenReshape_3[0][0]      \n                                                                 MultiTxtCGenLblOut[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenBN_10 (BatchNormali (None, 64, 64, 256)  1024        ImgMultiCGenConv2D_9[0][0]       \n__________________________________________________________________________________________________\nTxtMultiCGenMask_4 (Masking)    (None, 15, 284)      0           TxtMultiCGenConcat[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenDrop_11 (Dropout)   (None, 64, 64, 256)  0           ImgMultiCGenBN_10[0][0]          \n__________________________________________________________________________________________________\nTxtMultiCGenLSTM_5 (LSTM)       (None, 15, 100)      154000      TxtMultiCGenMask_4[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenConv2D_123 (Conv2DT (None, 128, 128, 128 819328      ImgMultiCGenDrop_11[0][0]        \n__________________________________________________________________________________________________\nTxtMultiCGenBN_6 (BatchNormaliz (None, 15, 100)      400         TxtMultiCGenLSTM_5[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenBN_13 (BatchNormali (None, 128, 128, 128 512         ImgMultiCGenConv2D_123[0][0]     \n__________________________________________________________________________________________________\nTxtMultiCGenDrop_7 (Dropout)    (None, 15, 100)      0           TxtMultiCGenBN_6[0][0]           \n__________________________________________________________________________________________________\nImgMultiCGenDrop_14 (Dropout)   (None, 128, 128, 128 0           ImgMultiCGenBN_13[0][0]          \n__________________________________________________________________________________________________\nTxtMultiCGenLSTM_8 (LSTM)       (None, 15, 200)      240800      TxtMultiCGenDrop_7[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenConv2D_15 (Conv2DTr (None, 128, 128, 64) 73792       ImgMultiCGenDrop_14[0][0]        \n__________________________________________________________________________________________________\nTxtMultiCGenBN_9 (BatchNormaliz (None, 15, 200)      800         TxtMultiCGenLSTM_8[0][0]         \n__________________________________________________________________________________________________\nImgMultiCGenBN_16 (BatchNormali (None, 128, 128, 64) 256         ImgMultiCGenConv2D_15[0][0]      \n__________________________________________________________________________________________________\nTxtMultiCGenDrop_10 (Dropout)   (None, 15, 200)      0           TxtMultiCGenBN_9[0][0]           \n__________________________________________________________________________________________________\nImgMultiCGenDrop_17 (Dropout)   (None, 128, 128, 64) 0           ImgMultiCGenBN_16[0][0]          \n__________________________________________________________________________________________________\nTxtMultiCGenLSTM_11 (LSTM)      (None, 15, 400)      961600      TxtMultiCGenDrop_10[0][0]        \n__________________________________________________________________________________________________\nImgMultiCGenOut (Conv2D)        (None, 128, 128, 3)  1731        ImgMultiCGenDrop_17[0][0]        \n__________________________________________________________________________________________________\nTxtMultiCGenOut (TimeDistribute (None, 15, 142)      56942       TxtMultiCGenLSTM_11[0][0]        \n==================================================================================================\nTotal params: 81,281,821\nTrainable params: 81,141,921\nNon-trainable params: 139,900\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multi_cgen_model = create_multi_cgenerator(latent_dims, img_shape, txt_shape, n_labels, multi_cgen_cfg)\n",
    "print(\"Multi CGAN-txt2img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "multi_cgen_model.model_name = \"Multi CGAN-txt&img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "multi_cgen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n",
      "=======================\n",
      " {'input_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'input_filters': 1024, 'latent_img_size': 65536, 'latent_img_shape': (8, 8, 1024), 'input_kernel_size': (5, 5), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 1024, 'kernel_size': (5, 5), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'dis_dropout_rate': 0.25, 'mid_dis_neurons': 2130, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'output_dis_neurons': 1, 'output_lyr_activation': 'softmax', 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000023B87B07D60>, 'timesteps': 15, 'max_features': 142, 'labels_neurons': 65536, 'labels_filters': 1024, 'labels_kernel_size': (5, 5), 'labels_stride': (2, 2), 'labels_reshape': (8, 8, 1024), 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x0000023B8640BAF0>, 'metrics': ['accuracy'], 'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'lstm_neurons': 400, 'hidden_return_sequences': True, 'memory_shape': (15, 142), 'latent_txt_size': 2130, 'labels_lstm_neurons': 400, 'labels_return_sequences': True, 'labels_img_reshape': (8, 8, 1024), 'labels_txt_reshape': (15, 142), 'output_txt_activation': 'softmax'} \n",
      "\n",
      "Multi CGAN-txt2img Discriminator Definition\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgCDisLblIn (InputLayer)       [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDense_1 (Dense)       (None, 65536)        1114112     ImgCDisLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_2 (BatchNormalizat (None, 65536)        262144      ImgCDisLblDense_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_3 (Dropout)      (None, 65536)        0           ImgCDisLblBN_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisReshape_4 (Reshape)      (None, 8, 8, 1024)   0           ImgCDisLblDrop_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_5 (Conv2DTrans (None, 16, 16, 512)  13107712    ImgCDisReshape_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_6 (BatchNormalizat (None, 16, 16, 512)  2048        ImgCDisLblConv2D_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_7 (Dropout)      (None, 16, 16, 512)  0           ImgCDisLblBN_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_8 (Conv2DTranspo (None, 32, 32, 256)  3277056     ImgCDisLblDrop_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_9 (BatchNormalizat (None, 32, 32, 256)  1024        ImgCDisLblDrop_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_10 (Dropout)     (None, 32, 32, 256)  0           ImgCDisLblBN_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_11 (Conv2DTran (None, 64, 64, 128)  819328      ImgCDisLblDrop_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_12 (BatchNormaliza (None, 64, 64, 128)  512         ImgCDisLblConv2D_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_13 (Dropout)     (None, 64, 64, 128)  0           ImgCDisLblBN_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCDisLblDense_1 (Dense)  (None, 2130)         36210       ImgCDisLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblConv2D_14 (Conv2DTran (None, 128, 128, 64) 204864      ImgCDisLblDrop_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCDisLblBN_2 (BatchNorma (None, 2130)         8520        MultiTxtCDisLblDense_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblBN_15 (BatchNormaliza (None, 128, 128, 64) 256         ImgCDisLblConv2D_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCDisLblDrop_3 (Dropout) (None, 2130)         0           MultiTxtCDisLblBN_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgCDisLblDrop_16 (Dropout)     (None, 128, 128, 64) 0           ImgCDisLblBN_15[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCDisReshape_4 (Reshape) (None, 15, 142)      0           MultiTxtCDisLblDrop_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ImgMulitCDisIn (InputLayer)     [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisLblOut (Conv2D)     (None, 128, 128, 3)  1731        ImgCDisLblDrop_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisMask_5 (Masking)    (None, 15, 142)      0           MultiTxtCDisReshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConcat (Concatenate (None, 128, 128, 6)  0           ImgMulitCDisIn[0][0]             \n",
      "                                                                 ImgMultiCDisLblOut[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_6 (LSTM)       (None, 15, 100)      97200       TxtMultiCDisMask_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_1 (Conv2D)   (None, 64, 64, 16)   2416        ImgMultiCDisConcat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisBN_7 (BatchNormaliz (None, 15, 100)      400         TxtMultiCDisLSTM_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_2 (BatchNormaliz (None, 64, 64, 16)   64          ImgMultiCDisConv2D_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisDrop_8 (Dropout)    (None, 15, 100)      0           TxtMultiCDisBN_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_3 (Dropout)    (None, 64, 64, 16)   0           ImgMultiCDisBN_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_9 (LSTM)       (None, 15, 200)      240800      TxtMultiCDisDrop_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_4 (Conv2D)   (None, 32, 32, 32)   12832       ImgMultiCDisDrop_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenBN_10 (BatchNormali (None, 15, 200)      800         TxtMultiCDisLSTM_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_5 (BatchNormaliz (None, 32, 32, 32)   128         ImgMultiCDisConv2D_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisDrop_11 (Dropout)   (None, 15, 200)      0           TxtMultiCGenBN_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_6 (Dropout)    (None, 32, 32, 32)   0           ImgMultiCDisBN_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_12 (LSTM)      (None, 15, 400)      961600      TxtMultiCDisDrop_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_7 (Conv2D)   (None, 32, 32, 64)   51264       ImgMultiCDisDrop_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisIn (InputLayer)     [(None, 15, 142)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLblOut (TimeDistrib (None, 15, 142)      56942       TxtMultiCDisLSTM_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_8 (BatchNormaliz (None, 32, 32, 64)   256         ImgMultiCDisConv2D_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisConcat (Concatenate (None, 15, 284)      0           TxtMultiCDisIn[0][0]             \n",
      "                                                                 TxtMultiCDisLblOut[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_9 (Dropout)    (None, 32, 32, 64)   0           ImgMultiCDisBN_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisMask_1 (Masking)    (None, 15, 284)      0           TxtMultiCDisConcat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_10 (Conv2D)  (None, 16, 16, 128)  204928      ImgMultiCDisDrop_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_2 (LSTM)       (None, 15, 400)      1096000     TxtMultiCDisMask_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_11 (BatchNormali (None, 16, 16, 128)  512         ImgMultiCDisConv2D_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisBN_3 (BatchNormaliz (None, 15, 400)      1600        TxtMultiCDisLSTM_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_12 (Dropout)   (None, 16, 16, 128)  0           ImgMultiCDisBN_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCtDisDrop_4 (Dropout)   (None, 15, 400)      0           TxtMultiCDisBN_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_13 (Conv2D)  (None, 8, 8, 256)    819456      ImgMultiCDisDrop_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_5 (LSTM)       (None, 15, 200)      480800      TxtMultiCtDisDrop_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_14 (BatchNormali (None, 8, 8, 256)    1024        ImgMultiCDisConv2D_13[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisBN_6 (BatchNormaliz (None, 15, 200)      800         TxtMultiCDisLSTM_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_15 (Dropout)   (None, 8, 8, 256)    0           ImgMultiCDisBN_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisDrop_7 (Dropout)    (None, 15, 200)      0           TxtMultiCDisBN_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisConv2D_16 (Conv2D)  (None, 4, 4, 512)    3277312     ImgMultiCDisDrop_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisLSTM_8 (LSTM)       (None, 15, 100)      120400      TxtMultiCDisDrop_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisBN_17 (BatchNormali (None, 4, 4, 512)    2048        ImgMultiCDisConv2D_16[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisBN_9 (BatchNormaliz (None, 15, 100)      400         TxtMultiCDisLSTM_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisDrop_18 (Dropout)   (None, 4, 4, 512)    0           ImgMultiCDisBN_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisDrop_10 (Dropout)   (None, 15, 100)      0           TxtMultiCDisBN_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCDisFlat_19 (Flatten)   (None, 8192)         0           ImgMultiCDisDrop_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCDisFlat_11 (Flatten)   (None, 1500)         0           TxtMultiCDisDrop_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDenseConcat (Concatena (None, 9692)         0           ImgMultiCDisFlat_19[0][0]        \n",
      "                                                                 TxtMultiCDisFlat_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_1 (Dense)        (None, 2130)         20646090    MultiCDisDenseConcat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_2 (Dense)        (None, 1065)         2269515     MultiCDisDense_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDrop_3 (Dropout)       (None, 1065)         0           MultiCDisDense_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_4 (Dense)        (None, 532)          567112      MultiCDisDrop_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_5 (Dense)        (None, 266)          141778      MultiCDisDense_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDrop_6 (Dropout)       (None, 266)          0           MultiCDisDense_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_7 (Dense)        (None, 133)          35511       MultiCDisDrop_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisDense_8 (Dense)        (None, 66)           8844        MultiCDisDense_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "MultiCDisOut (Dense)            (None, 1)            67          MultiCDisDense_8[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 49,934,416\n",
      "Trainable params: 49,793,148\n",
      "Non-trainable params: 141,268\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(txt_shape)\n",
    "multi_cdis_model = create_multi_cdiscriminator(img_shape, txt_shape, n_labels, multi_cdis_cfg)\n",
    "print(\"Multi CGAN-txt2img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "multi_cdis_model.model_name = \"Multi CGAN-txt&img Discriminator\"\n",
    "# compile model\n",
    "\n",
    "multi_cdis_model.compile(loss=multi_cdis_cfg[\"loss\"], \n",
    "                    optimizer=multi_cdis_cfg[\"optimizer\"], \n",
    "                    metrics=multi_cdis_cfg[\"metrics\"])\n",
    "\n",
    "# compile model\n",
    "multi_cdis_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Model definition\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "MultiCGenLblIn (InputLayer)     [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MultiImgCGenLblDense_3 (Dense)  (None, 65536)        1114112     MultiCGenLblIn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenIn (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MultiImgCGenLblBN_4 (BatchNorma (None, 65536)        262144      MultiImgCGenLblDense_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiGenDense_1 (Dense)      (None, 65536)        8454144     ImgMultiCGenIn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "MultiImgCGenLblDrop_5 (Dropout) (None, 65536)        0           MultiImgCGenLblBN_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenReshape_2 (Reshape) (None, 8, 8, 1024)   0           ImgMultiGenDense_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "MultiImgCGenLblOut (Reshape)    (None, 8, 8, 1024)   0           MultiImgCGenLblDrop_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConcat (Concatenate (None, 8, 8, 2048)   0           ImgMultiCGenReshape_2[0][0]      \n",
      "                                                                 MultiImgCGenLblOut[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConv2D_3 (Conv2DTra (None, 16, 16, 1024) 52429824    ImgMultiCGenConcat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiGenBN_4 (BatchNormaliza (None, 16, 16, 1024) 4096        ImgMultiCGenConv2D_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenDrop_5 (Dropout)    (None, 16, 16, 1024) 0           ImgMultiGenBN_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCGenLblDense_3 (Dense)  (None, 2130)         36210       MultiCGenLblIn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConv2D_6 (Conv2DTra (None, 32, 32, 512)  13107712    ImgMultiCGenDrop_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCGenLblBN_4 (BatchNorma (None, 2130)         8520        MultiTxtCGenLblDense_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenBN_7 (BatchNormaliz (None, 32, 32, 512)  2048        ImgMultiCGenConv2D_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiGenDense_2 (Dense)      (None, 2130)         274770      ImgMultiCGenIn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCGenLblDrop_5 (Dropout) (None, 2130)         0           MultiTxtCGenLblBN_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenDrop_8 (Dropout)    (None, 32, 32, 512)  0           ImgMultiCGenBN_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenReshape_3 (Reshape) (None, 15, 142)      0           TxtMultiGenDense_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "MultiTxtCGenLblOut (Reshape)    (None, 15, 142)      0           MultiTxtCGenLblDrop_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConv2D_9 (Conv2DTra (None, 64, 64, 256)  3277056     ImgMultiCGenDrop_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenConcat (Concatenate (None, 15, 284)      0           TxtMultiCGenReshape_3[0][0]      \n",
      "                                                                 MultiTxtCGenLblOut[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenBN_10 (BatchNormali (None, 64, 64, 256)  1024        ImgMultiCGenConv2D_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenMask_4 (Masking)    (None, 15, 284)      0           TxtMultiCGenConcat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenDrop_11 (Dropout)   (None, 64, 64, 256)  0           ImgMultiCGenBN_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenLSTM_5 (LSTM)       (None, 15, 100)      154000      TxtMultiCGenMask_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConv2D_123 (Conv2DT (None, 128, 128, 128 819328      ImgMultiCGenDrop_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenBN_6 (BatchNormaliz (None, 15, 100)      400         TxtMultiCGenLSTM_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenBN_13 (BatchNormali (None, 128, 128, 128 512         ImgMultiCGenConv2D_123[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenDrop_7 (Dropout)    (None, 15, 100)      0           TxtMultiCGenBN_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenDrop_14 (Dropout)   (None, 128, 128, 128 0           ImgMultiCGenBN_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenLSTM_8 (LSTM)       (None, 15, 200)      240800      TxtMultiCGenDrop_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenConv2D_15 (Conv2DTr (None, 128, 128, 64) 73792       ImgMultiCGenDrop_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenBN_9 (BatchNormaliz (None, 15, 200)      800         TxtMultiCGenLSTM_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenBN_16 (BatchNormali (None, 128, 128, 64) 256         ImgMultiCGenConv2D_15[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenDrop_10 (Dropout)   (None, 15, 200)      0           TxtMultiCGenBN_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenDrop_17 (Dropout)   (None, 128, 128, 64) 0           ImgMultiCGenBN_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenLSTM_11 (LSTM)      (None, 15, 400)      961600      TxtMultiCGenDrop_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgMultiCGenOut (Conv2D)        (None, 128, 128, 3)  1731        ImgMultiCGenDrop_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "TxtMultiCGenOut (TimeDistribute (None, 15, 142)      56942       TxtMultiCGenLSTM_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Functional)            (None, 1)            49934416    ImgMultiCGenOut[0][0]            \n",
      "                                                                 TxtMultiCGenOut[0][0]            \n",
      "                                                                 MultiCGenLblIn[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 131,216,237\n",
      "Trainable params: 81,141,921\n",
      "Non-trainable params: 50,074,316\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi CGAN-txt2img Model definition\")\n",
    "multi_cgan_model = create_multi_cgan(multi_cgen_model, multi_cdis_model, gan_cfg)\n",
    "multi_cgan_model.summary()\n",
    "multi_cgan_model.model_name = \"Multi CGAN-txt&img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-21 09:19:13\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(multi_cgen_model, model_fn_path, multi_cgen_model.model_name, timestamp)\n",
    "export_model(multi_cdis_model, model_fn_path, multi_cdis_model.model_name, timestamp)\n",
    "export_model(multi_cgan_model, model_fn_path, multi_cgan_model.model_name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-Images: (59, 128, 128, 3) \n-Text: (59, 15, 142) \n-Real/Fake: (59, 1) \n-txt&img Labels: (59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"-Images:\", X_img.shape, \"\\n-Text:\", X_txt.shape, \"\\n-Real/Fake:\", y.shape, \"\\n-txt&img Labels:\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Training Config:\n dict_keys(['max_epochs', 'latent_dims', 'trained_epochs', 'batch_size', 'synth_batch', 'balance_batch', 'gen_sample_size', 'models_fn_path', 'report_fn_path', 'learning_history', 'dis_model_name', 'gen_model_name', 'gan_model_name', 'check_epochs', 'save_epochs', 'max_save_models', 'pretrained', 'conditioned', 'dataset_size', 'img_shape', 'txt_shape', 'label_shape', 'cat_shape', 'data_cols', 'bow_lexicon', 'tfidf_lexicon'])\n"
     ]
    }
   ],
   "source": [
    "# training and batch size\n",
    "gan_train_cfg = {\n",
    "    \"max_epochs\": 1000,\n",
    "    \"latent_dims\": latent_dims,\n",
    "    # \"max_epochs\": ini_config.get(\"Training\", \"MaxEpochs\"),\n",
    "    \"trained_epochs\": 0,\n",
    "    \"batch_size\": 32,\n",
    "    \"synth_batch\": 1,\n",
    "    \"balance_batch\": False,\n",
    "    \"gen_sample_size\": 3,\n",
    "    \"models_fn_path\": model_fn_path,\n",
    "    \"report_fn_path\": report_fn_path,\n",
    "    # \"ini_fn_path\": ini_fn_path,\n",
    "    # \"ini_cfg_fn\": ini_fn,\n",
    "    \"learning_history\": None,\n",
    "    \"dis_model_name\": multi_cgen_model.model_name,\n",
    "    \"gen_model_name\": multi_cdis_model.model_name,\n",
    "    \"gan_model_name\": multi_cgan_model.model_name,\n",
    "    # \"dis_model_name\": cdis_img_model.model_name,\n",
    "    # \"gen_model_name\": cgen_img_model.model_name,\n",
    "    # \"gan_model_name\": cgan_img_model.model_name,\n",
    "    # \"dis_model_name\": dis_model.model_name,\n",
    "    # \"gen_model_name\": gen_model.model_name,\n",
    "    # \"gan_model_name\": gan_model.model_name,\n",
    "    \"check_epochs\": 10*1,\n",
    "    \"save_epochs\": 50*1,\n",
    "    \"max_save_models\": 3,\n",
    "    \"latent_dims\": latent_dims, # X_txt[0].shape,\n",
    "    \"pretrained\": False,\n",
    "    \"conditioned\": True,\n",
    "    \"dataset_size\": X_img.shape[0],\n",
    "    \"img_shape\": X_img[0].shape,\n",
    "    \"txt_shape\": X_txt[0].shape,\n",
    "    \"label_shape\": y_labels[0].shape,\n",
    "    \"cat_shape\": y[0].shape,\n",
    "    # \"data_cols\": 2,\n",
    "    # \"data_cols\": 3,\n",
    "    \"data_cols\": 4,\n",
    "    \"bow_lexicon\": load_lexicon(lex_fn_path),\n",
    "    \"tfidf_lexicon\": format_tfidf_tokens(tfidf_tokens.values),\n",
    "    }\n",
    "\n",
    "print(\"Model Training Config:\\n\", gan_train_cfg.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 128, 128, 3) (59, 15, 142) (59, 16) (59, 1)\n4\n"
     ]
    }
   ],
   "source": [
    "# gan_data = (X_img, y)\n",
    "# gan_data = (X_img, y_labels, y)\n",
    "gan_data = (X_img, X_txt, y_labels, y)\n",
    "print(X_img.shape, X_txt.shape, y_labels.shape, y.shape)\n",
    "print(len(gan_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 554, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 0.309, acc: 0.000]\n",
      "Epoch:554 elapsed time: 25.01 [s]\n",
      ">>> Epoch: 555, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 0.375, acc: 0.000]\n",
      "Epoch:555 elapsed time: 24.95 [s]\n",
      ">>> Epoch: 556, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.293, acc: 0.062] || [Gen loss: 0.267, acc: 0.000]\n",
      "Epoch:556 elapsed time: 23.62 [s]\n",
      ">>> Epoch: 557, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 0.294, acc: 0.000]\n",
      "Epoch:557 elapsed time: 23.63 [s]\n",
      ">>> Epoch: 558, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.347, acc: 0.062] || [Gen loss: 0.184, acc: 0.000]\n",
      "Epoch:558 elapsed time: 23.64 [s]\n",
      ">>> Epoch: 559, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 0.249, acc: 0.000]\n",
      "Epoch:559 elapsed time: 24.34 [s]\n",
      ">>> Epoch: 560, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.061, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 0.311, acc: 0.000]\n",
      "Epoch:560 elapsed time: 24.65 [s]\n",
      ">>> Epoch: 561, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.469, acc: 0.062] || [Gen loss: 0.250, acc: 0.000]\n",
      "Epoch: 561 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.507\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.072\n",
      "Ploting results\n",
      "Epoch:561 elapsed time: 47.95 [s]\n",
      ">>> Epoch: 562, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.347, acc: 0.354] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 0.040, acc: 0.000]\n",
      "Epoch:562 elapsed time: 23.50 [s]\n",
      ">>> Epoch: 563, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.364, acc: 0.000]\n",
      "Epoch:563 elapsed time: 23.72 [s]\n",
      ">>> Epoch: 564, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.348, acc: 0.062] || [Gen loss: 0.186, acc: 0.000]\n",
      "Epoch:564 elapsed time: 24.30 [s]\n",
      ">>> Epoch: 565, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 0.172, acc: 0.000]\n",
      "Epoch:565 elapsed time: 25.25 [s]\n",
      ">>> Epoch: 566, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.313, acc: 0.062] || [Gen loss: 0.263, acc: 0.000]\n",
      "Epoch:566 elapsed time: 24.61 [s]\n",
      ">>> Epoch: 567, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.336, acc: 0.062] || [Gen loss: 0.258, acc: 0.000]\n",
      "Epoch:567 elapsed time: 23.85 [s]\n",
      ">>> Epoch: 568, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.392, acc: 0.000]\n",
      "Epoch:568 elapsed time: 23.80 [s]\n",
      ">>> Epoch: 569, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 0.250, acc: 0.000]\n",
      "Epoch:569 elapsed time: 23.85 [s]\n",
      ">>> Epoch: 570, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.370, acc: 0.062] || [Gen loss: 0.214, acc: 0.000]\n",
      "Epoch:570 elapsed time: 24.59 [s]\n",
      ">>> Epoch: 571, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.353, acc: 0.062] || [Gen loss: 0.214, acc: 0.000]\n",
      "Epoch: 571 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.529\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.045\n",
      "Ploting results\n",
      "Epoch:571 elapsed time: 48.50 [s]\n",
      ">>> Epoch: 572, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.362, acc: 0.354] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch:572 elapsed time: 23.66 [s]\n",
      ">>> Epoch: 573, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.466, acc: 0.062] || [Gen loss: 0.331, acc: 0.000]\n",
      "Epoch:573 elapsed time: 23.62 [s]\n",
      ">>> Epoch: 574, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 0.277, acc: 0.000]\n",
      "Epoch:574 elapsed time: 23.39 [s]\n",
      ">>> Epoch: 575, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.145, acc: 0.000]\n",
      "Epoch:575 elapsed time: 23.97 [s]\n",
      ">>> Epoch: 576, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 0.388, acc: 0.000]\n",
      "Epoch:576 elapsed time: 24.21 [s]\n",
      ">>> Epoch: 577, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.461, acc: 0.062] || [Gen loss: 0.246, acc: 0.000]\n",
      "Epoch:577 elapsed time: 24.90 [s]\n",
      ">>> Epoch: 578, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.126, acc: 0.000]\n",
      "Epoch:578 elapsed time: 24.59 [s]\n",
      ">>> Epoch: 579, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.352, acc: 0.000]\n",
      "Epoch:579 elapsed time: 23.62 [s]\n",
      ">>> Epoch: 580, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.435, acc: 0.062] || [Gen loss: 0.298, acc: 0.000]\n",
      "Epoch:580 elapsed time: 23.53 [s]\n",
      ">>> Epoch: 581, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.411, acc: 0.000]\n",
      "Epoch: 581 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.478\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.053\n",
      "Ploting results\n",
      "Epoch:581 elapsed time: 46.92 [s]\n",
      ">>> Epoch: 582, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.330, acc: 0.354] || [F-Dis loss: 0.458, acc: 0.062] || [Gen loss: 0.179, acc: 0.000]\n",
      "Epoch:582 elapsed time: 23.70 [s]\n",
      ">>> Epoch: 583, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.342, acc: 0.062] || [Gen loss: 0.304, acc: 0.000]\n",
      "Epoch:583 elapsed time: 25.73 [s]\n",
      ">>> Epoch: 584, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.058, acc: 0.938] || [F-Dis loss: 0.363, acc: 0.062] || [Gen loss: 0.242, acc: 0.000]\n",
      "Epoch:584 elapsed time: 24.81 [s]\n",
      ">>> Epoch: 585, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.137, acc: 0.000]\n",
      "Epoch:585 elapsed time: 24.12 [s]\n",
      ">>> Epoch: 586, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 0.334, acc: 0.000]\n",
      "Epoch:586 elapsed time: 23.61 [s]\n",
      ">>> Epoch: 587, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.362, acc: 0.062] || [Gen loss: 0.237, acc: 0.000]\n",
      "Epoch:587 elapsed time: 23.66 [s]\n",
      ">>> Epoch: 588, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.371, acc: 0.062] || [Gen loss: 0.151, acc: 0.000]\n",
      "Epoch:588 elapsed time: 23.64 [s]\n",
      ">>> Epoch: 589, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 0.196, acc: 0.000]\n",
      "Epoch:589 elapsed time: 23.53 [s]\n",
      ">>> Epoch: 590, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.342, acc: 0.062] || [Gen loss: 0.231, acc: 0.000]\n",
      "Epoch:590 elapsed time: 25.17 [s]\n",
      ">>> Epoch: 591, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.458, acc: 0.062] || [Gen loss: 0.331, acc: 0.000]\n",
      "Epoch: 591 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.508\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.027\n",
      "Ploting results\n",
      "Epoch:591 elapsed time: 47.50 [s]\n",
      ">>> Epoch: 592, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.351, acc: 0.354] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.177, acc: 0.000]\n",
      "Epoch:592 elapsed time: 23.17 [s]\n",
      ">>> Epoch: 593, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.469, acc: 0.062] || [Gen loss: 0.176, acc: 0.000]\n",
      "Epoch:593 elapsed time: 23.41 [s]\n",
      ">>> Epoch: 594, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.365, acc: 0.062] || [Gen loss: 0.202, acc: 0.000]\n",
      "Epoch:594 elapsed time: 23.86 [s]\n",
      ">>> Epoch: 595, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 0.394, acc: 0.000]\n",
      "Epoch:595 elapsed time: 25.27 [s]\n",
      ">>> Epoch: 596, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.169, acc: 0.000]\n",
      "Epoch:596 elapsed time: 24.19 [s]\n",
      ">>> Epoch: 597, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.529, acc: 0.062] || [Gen loss: 0.291, acc: 0.000]\n",
      "Epoch:597 elapsed time: 23.86 [s]\n",
      ">>> Epoch: 598, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 0.149, acc: 0.000]\n",
      "Epoch:598 elapsed time: 23.57 [s]\n",
      ">>> Epoch: 599, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.358, acc: 0.062] || [Gen loss: 0.267, acc: 0.000]\n",
      "Epoch:599 elapsed time: 23.68 [s]\n",
      ">>> Epoch: 600, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.441, acc: 0.062] || [Gen loss: 0.163, acc: 0.000]\n",
      "Epoch:600 elapsed time: 25.11 [s]\n",
      ">>> Epoch: 601, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.477, acc: 0.062] || [Gen loss: 0.137, acc: 0.000]\n",
      "Epoch: 601 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.544\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.040\n",
      "Ploting results\n",
      "Epoch: 601 Saving the training progress...\n",
      "Epoch:601 elapsed time: 51.86 [s]\n",
      ">>> Epoch: 602, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.373, acc: 0.354] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 0.402, acc: 0.000]\n",
      "Epoch:602 elapsed time: 25.62 [s]\n",
      ">>> Epoch: 603, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.459, acc: 0.062] || [Gen loss: 0.339, acc: 0.000]\n",
      "Epoch:603 elapsed time: 24.97 [s]\n",
      ">>> Epoch: 604, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.317, acc: 0.000]\n",
      "Epoch:604 elapsed time: 23.93 [s]\n",
      ">>> Epoch: 605, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.358, acc: 0.000]\n",
      "Epoch:605 elapsed time: 23.94 [s]\n",
      ">>> Epoch: 606, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 0.133, acc: 0.000]\n",
      "Epoch:606 elapsed time: 23.99 [s]\n",
      ">>> Epoch: 607, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.433, acc: 0.062] || [Gen loss: 0.322, acc: 0.000]\n",
      "Epoch:607 elapsed time: 24.94 [s]\n",
      ">>> Epoch: 608, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.456, acc: 0.062] || [Gen loss: 0.352, acc: 0.000]\n",
      "Epoch:608 elapsed time: 24.84 [s]\n",
      ">>> Epoch: 609, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 0.278, acc: 0.000]\n",
      "Epoch:609 elapsed time: 24.26 [s]\n",
      ">>> Epoch: 610, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 0.275, acc: 0.000]\n",
      "Epoch:610 elapsed time: 23.63 [s]\n",
      ">>> Epoch: 611, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 0.334, acc: 0.000]\n",
      "Epoch: 611 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.592\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.020\n",
      "Ploting results\n",
      "Epoch:611 elapsed time: 48.11 [s]\n",
      ">>> Epoch: 612, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.403, acc: 0.354] || [F-Dis loss: 0.340, acc: 0.062] || [Gen loss: 0.123, acc: 0.000]\n",
      "Epoch:612 elapsed time: 23.92 [s]\n",
      ">>> Epoch: 613, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.283, acc: 0.000]\n",
      "Epoch:613 elapsed time: 25.35 [s]\n",
      ">>> Epoch: 614, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 0.343, acc: 0.000]\n",
      "Epoch:614 elapsed time: 24.51 [s]\n",
      ">>> Epoch: 615, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.062, acc: 0.938] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 0.176, acc: 0.000]\n",
      "Epoch:615 elapsed time: 23.95 [s]\n",
      ">>> Epoch: 616, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.347, acc: 0.000]\n",
      "Epoch:616 elapsed time: 23.80 [s]\n",
      ">>> Epoch: 617, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.301, acc: 0.000]\n",
      "Epoch:617 elapsed time: 24.04 [s]\n",
      ">>> Epoch: 618, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.051, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.231, acc: 0.000]\n",
      "Epoch:618 elapsed time: 23.93 [s]\n",
      ">>> Epoch: 619, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.093, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 0.326, acc: 0.000]\n",
      "Epoch:619 elapsed time: 23.79 [s]\n",
      ">>> Epoch: 620, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.251, acc: 0.000]\n",
      "Epoch:620 elapsed time: 25.05 [s]\n",
      ">>> Epoch: 621, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.049, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.378, acc: 0.000]\n",
      "Epoch: 621 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.482\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.068\n",
      "Ploting results\n",
      "Epoch:621 elapsed time: 49.31 [s]\n",
      ">>> Epoch: 622, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.334, acc: 0.354] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 0.245, acc: 0.000]\n",
      "Epoch:622 elapsed time: 24.86 [s]\n",
      ">>> Epoch: 623, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 0.193, acc: 0.000]\n",
      "Epoch:623 elapsed time: 25.03 [s]\n",
      ">>> Epoch: 624, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.306, acc: 0.062] || [Gen loss: 0.233, acc: 0.000]\n",
      "Epoch:624 elapsed time: 24.96 [s]\n",
      ">>> Epoch: 625, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.094, acc: 0.000]\n",
      "Epoch:625 elapsed time: 25.03 [s]\n",
      ">>> Epoch: 626, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.466, acc: 0.062] || [Gen loss: 0.221, acc: 0.000]\n",
      "Epoch:626 elapsed time: 24.67 [s]\n",
      ">>> Epoch: 627, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.328, acc: 0.062] || [Gen loss: 0.213, acc: 0.000]\n",
      "Epoch:627 elapsed time: 25.51 [s]\n",
      ">>> Epoch: 628, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.483, acc: 0.062] || [Gen loss: 0.071, acc: 0.000]\n",
      "Epoch:628 elapsed time: 26.81 [s]\n",
      ">>> Epoch: 629, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 0.230, acc: 0.000]\n",
      "Epoch:629 elapsed time: 25.13 [s]\n",
      ">>> Epoch: 630, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 0.205, acc: 0.000]\n",
      "Epoch:630 elapsed time: 24.53 [s]\n",
      ">>> Epoch: 631, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.500, acc: 0.062] || [Gen loss: 0.136, acc: 0.000]\n",
      "Epoch: 631 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.354\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.038\n",
      "Ploting results\n",
      "Epoch:631 elapsed time: 48.88 [s]\n",
      ">>> Epoch: 632, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.251, acc: 0.354] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.293, acc: 0.000]\n",
      "Epoch:632 elapsed time: 23.24 [s]\n",
      ">>> Epoch: 633, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.328, acc: 0.062] || [Gen loss: 0.344, acc: 0.000]\n",
      "Epoch:633 elapsed time: 23.80 [s]\n",
      ">>> Epoch: 634, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.523, acc: 0.062] || [Gen loss: 0.365, acc: 0.000]\n",
      "Epoch:634 elapsed time: 24.91 [s]\n",
      ">>> Epoch: 635, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.334, acc: 0.062] || [Gen loss: 0.047, acc: 0.000]\n",
      "Epoch:635 elapsed time: 25.04 [s]\n",
      ">>> Epoch: 636, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 0.310, acc: 0.000]\n",
      "Epoch:636 elapsed time: 24.20 [s]\n",
      ">>> Epoch: 637, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 0.333, acc: 0.000]\n",
      "Epoch:637 elapsed time: 24.06 [s]\n",
      ">>> Epoch: 638, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.009, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 0.199, acc: 0.000]\n",
      "Epoch:638 elapsed time: 25.02 [s]\n",
      ">>> Epoch: 639, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.053, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 0.335, acc: 0.000]\n",
      "Epoch:639 elapsed time: 25.19 [s]\n",
      ">>> Epoch: 640, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.446, acc: 0.062] || [Gen loss: 0.285, acc: 0.000]\n",
      "Epoch:640 elapsed time: 24.46 [s]\n",
      ">>> Epoch: 641, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.455, acc: 0.062] || [Gen loss: 0.312, acc: 0.000]\n",
      "Epoch: 641 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.420\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.019\n",
      "Ploting results\n",
      "Epoch:641 elapsed time: 47.42 [s]\n",
      ">>> Epoch: 642, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.295, acc: 0.354] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 0.369, acc: 0.000]\n",
      "Epoch:642 elapsed time: 23.29 [s]\n",
      ">>> Epoch: 643, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.320, acc: 0.000]\n",
      "Epoch:643 elapsed time: 24.10 [s]\n",
      ">>> Epoch: 644, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.333, acc: 0.000]\n",
      "Epoch:644 elapsed time: 25.38 [s]\n",
      ">>> Epoch: 645, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 0.365, acc: 0.000]\n",
      "Epoch:645 elapsed time: 24.62 [s]\n",
      ">>> Epoch: 646, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.092, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.582, acc: 0.000]\n",
      "Epoch:646 elapsed time: 23.99 [s]\n",
      ">>> Epoch: 647, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.066, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.657, acc: 0.000]\n",
      "Epoch:647 elapsed time: 23.92 [s]\n",
      ">>> Epoch: 648, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.479, acc: 0.000]\n",
      "Epoch:648 elapsed time: 23.82 [s]\n",
      ">>> Epoch: 649, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.335, acc: 0.000]\n",
      "Epoch:649 elapsed time: 24.91 [s]\n",
      ">>> Epoch: 650, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.053, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.318, acc: 0.000]\n",
      "Epoch:650 elapsed time: 25.41 [s]\n",
      ">>> Epoch: 651, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 0.409, acc: 0.000]\n",
      "Epoch: 651 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.401\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.025\n",
      "Ploting results\n",
      "Epoch: 651 Saving the training progress...\n",
      "Epoch:651 elapsed time: 55.44 [s]\n",
      ">>> Epoch: 652, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.277, acc: 0.354] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.284, acc: 0.000]\n",
      "Epoch:652 elapsed time: 25.26 [s]\n",
      ">>> Epoch: 653, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.499, acc: 0.062] || [Gen loss: 0.451, acc: 0.000]\n",
      "Epoch:653 elapsed time: 24.61 [s]\n",
      ">>> Epoch: 654, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.385, acc: 0.000]\n",
      "Epoch:654 elapsed time: 23.81 [s]\n",
      ">>> Epoch: 655, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.365, acc: 0.062] || [Gen loss: 0.283, acc: 0.000]\n",
      "Epoch:655 elapsed time: 24.53 [s]\n",
      ">>> Epoch: 656, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.049, acc: 0.938] || [F-Dis loss: 0.400, acc: 0.062] || [Gen loss: 0.250, acc: 0.000]\n",
      "Epoch:656 elapsed time: 24.67 [s]\n",
      ">>> Epoch: 657, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.405, acc: 0.062] || [Gen loss: 0.479, acc: 0.000]\n",
      "Epoch:657 elapsed time: 24.20 [s]\n",
      ">>> Epoch: 658, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.306, acc: 0.062] || [Gen loss: 0.335, acc: 0.000]\n",
      "Epoch:658 elapsed time: 23.91 [s]\n",
      ">>> Epoch: 659, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.349, acc: 0.000]\n",
      "Epoch:659 elapsed time: 23.75 [s]\n",
      ">>> Epoch: 660, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.077, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.414, acc: 0.000]\n",
      "Epoch:660 elapsed time: 25.40 [s]\n",
      ">>> Epoch: 661, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.479, acc: 0.000]\n",
      "Epoch: 661 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.470\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.010\n",
      "Ploting results\n",
      "Epoch:661 elapsed time: 48.29 [s]\n",
      ">>> Epoch: 662, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.330, acc: 0.354] || [F-Dis loss: 0.358, acc: 0.062] || [Gen loss: 0.442, acc: 0.000]\n",
      "Epoch:662 elapsed time: 23.64 [s]\n",
      ">>> Epoch: 663, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.306, acc: 0.000]\n",
      "Epoch:663 elapsed time: 23.93 [s]\n",
      ">>> Epoch: 664, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.449, acc: 0.000]\n",
      "Epoch:664 elapsed time: 23.86 [s]\n",
      ">>> Epoch: 665, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.063, acc: 0.938] || [F-Dis loss: 0.384, acc: 0.062] || [Gen loss: 0.657, acc: 0.000]\n",
      "Epoch:665 elapsed time: 23.80 [s]\n",
      ">>> Epoch: 666, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.066, acc: 0.938] || [F-Dis loss: 0.456, acc: 0.062] || [Gen loss: 0.458, acc: 0.000]\n",
      "Epoch:666 elapsed time: 24.07 [s]\n",
      ">>> Epoch: 667, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.517, acc: 0.000]\n",
      "Epoch:667 elapsed time: 24.58 [s]\n",
      ">>> Epoch: 668, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.534, acc: 0.062] || [Gen loss: 0.371, acc: 0.000]\n",
      "Epoch:668 elapsed time: 24.23 [s]\n",
      ">>> Epoch: 669, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.357, acc: 0.062] || [Gen loss: 0.492, acc: 0.000]\n",
      "Epoch:669 elapsed time: 23.89 [s]\n",
      ">>> Epoch: 670, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 0.258, acc: 0.000]\n",
      "Epoch:670 elapsed time: 23.67 [s]\n",
      ">>> Epoch: 671, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.355, acc: 0.062] || [Gen loss: 0.546, acc: 0.000]\n",
      "Epoch: 671 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.439\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.032\n",
      "Ploting results\n",
      "Epoch:671 elapsed time: 47.22 [s]\n",
      ">>> Epoch: 672, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.301, acc: 0.354] || [F-Dis loss: 0.379, acc: 0.062] || [Gen loss: 0.362, acc: 0.000]\n",
      "Epoch:672 elapsed time: 23.65 [s]\n",
      ">>> Epoch: 673, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.056, acc: 0.938] || [F-Dis loss: 0.369, acc: 0.062] || [Gen loss: 0.297, acc: 0.000]\n",
      "Epoch:673 elapsed time: 24.54 [s]\n",
      ">>> Epoch: 674, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 0.433, acc: 0.000]\n",
      "Epoch:674 elapsed time: 24.46 [s]\n",
      ">>> Epoch: 675, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.113, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.244, acc: 0.000]\n",
      "Epoch:675 elapsed time: 23.92 [s]\n",
      ">>> Epoch: 676, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.433, acc: 0.062] || [Gen loss: 0.437, acc: 0.000]\n",
      "Epoch:676 elapsed time: 23.87 [s]\n",
      ">>> Epoch: 677, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.349, acc: 0.062] || [Gen loss: 0.460, acc: 0.000]\n",
      "Epoch:677 elapsed time: 23.85 [s]\n",
      ">>> Epoch: 678, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.477, acc: 0.062] || [Gen loss: 0.367, acc: 0.000]\n",
      "Epoch:678 elapsed time: 24.30 [s]\n",
      ">>> Epoch: 679, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 0.183, acc: 0.000]\n",
      "Epoch:679 elapsed time: 23.96 [s]\n",
      ">>> Epoch: 680, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.455, acc: 0.062] || [Gen loss: 0.271, acc: 0.000]\n",
      "Epoch:680 elapsed time: 24.50 [s]\n",
      ">>> Epoch: 681, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.354, acc: 0.000]\n",
      "Epoch: 681 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.452\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.039\n",
      "Ploting results\n",
      "Epoch:681 elapsed time: 47.47 [s]\n",
      ">>> Epoch: 682, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.319, acc: 0.354] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.322, acc: 0.000]\n",
      "Epoch:682 elapsed time: 23.43 [s]\n",
      ">>> Epoch: 683, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.052, acc: 0.938] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 0.264, acc: 0.000]\n",
      "Epoch:683 elapsed time: 24.04 [s]\n",
      ">>> Epoch: 684, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 0.304, acc: 0.000]\n",
      "Epoch:684 elapsed time: 24.90 [s]\n",
      ">>> Epoch: 685, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.383, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch:685 elapsed time: 25.62 [s]\n",
      ">>> Epoch: 686, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.425, acc: 0.062] || [Gen loss: 0.363, acc: 0.000]\n",
      "Epoch:686 elapsed time: 24.60 [s]\n",
      ">>> Epoch: 687, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.052, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 0.221, acc: 0.000]\n",
      "Epoch:687 elapsed time: 24.39 [s]\n",
      ">>> Epoch: 688, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.068, acc: 0.938] || [F-Dis loss: 0.525, acc: 0.062] || [Gen loss: 0.222, acc: 0.000]\n",
      "Epoch:688 elapsed time: 23.69 [s]\n",
      ">>> Epoch: 689, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.177, acc: 0.000]\n",
      "Epoch:689 elapsed time: 23.85 [s]\n",
      ">>> Epoch: 690, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 0.099, acc: 0.000]\n",
      "Epoch:690 elapsed time: 23.71 [s]\n",
      ">>> Epoch: 691, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.475, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch: 691 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.537\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.035\n",
      "Ploting results\n",
      "Epoch:691 elapsed time: 48.59 [s]\n",
      ">>> Epoch: 692, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.364, acc: 0.354] || [F-Dis loss: 0.479, acc: 0.062] || [Gen loss: 0.349, acc: 0.000]\n",
      "Epoch:692 elapsed time: 24.89 [s]\n",
      ">>> Epoch: 693, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.059, acc: 0.938] || [F-Dis loss: 0.416, acc: 0.062] || [Gen loss: 0.310, acc: 0.000]\n",
      "Epoch:693 elapsed time: 24.07 [s]\n",
      ">>> Epoch: 694, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.409, acc: 0.062] || [Gen loss: 0.299, acc: 0.000]\n",
      "Epoch:694 elapsed time: 23.64 [s]\n",
      ">>> Epoch: 695, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.006, acc: 0.938] || [F-Dis loss: 0.446, acc: 0.062] || [Gen loss: 0.291, acc: 0.000]\n",
      "Epoch:695 elapsed time: 23.60 [s]\n",
      ">>> Epoch: 696, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 0.307, acc: 0.000]\n",
      "Epoch:696 elapsed time: 23.74 [s]\n",
      ">>> Epoch: 697, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 0.278, acc: 0.000]\n",
      "Epoch:697 elapsed time: 23.84 [s]\n",
      ">>> Epoch: 698, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 0.319, acc: 0.000]\n",
      "Epoch:698 elapsed time: 24.68 [s]\n",
      ">>> Epoch: 699, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 0.308, acc: 0.000]\n",
      "Epoch:699 elapsed time: 24.71 [s]\n",
      ">>> Epoch: 700, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.008, acc: 0.938] || [F-Dis loss: 0.483, acc: 0.062] || [Gen loss: 0.240, acc: 0.000]\n",
      "Epoch:700 elapsed time: 24.05 [s]\n",
      ">>> Epoch: 701, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.311, acc: 0.000]\n",
      "Epoch: 701 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.544\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.037\n",
      "Ploting results\n",
      "Epoch: 701 Saving the training progress...\n",
      "Epoch:701 elapsed time: 50.14 [s]\n",
      ">>> Epoch: 702, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.373, acc: 0.354] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 0.275, acc: 0.000]\n",
      "Epoch:702 elapsed time: 25.83 [s]\n",
      ">>> Epoch: 703, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.271, acc: 0.062] || [Gen loss: 0.201, acc: 0.000]\n",
      "Epoch:703 elapsed time: 26.03 [s]\n",
      ">>> Epoch: 704, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.047, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 0.286, acc: 0.000]\n",
      "Epoch:704 elapsed time: 24.57 [s]\n",
      ">>> Epoch: 705, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.386, acc: 0.062] || [Gen loss: 0.204, acc: 0.000]\n",
      "Epoch:705 elapsed time: 23.99 [s]\n",
      ">>> Epoch: 706, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.448, acc: 0.062] || [Gen loss: 0.215, acc: 0.000]\n",
      "Epoch:706 elapsed time: 24.05 [s]\n",
      ">>> Epoch: 707, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 0.211, acc: 0.000]\n",
      "Epoch:707 elapsed time: 24.42 [s]\n",
      ">>> Epoch: 708, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.361, acc: 0.062] || [Gen loss: 0.485, acc: 0.000]\n",
      "Epoch:708 elapsed time: 24.45 [s]\n",
      ">>> Epoch: 709, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.413, acc: 0.062] || [Gen loss: 0.532, acc: 0.000]\n",
      "Epoch:709 elapsed time: 23.98 [s]\n",
      ">>> Epoch: 710, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.460, acc: 0.062] || [Gen loss: 0.267, acc: 0.000]\n",
      "Epoch:710 elapsed time: 24.54 [s]\n",
      ">>> Epoch: 711, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.327, acc: 0.000]\n",
      "Epoch: 711 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.517\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.039\n",
      "Ploting results\n",
      "Epoch:711 elapsed time: 54.18 [s]\n",
      ">>> Epoch: 712, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.357, acc: 0.354] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.291, acc: 0.000]\n",
      "Epoch:712 elapsed time: 24.69 [s]\n",
      ">>> Epoch: 713, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.267, acc: 0.000]\n",
      "Epoch:713 elapsed time: 25.76 [s]\n",
      ">>> Epoch: 714, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.321, acc: 0.062] || [Gen loss: 0.519, acc: 0.000]\n",
      "Epoch:714 elapsed time: 25.47 [s]\n",
      ">>> Epoch: 715, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 0.107, acc: 0.000]\n",
      "Epoch:715 elapsed time: 24.74 [s]\n",
      ">>> Epoch: 716, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch:716 elapsed time: 24.04 [s]\n",
      ">>> Epoch: 717, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.060, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.262, acc: 0.000]\n",
      "Epoch:717 elapsed time: 23.97 [s]\n",
      ">>> Epoch: 718, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.491, acc: 0.062] || [Gen loss: 0.297, acc: 0.000]\n",
      "Epoch:718 elapsed time: 23.70 [s]\n",
      ">>> Epoch: 719, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.468, acc: 0.062] || [Gen loss: 0.165, acc: 0.000]\n",
      "Epoch:719 elapsed time: 24.18 [s]\n",
      ">>> Epoch: 720, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 0.438, acc: 0.000]\n",
      "Epoch:720 elapsed time: 24.56 [s]\n",
      ">>> Epoch: 721, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 0.251, acc: 0.000]\n",
      "Epoch: 721 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.478\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.037\n",
      "Ploting results\n",
      "Epoch:721 elapsed time: 54.34 [s]\n",
      ">>> Epoch: 722, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.341, acc: 0.354] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.387, acc: 0.000]\n",
      "Epoch:722 elapsed time: 25.72 [s]\n",
      ">>> Epoch: 723, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 0.306, acc: 0.000]\n",
      "Epoch:723 elapsed time: 23.57 [s]\n",
      ">>> Epoch: 724, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 0.244, acc: 0.000]\n",
      "Epoch:724 elapsed time: 22.51 [s]\n",
      ">>> Epoch: 725, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.374, acc: 0.062] || [Gen loss: 0.376, acc: 0.000]\n",
      "Epoch:725 elapsed time: 22.89 [s]\n",
      ">>> Epoch: 726, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.052, acc: 0.938] || [F-Dis loss: 0.359, acc: 0.062] || [Gen loss: 0.410, acc: 0.000]\n",
      "Epoch:726 elapsed time: 24.84 [s]\n",
      ">>> Epoch: 727, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.048, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.383, acc: 0.000]\n",
      "Epoch:727 elapsed time: 25.53 [s]\n",
      ">>> Epoch: 728, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.070, acc: 0.938] || [F-Dis loss: 0.409, acc: 0.062] || [Gen loss: 0.194, acc: 0.000]\n",
      "Epoch:728 elapsed time: 24.93 [s]\n",
      ">>> Epoch: 729, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.053, acc: 0.938] || [F-Dis loss: 0.386, acc: 0.062] || [Gen loss: 0.183, acc: 0.000]\n",
      "Epoch:729 elapsed time: 24.21 [s]\n",
      ">>> Epoch: 730, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.443, acc: 0.062] || [Gen loss: 0.257, acc: 0.000]\n",
      "Epoch:730 elapsed time: 24.10 [s]\n",
      ">>> Epoch: 731, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 0.022, acc: 0.000]\n",
      "Epoch: 731 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 2.689\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.112\n",
      "Ploting results\n",
      "Epoch:731 elapsed time: 47.13 [s]\n",
      ">>> Epoch: 732, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 1.809, acc: 0.354] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 0.307, acc: 0.000]\n",
      "Epoch:732 elapsed time: 23.61 [s]\n",
      ">>> Epoch: 733, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.458, acc: 0.062] || [Gen loss: 0.262, acc: 0.000]\n",
      "Epoch:733 elapsed time: 24.41 [s]\n",
      ">>> Epoch: 734, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.388, acc: 0.062] || [Gen loss: 0.300, acc: 0.000]\n",
      "Epoch:734 elapsed time: 24.61 [s]\n",
      ">>> Epoch: 735, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 0.325, acc: 0.000]\n",
      "Epoch:735 elapsed time: 24.32 [s]\n",
      ">>> Epoch: 736, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.056, acc: 0.938] || [F-Dis loss: 0.416, acc: 0.062] || [Gen loss: 0.395, acc: 0.000]\n",
      "Epoch:736 elapsed time: 23.72 [s]\n",
      ">>> Epoch: 737, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.076, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 0.419, acc: 0.000]\n",
      "Epoch:737 elapsed time: 23.61 [s]\n",
      ">>> Epoch: 738, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.592, acc: 0.000]\n",
      "Epoch:738 elapsed time: 23.88 [s]\n",
      ">>> Epoch: 739, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.363, acc: 0.062] || [Gen loss: 0.194, acc: 0.000]\n",
      "Epoch:739 elapsed time: 25.84 [s]\n",
      ">>> Epoch: 740, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 0.223, acc: 0.000]\n",
      "Epoch:740 elapsed time: 25.31 [s]\n",
      ">>> Epoch: 741, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.413, acc: 0.062] || [Gen loss: 0.442, acc: 0.000]\n",
      "Epoch: 741 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 2.136\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.120\n",
      "Ploting results\n",
      "Epoch:741 elapsed time: 47.58 [s]\n",
      ">>> Epoch: 742, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 1.439, acc: 0.354] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 0.310, acc: 0.000]\n",
      "Epoch:742 elapsed time: 23.54 [s]\n",
      ">>> Epoch: 743, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 0.360, acc: 0.000]\n",
      "Epoch:743 elapsed time: 24.19 [s]\n",
      ">>> Epoch: 744, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.102, acc: 0.000]\n",
      "Epoch:744 elapsed time: 24.32 [s]\n",
      ">>> Epoch: 745, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 0.247, acc: 0.000]\n",
      "Epoch:745 elapsed time: 24.01 [s]\n",
      ">>> Epoch: 746, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 0.406, acc: 0.000]\n",
      "Epoch:746 elapsed time: 26.91 [s]\n",
      ">>> Epoch: 747, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.473, acc: 0.062] || [Gen loss: 0.359, acc: 0.000]\n",
      "Epoch:747 elapsed time: 26.79 [s]\n",
      ">>> Epoch: 748, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 0.344, acc: 0.000]\n",
      "Epoch:748 elapsed time: 26.06 [s]\n",
      ">>> Epoch: 749, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.469, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch:749 elapsed time: 24.83 [s]\n",
      ">>> Epoch: 750, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.365, acc: 0.000]\n",
      "Epoch:750 elapsed time: 24.19 [s]\n",
      ">>> Epoch: 751, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.400, acc: 0.062] || [Gen loss: 0.468, acc: 0.000]\n",
      "Epoch: 751 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.421\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.034\n",
      "Ploting results\n",
      "Epoch: 751 Saving the training progress...\n",
      "Epoch:751 elapsed time: 50.83 [s]\n",
      ">>> Epoch: 752, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.288, acc: 0.354] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 0.345, acc: 0.000]\n",
      "Epoch:752 elapsed time: 25.97 [s]\n",
      ">>> Epoch: 753, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.448, acc: 0.062] || [Gen loss: 0.280, acc: 0.000]\n",
      "Epoch:753 elapsed time: 29.47 [s]\n",
      ">>> Epoch: 754, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.364, acc: 0.062] || [Gen loss: 0.197, acc: 0.000]\n",
      "Epoch:754 elapsed time: 24.13 [s]\n",
      ">>> Epoch: 755, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.262, acc: 0.000]\n",
      "Epoch:755 elapsed time: 26.52 [s]\n",
      ">>> Epoch: 756, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 0.283, acc: 0.000]\n",
      "Epoch:756 elapsed time: 25.50 [s]\n",
      ">>> Epoch: 757, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 0.408, acc: 0.000]\n",
      "Epoch:757 elapsed time: 24.83 [s]\n",
      ">>> Epoch: 758, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 0.370, acc: 0.000]\n",
      "Epoch:758 elapsed time: 24.33 [s]\n",
      ">>> Epoch: 759, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.048, acc: 0.938] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 0.267, acc: 0.000]\n",
      "Epoch:759 elapsed time: 24.12 [s]\n",
      ">>> Epoch: 760, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.483, acc: 0.062] || [Gen loss: 0.314, acc: 0.000]\n",
      "Epoch:760 elapsed time: 23.72 [s]\n",
      ">>> Epoch: 761, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 0.404, acc: 0.000]\n",
      "Epoch: 761 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.484\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.030\n",
      "Ploting results\n",
      "Epoch:761 elapsed time: 47.16 [s]\n",
      ">>> Epoch: 762, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.335, acc: 0.354] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.257, acc: 0.000]\n",
      "Epoch:762 elapsed time: 25.39 [s]\n",
      ">>> Epoch: 763, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.009, acc: 0.938] || [F-Dis loss: 0.484, acc: 0.062] || [Gen loss: 0.276, acc: 0.000]\n",
      "Epoch:763 elapsed time: 24.64 [s]\n",
      ">>> Epoch: 764, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 0.304, acc: 0.000]\n",
      "Epoch:764 elapsed time: 23.87 [s]\n",
      ">>> Epoch: 765, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 0.439, acc: 0.000]\n",
      "Epoch:765 elapsed time: 23.88 [s]\n",
      ">>> Epoch: 766, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.449, acc: 0.062] || [Gen loss: 0.350, acc: 0.000]\n",
      "Epoch:766 elapsed time: 24.01 [s]\n",
      ">>> Epoch: 767, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.080, acc: 0.938] || [F-Dis loss: 0.356, acc: 0.062] || [Gen loss: 0.305, acc: 0.000]\n",
      "Epoch:767 elapsed time: 24.95 [s]\n",
      ">>> Epoch: 768, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.073, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 0.163, acc: 0.000]\n",
      "Epoch:768 elapsed time: 25.55 [s]\n",
      ">>> Epoch: 769, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.368, acc: 0.062] || [Gen loss: 0.252, acc: 0.000]\n",
      "Epoch:769 elapsed time: 25.82 [s]\n",
      ">>> Epoch: 770, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.434, acc: 0.062] || [Gen loss: 0.013, acc: 0.000]\n",
      "Epoch:770 elapsed time: 24.52 [s]\n",
      ">>> Epoch: 771, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 0.237, acc: 0.000]\n",
      "Epoch: 771 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 1.070\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.066\n",
      "Ploting results\n",
      "Epoch:771 elapsed time: 46.98 [s]\n",
      ">>> Epoch: 772, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.716, acc: 0.354] || [F-Dis loss: 0.462, acc: 0.062] || [Gen loss: 0.453, acc: 0.000]\n",
      "Epoch:772 elapsed time: 23.13 [s]\n",
      ">>> Epoch: 773, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.408, acc: 0.000]\n",
      "Epoch:773 elapsed time: 25.46 [s]\n",
      ">>> Epoch: 774, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 0.562, acc: 0.000]\n",
      "Epoch:774 elapsed time: 24.83 [s]\n",
      ">>> Epoch: 775, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.060, acc: 0.938] || [F-Dis loss: 0.374, acc: 0.062] || [Gen loss: 0.298, acc: 0.000]\n",
      "Epoch:775 elapsed time: 25.71 [s]\n",
      ">>> Epoch: 776, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.337, acc: 0.000]\n",
      "Epoch:776 elapsed time: 25.51 [s]\n",
      ">>> Epoch: 777, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.009, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.324, acc: 0.000]\n",
      "Epoch:777 elapsed time: 25.47 [s]\n",
      ">>> Epoch: 778, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.070, acc: 0.938] || [F-Dis loss: 0.469, acc: 0.062] || [Gen loss: 0.329, acc: 0.000]\n",
      "Epoch:778 elapsed time: 26.53 [s]\n",
      ">>> Epoch: 779, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.051, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 0.225, acc: 0.000]\n",
      "Epoch:779 elapsed time: 26.53 [s]\n",
      ">>> Epoch: 780, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.049, acc: 0.938] || [F-Dis loss: 0.424, acc: 0.062] || [Gen loss: 0.457, acc: 0.000]\n",
      "Epoch:780 elapsed time: 25.23 [s]\n",
      ">>> Epoch: 781, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.055, acc: 0.938] || [F-Dis loss: 0.433, acc: 0.062] || [Gen loss: 0.279, acc: 0.000]\n",
      "Epoch: 781 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.510\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.032\n",
      "Ploting results\n",
      "Epoch:781 elapsed time: 51.51 [s]\n",
      ">>> Epoch: 782, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.353, acc: 0.354] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.381, acc: 0.000]\n",
      "Epoch:782 elapsed time: 27.80 [s]\n",
      ">>> Epoch: 783, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.434, acc: 0.062] || [Gen loss: 0.317, acc: 0.000]\n",
      "Epoch:783 elapsed time: 27.49 [s]\n",
      ">>> Epoch: 784, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 0.438, acc: 0.000]\n",
      "Epoch:784 elapsed time: 26.15 [s]\n",
      ">>> Epoch: 785, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.481, acc: 0.062] || [Gen loss: 0.488, acc: 0.000]\n",
      "Epoch:785 elapsed time: 27.14 [s]\n",
      ">>> Epoch: 786, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 0.296, acc: 0.000]\n",
      "Epoch:786 elapsed time: 35.14 [s]\n",
      ">>> Epoch: 787, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.476, acc: 0.062] || [Gen loss: 0.326, acc: 0.000]\n",
      "Epoch:787 elapsed time: 42.33 [s]\n",
      ">>> Epoch: 788, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 0.468, acc: 0.000]\n",
      "Epoch:788 elapsed time: 36.74 [s]\n",
      ">>> Epoch: 789, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.386, acc: 0.062] || [Gen loss: 0.250, acc: 0.000]\n",
      "Epoch:789 elapsed time: 35.15 [s]\n",
      ">>> Epoch: 790, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.058, acc: 0.938] || [F-Dis loss: 0.430, acc: 0.062] || [Gen loss: 0.051, acc: 0.000]\n",
      "Epoch:790 elapsed time: 34.57 [s]\n",
      ">>> Epoch: 791, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.460, acc: 0.062] || [Gen loss: 0.325, acc: 0.000]\n",
      "Epoch: 791 Testing model training process...\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.758\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.049\n",
      "Ploting results\n",
      "Epoch:791 elapsed time: 56.89 [s]\n",
      ">>> Epoch: 792, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.518, acc: 0.354] || [F-Dis loss: 0.452, acc: 0.062] || [Gen loss: 0.175, acc: 0.000]\n",
      "Epoch:792 elapsed time: 28.70 [s]\n",
      ">>> Epoch: 793, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 0.279, acc: 0.000]\n",
      "Epoch:793 elapsed time: 25.86 [s]\n",
      ">>> Epoch: 794, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 0.266, acc: 0.000]\n",
      "Epoch:794 elapsed time: 26.57 [s]\n",
      ">>> Epoch: 795, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 0.421, acc: 0.000]\n",
      "Epoch:795 elapsed time: 36.39 [s]\n",
      ">>> Epoch: 796, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.514, acc: 0.062] || [Gen loss: 0.368, acc: 0.000]\n",
      "Epoch:796 elapsed time: 39.88 [s]\n",
      ">>> Epoch: 797, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 0.455, acc: 0.000]\n",
      "Epoch:797 elapsed time: 40.11 [s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-058be2ae8cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# training with the muti conditional gan with images + text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtraining_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_cgen_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_cdis_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_cgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_train_cfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-66271b43091d>\u001b[0m in \u001b[0;36mtraining_model\u001b[1;34m(gen_model, dis_model, gan_model, data, train_cfg)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mdhr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_multi_cgan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;31m# epoch log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-fd5f75d43152>\u001b[0m in \u001b[0;36mtrain_multi_cgan\u001b[1;34m(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# train for real samples batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdhr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdis_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXi_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXl_real\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m# train for fake samples batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdhf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdis_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXi_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXl_fake\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1725\u001b[0m                                                     class_weight)\n\u001b[0;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# traininng with the traditional gan\n",
    "# training_model(gen_model, dis_model, gan_model, gan_data, gan_train_cfg)\n",
    "\n",
    "# training with the conditional gan with images\n",
    "# training_model(cgen_img_model, cdis_img_model, cgan_img_model, gan_data, gan_train_cfg)\n",
    "\n",
    "# training with the muti conditional gan with images + text\n",
    "training_model(multi_cgen_model, multi_cdis_model, multi_cgan_model, gan_data, gan_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train_cfg[\"trained_epochs\"]\n"
   ]
  },
  {
   "source": [
    "# THE END"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}