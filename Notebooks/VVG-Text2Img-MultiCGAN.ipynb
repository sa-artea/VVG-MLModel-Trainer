{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "useful links:\n",
    "\n",
    "- Data Preparation for Variable Length Input Sequences, URL: https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "- Masking and padding with Keras, URL: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "- Step-by-step understanding LSTM Autoencoder layers, URL: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352XX, \n",
    "- Understanding input_shape parameter in LSTM with Keras, URL: https://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\n",
    "- tf.convert_to_tensor, URL: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n",
    "- ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int) in Python, URL: https://datascience.stackexchange.com/questions/82440/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type\n",
    "- How to Identify and Diagnose GAN Failure Modes, URL: https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/\n",
    "- How to Develop a GAN for Generating MNIST Handwritten Digits\n",
    ", URL: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "- How to Visualize a Deep Learning Neural Network Model in Keras\n",
    ", URL: https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- Tips for Training Stable Generative Adversarial Networks\n",
    ", URL: https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
    "- How to Implement GAN Hacks in Keras to Train Stable Models\n",
    ", URL: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/\n",
    "- How to Configure Image Data Augmentation in Keras, URL: https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "import cv2\n",
    "import datetime\n",
    "import copy\n",
    "import gc\n",
    "from statistics import mean\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# natural language processing packages\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading nlkt data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# sample handling sklearn package\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "# # Keras + Tensorflow ML libraries\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers\n",
    "\n",
    "# preprocessing and processing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# shapping layers\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# basic layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# data processing layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "\n",
    "# recurrent and convolutional layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "\n",
    "# activarion function\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# optimization loss functions\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import SGD # OJO!\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam # OJO!\n",
    "from tensorflow.keras.optimizers import Adadelta # OJO!\n",
    "from tensorflow.keras.optimizers import Adagrad # OJO!\n",
    "\n",
    "# image augmentation and processing\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ==============================="
   ]
  },
  {
   "source": [
    "# FUNCTION DEFINITION"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the image from file with cv2\n",
    "def read_img(img_fpn):\n",
    "    ans = cv2.imread(img_fpn, cv2.IMREAD_UNCHANGED)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction to scale the image and reduce cv2\n",
    "def scale_img(img, scale_pct):\n",
    "\n",
    "    width = int(img.shape[1]*scale_pct/100)\n",
    "    height = int(img.shape[0]*scale_pct/100)\n",
    "    dim = (width, height)\n",
    "    # resize image\n",
    "    ans = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img.astype(\"float32\")/float(rangev)\n",
    "    \n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = (img.astype(\"float32\")-rangev)/rangev\n",
    "    # ans = pd.Series(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pad the image in the center\n",
    "def pad_img(img, h, w, img_type):\n",
    "    #  in case when you have odd number\n",
    "    ans = None\n",
    "    top_pad = np.floor((h - img.shape[0]) / 2).astype(np.uint8) # floor\n",
    "    bottom_pad = np.ceil((h - img.shape[0]) / 2).astype(np.uint8)\n",
    "    right_pad = np.ceil((w - img.shape[1]) / 2).astype(np.uint8)\n",
    "    left_pad = np.floor((w - img.shape[1]) / 2).astype(np.uint8) # floor\n",
    "    # print((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "    if img_type == \"rgb\":\n",
    "        ans = np.copy(np.pad(img, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode=\"constant\", constant_values=0.0))   \n",
    "    if img_type == \"bw\":\n",
    "        ans = np.copy(np.pad(img, ((int(top_pad), int(bottom_pad)), (int(left_pad), int(right_pad))), mode=\"constant\", constant_values=0))\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shape(src_df, img_col, shape_col):\n",
    "\n",
    "    ans = src_df\n",
    "    src_col = list(ans[img_col])\n",
    "    tgt_col = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for data in src_col:\n",
    "        tshape = data.shape\n",
    "        tgt_col.append(tshape)\n",
    "\n",
    "    ans[shape_col] = tgt_col\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to padd the images in the dataset, needs the shape, the type of image and the src + tgt columns of the frame to work with\n",
    "def padding_images(src_df, src_col, tgt_col, max_shape, img_type):\n",
    "    # ans = src_df\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "    max_x, max_y = max_shape[0], max_shape[1]\n",
    "\n",
    "    for timg in src_images:        \n",
    "        pimg = pad_img(timg, max_y, max_x, img_type)\n",
    "        tgt_images.append(pimg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the images in in memory\n",
    "def get_images(rootf, src_df, src_col, tgt_col, scale_pct):\n",
    "    ans = src_df\n",
    "    src_files = list(ans[src_col])\n",
    "    tgt_files = list()\n",
    "\n",
    "    # ansdict = {}\n",
    "    for tfile in src_files:\n",
    "        tfpn = os.path.join(rootf, tfile)\n",
    "        timg = read_img(tfpn)\n",
    "        timg = scale_img(timg, scale_pct)\n",
    "        tgt_files.append(timg)\n",
    "\n",
    "    ans[tgt_col] = tgt_files\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize the images in the dataset, it has 2 options\n",
    "def standarize_images(src_df, src_col, tgt_col, img_type, std_opt):\n",
    "    src_images = src_df[src_col]\n",
    "    tgt_images = list()\n",
    "\n",
    "    for timg in src_images:\n",
    "        # pcolor image\n",
    "        if img_type == \"rgb\":\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # b&w image\n",
    "        if img_type == \"rb\":\n",
    "            timg = np.asarray(timg) #, dtype=\"uint8\")\n",
    "            timg = timg[:,:,np.newaxis]\n",
    "            timg = np.asarray(timg, dtype=\"object\")\n",
    "        \n",
    "        # std_opt affect the standarization results\n",
    "        # result 0.0 < std_timg < 1.0\n",
    "        # result -1.0 < std_timg < 1.0\n",
    "        std_timg = std_img(timg, 0, 255, std_opt)\n",
    "        tgt_images.append(std_timg)\n",
    "\n",
    "    src_df[tgt_col] = tgt_images\n",
    "    return src_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function than rotates the original image to create a new example\n",
    "def syth_rgb_img(data):\n",
    "\n",
    "    samples = expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(rotation_range=90)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"uint8\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the max shape in the image dataset\n",
    "def get_mshape(shape_data, imgt):\n",
    "\n",
    "    max_x, max_y, max_ch = 0, 0, 0\n",
    "    shape_data = list(shape_data)\n",
    "    ans = None\n",
    "\n",
    "    if imgt == \"rgb\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty, tch = tshape[0], tshape[1], tshape[2]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            if tch > max_ch:\n",
    "                max_ch = tch\n",
    "            \n",
    "        ans = (max_x, max_y, max_ch)\n",
    "    \n",
    "    elif imgt == \"bw\":\n",
    "\n",
    "        for tshape in shape_data:\n",
    "            # tshape = eval(tshape)\n",
    "            tx, ty = tshape[0], tshape[1]\n",
    "\n",
    "            if tx > max_x:\n",
    "                max_x = tx\n",
    "            if ty > max_y:\n",
    "                max_y = ty\n",
    "            \n",
    "        ans = (max_x, max_y)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "\n",
    "taken from https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "'''\n",
    "def temporalize(data, lookback):\n",
    "    output_X = list()\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        temp = list()\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            temp.append(data[[(i+j+1)], :])\n",
    "        temp = np.array(temp, dtype=\"object\")\n",
    "        output_X.append(temp)\n",
    "    output_X = np.array(output_X, dtype=\"object\")\n",
    "    return output_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the pandas df data into usable word dense vector representation, YOU NEED IT FOR THE CSV to be useful!\n",
    "def format_dvector(work_corpus):\n",
    "\n",
    "    ans = list()\n",
    "    for dvector in work_corpus:\n",
    "        dvector = eval(dvector)\n",
    "        dvector = np.asarray(dvector)\n",
    "        ans.append(dvector)\n",
    "    ans = np.asarray(ans, dtype=\"object\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct to concatenate all label columns into one for a single y in ML training, returns a list\n",
    "def concat_labels(row, cname):\n",
    "\n",
    "    ans = list()\n",
    "    for c in cname:\n",
    "        r = row[c]\n",
    "        r = eval(r)\n",
    "        ans = ans + r\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the ML model\n",
    "def save_model(model, m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    # print(fpn)\n",
    "    model.save(fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the ML model\n",
    "def load_model(m_path, m_file):\n",
    "\n",
    "    fpn = os.path.join(m_path, m_file)\n",
    "    fpn = fpn + \".h5\"\n",
    "    model = keras.models.load_model(fpn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cast dataframe and avoid problems with keras\n",
    "def cast_batch(data):\n",
    "\n",
    "    cast_data = list()\n",
    "\n",
    "    if len(data) >= 2:\n",
    "\n",
    "        for d in data:\n",
    "            d = np.asarray(d).astype(\"float32\")\n",
    "            cast_data.append(d)\n",
    "\n",
    "    return cast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select real elements to train the discriminator\n",
    "def gen_real_samples(data, sample_size, half_batch):\n",
    "\n",
    "    real_data = list()\n",
    "    rand_index = np.random.randint(0, sample_size, size=half_batch)\n",
    "\n",
    "    # need at leas X, y\n",
    "    # posible combinations are:\n",
    "    # X_img/X_txt, y\n",
    "    # X_img/X_txt, X_labels, y\n",
    "    # X_img, X_txt, X_labels, y\n",
    "    if len(data) >= 2:\n",
    "        # selectinc the columns in the dataset\n",
    "        for d in data:\n",
    "            # td_real = d[rand_index]\n",
    "            td_real = copy.deepcopy(d[rand_index])\n",
    "            real_data.append(td_real)\n",
    "\n",
    "    # casting data\n",
    "    real_data = cast_batch(real_data)\n",
    "\n",
    "    return real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create fake elements to train the discriminator\n",
    "def gen_fake_samples(gen_model, dataset_shape, half_batch):\n",
    "\n",
    "    # fake data\n",
    "    fake_data = None\n",
    "    # conditional labels for the gan model\n",
    "    conditional = dataset_shape.get(\"conditioned\")\n",
    "    # configuratin keys for the generator\n",
    "    latent_shape = dataset_shape.get(\"latent_shape\")\n",
    "    txt_shape = dataset_shape.get(\"txt_shape\")\n",
    "    cat_shape = dataset_shape.get(\"cat_shape\")\n",
    "    label_shape = dataset_shape.get(\"label_shape\")\n",
    "    data_cols = dataset_shape.get(\"data_cols\")\n",
    "\n",
    "    # generator config according to the dataset\n",
    "    # X:images -> y:Real/Fake\n",
    "    if data_cols == 2:\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # random generated image from the model\n",
    "        Xi_fake = gen_model.predict(latent_text)\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, y_fake)\n",
    "\n",
    "    # X_img, X_labels(classification), y (fake/real)\n",
    "    elif (conditional == True) and data_cols == 3:\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_fake = gen_fake_labels(label_shape, half_batch)\n",
    "\n",
    "        # random generated image from the model\n",
    "        Xi_fake = gen_model.predict([latent_text, Xl_fake])\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, Xl_fake, y_fake)\n",
    "\n",
    "    elif (conditional == False) and data_cols == 3:\n",
    "        \n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # random generated image + text from the model\n",
    "        Xi_fake, Xt_fake = gen_model.predict(latent_text)\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, Xt_fake, y_fake)\n",
    "\n",
    "    # X_img(rgb), X_txt(text), X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 4:\n",
    "\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, half_batch)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_fake = gen_fake_negclass(cat_shape, half_batch)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_fake = gen_fake_labels(label_shape, half_batch)\n",
    "\n",
    "        # random generated image from the model\n",
    "        Xi_fake, Xt_fake = gen_model.predict([latent_text, Xl_fake])\n",
    "        # fake samples\n",
    "        fake_data = (Xi_fake, Xt_fake, Xl_fake, y_fake)\n",
    "\n",
    "    # casting data type\n",
    "    fake_data = cast_batch(fake_data)\n",
    "    \n",
    "    return fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create inputs to updalte the GAN generator\n",
    "def gen_latent_data(dataset_shape, batch_size):\n",
    "\n",
    "    # latent data\n",
    "    latent_data = None\n",
    "\n",
    "    # conditional labels for the gan model\n",
    "    conditional = dataset_shape.get(\"conditioned\")\n",
    "    # configuratin keys for the generator\n",
    "    latent_shape = dataset_shape.get(\"latent_shape\")\n",
    "    txt_shape = dataset_shape.get(\"txt_shape\")\n",
    "    cat_shape = dataset_shape.get(\"cat_shape\")\n",
    "    label_shape = dataset_shape.get(\"label_shape\")\n",
    "    data_cols = dataset_shape.get(\"data_cols\")\n",
    "\n",
    "    # generator config according to the dataset\n",
    "    # X:images -> y:Real/Fake\n",
    "    if data_cols == 2:\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # fake samples\n",
    "        latent_data = (latent_text, y_gen)\n",
    "\n",
    "    # X_img, X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 3 and (conditional == True):\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_gen = gen_fake_labels(label_shape, batch_size)\n",
    "        # gen samples\n",
    "        latent_data = (latent_text, Xl_gen, y_gen)\n",
    "\n",
    "    elif data_cols == 3 and (conditional == False):\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # fake samples\n",
    "        latent_data = (latent_text, y_gen)\n",
    "\n",
    "    # X_img(rgb), X_txt(text), X_labels(classification), y (fake/real)\n",
    "    elif data_cols == 4:\n",
    "        # random textual latent space \n",
    "        latent_text = gen_latent_txt(latent_shape, batch_size)\n",
    "        # marking the images as fake in all accounts\n",
    "        y_gen = gen_fake_posclass(cat_shape, batch_size)\n",
    "        # marking all the images with fake labels\n",
    "        Xl_gen = gen_fake_labels(label_shape, batch_size)\n",
    "        # gen samples\n",
    "        latent_data = (latent_text, Xl_gen, y_gen)\n",
    "\n",
    "    return latent_data\n",
    "# latent_gen = gen_latent_txt(latent_shape, batch_size)\n",
    "# create inverted category for the fake noisy text\n",
    "# y_gen = get_fake_positive(cat_shape[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate random/latent text for the GAN generator\n",
    "def gen_latent_txt(latent_shape, n_samples):\n",
    "\n",
    "    ans = None\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        noise = np.random.normal(0.0, 1.0, size=latent_shape)\n",
    "        if ans is None:\n",
    "            txt = np.expand_dims(noise, axis=0)\n",
    "            ans = txt\n",
    "        else:\n",
    "            img = np.expand_dims(txt, axis=0)\n",
    "            ans = np.concatenate((ans, txt), axis=0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfunction to smooth the fake positives\n",
    "def smooth_positives(y):\n",
    "\treturn y - 0.3 + (np.random.random(y.shape)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to smooth the fake negatives\n",
    "def smooth_negatives(y):\n",
    "\treturn y + np.random.random(y.shape)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to smooth the data labels\n",
    "def smooth_labels(y):\n",
    "    # label smoothing formula\n",
    "    # alpha: 0.0 -> original distribution, 1.0 uniform distribution\n",
    "    # K: number of label classes\n",
    "    # y_ls = (1 - alpha) * y_hot + alpha / K\n",
    "    alpha = 0.3\n",
    "    K = y[0].shape[0]\n",
    "    ans = (1-alpha)*y + alpha/K\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake true categories for the generator\n",
    "def gen_fake_posclass(cat_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_shape[0])\n",
    "    ans = np.ones(sz)\n",
    "    # smoothing fakes\n",
    "    ans = smooth_positives(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake negative category to train the GAN\n",
    "def gen_fake_negclass(cat_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, cat_shape[0])\n",
    "    ans = np.zeros(sz)\n",
    "    ans = smooth_negatives(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate fake labels to train the GAN\n",
    "def gen_fake_labels(label_shape, batch_size):\n",
    "\n",
    "    sz = (batch_size, label_shape[0])\n",
    "    ans = np.random.randint(0,1, size=sz)\n",
    "    ans = smooth_labels(ans)\n",
    "    ans = ans.astype(\"float32\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create text similar to the original one with 5% of noise\n",
    "def syth_text(data, nptc=0.05):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetizing a noisy std image from real data\n",
    "def syth_std_img(data):\n",
    "\n",
    "    samples = np.expand_dims(data, 0)\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=10)\n",
    "    # datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True, vertical_flip=True)\n",
    "    ans = datagen.flow(samples, batch_size=1)\n",
    "    ans = ans[0].astype(\"float32\")\n",
    "    ans = np.squeeze(ans, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create new categories with some noise, default 5%\n",
    "def syth_categories(data, nptc=0.05):\n",
    "\n",
    "    ans = None\n",
    "    noise = np.random.normal(0, nptc, data.shape)\n",
    "    ans = data + noise\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to artificially span a batch with some noise and alterations by an specific number\n",
    "def expand_samples(data, synth_batch):\n",
    "\n",
    "    X_txt = data[0]\n",
    "    X_img = data[1]\n",
    "    y = data[2]\n",
    "    labels = data[3]\n",
    "\n",
    "    # creating the exapnded batch response\n",
    "    Xe_txt, Xe_img, ye, lbe = None, None, None, None\n",
    "\n",
    "    # iterating in the original batch\n",
    "    for Xtt, Xit, yt, lb in zip(X_txt, X_img, y, labels):\n",
    "\n",
    "        # temporal synth minibatch per original image\n",
    "        synth_Xt, synth_Xi, synth_y, synth_lb = None, None, None, None\n",
    "\n",
    "        # synthetizing artificial data for the batch\n",
    "        for i in range(synth_batch):\n",
    "\n",
    "            # generating first element\n",
    "            if (synth_Xt is None) and (synth_Xi is None) and (synth_y is None) and (synth_lb is None):\n",
    "                # gen text\n",
    "                gen_Xt = copy.deepcopy(Xtt)\n",
    "                gen_Xt = np.array(gen_Xt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = gen_Xt\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = gen_Xi\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = gen_yt\n",
    "\n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = gen_lb\n",
    "\n",
    "            # generatin the rest of the elements\n",
    "            else:\n",
    "                # gen text\n",
    "                gen_Xt = syth_text(Xtt)\n",
    "                gen_Xt = np.expand_dims(gen_Xt, axis=0)\n",
    "                synth_Xt = np.concatenate((synth_Xt, gen_Xt), axis=0)\n",
    "\n",
    "                # gen images\n",
    "                gen_Xi = syth_std_img(Xit)\n",
    "                gen_Xi = np.expand_dims(gen_Xi, axis=0)\n",
    "                synth_Xi = np.concatenate((synth_Xi, gen_Xi), axis=0)\n",
    "\n",
    "                # gen category\n",
    "                gen_yt = syth_categories(yt)\n",
    "                gen_yt = np.expand_dims(gen_yt, axis=0)\n",
    "                synth_y = np.concatenate((synth_y, gen_yt), axis=0)\n",
    "        \n",
    "                # gen labels\n",
    "                gen_lb = syth_categories(lb)\n",
    "                gen_lb = np.expand_dims(gen_lb, axis=0)\n",
    "                synth_lb = np.concatenate((synth_lb, gen_lb), axis=0)\n",
    "\n",
    "        # adding the first part to the training batch\n",
    "        if (Xe_txt is None) and (Xe_img is None) and (ye is None) and (lbe is None):\n",
    "            # adding text\n",
    "            Xe_txt = synth_Xt\n",
    "            # adding images\n",
    "            Xe_img = synth_Xi\n",
    "            # adding categories\n",
    "            ye = synth_y\n",
    "            # adding labels\n",
    "            lbe = synth_lb\n",
    "\n",
    "        # adding the rest of the batch\n",
    "        else:\n",
    "            # adding text\n",
    "            Xe_txt = np.concatenate((Xe_txt, synth_Xt), axis=0)\n",
    "            # adding images\n",
    "            Xe_img = np.concatenate((Xe_img, synth_Xi), axis=0)\n",
    "            # adding category\n",
    "            ye = np.concatenate((ye, synth_y), axis=0)\n",
    "            # adding labels\n",
    "            lbe = np.concatenate((lbe, synth_lb), axis=0)\n",
    "\n",
    "    Xe_txt, Xe_img, ye, lbe = cast_batch(Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    e_data = (Xe_txt, Xe_img, ye, lbe)\n",
    "\n",
    "    return e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drift_labels(Xt_real, Xi_real, y_real, Xt_fake, Xi_fake, y_fake, batch_size, drift_pct):\n",
    "def drift_labels(real_data, fake_data, batch_size, drift_pct):\n",
    "\n",
    "    # setting the size for the drift labels\n",
    "    drift_size = int(math.ceil(drift_pct*batch_size))\n",
    "    # random index for drift elements!!!\n",
    "    rand_drifters = np.random.choice(batch_size, size=drift_size, replace=False)\n",
    "    # print(\"batch size\", batch_size, \"\\nrandom choise to change\", drift_size, \"\\n\", rand_drifters)\n",
    "\n",
    "    # if the dataset has at leas X, y... NEED TO PASS A GOOD ORDER\n",
    "    if (len(real_data) and len(fake_data)) >= 2:\n",
    "\n",
    "        # iterating over the random choose index\n",
    "        for drift in rand_drifters:\n",
    "\n",
    "            # taking one real + fake column at a time\n",
    "            # X_img/txt, y\n",
    "            # X_img/txt, X_labels, y\n",
    "            # X_img, X_txt, X_labels, y\n",
    "            for real_col, fake_col in zip(real_data, fake_data):\n",
    "\n",
    "                # copying real data in temporal var\n",
    "                temp_drift = copy.deepcopy(real_col[drift])\n",
    "                # replacing real with fakes\n",
    "                real_col[drift] = copy.deepcopy(fake_col[drift])\n",
    "                # updating fakes with temporal original\n",
    "                fake_col[drift] = temp_drift\n",
    "\n",
    "    return real_data, fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standarize image, has 2 types, from 0 to 1 and from -1 to 1\n",
    "def inv_std_img(img, minv, maxv, stype=\"std\"):\n",
    "    ans = None\n",
    "    rangev = maxv - minv\n",
    "\n",
    "    if stype == \"std\":\n",
    "        ans = img*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    elif stype == \"ctr\":\n",
    "        rangev = float(rangev/2)\n",
    "        ans = img+rangev\n",
    "        ans = ans*rangev\n",
    "        ans = np.asarray(ans).astype(\"uint8\")\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the generated images within a training epoch\n",
    "def plot_gen_images(examples, epoch, report_fp_name, n_sample):\n",
    "\n",
    "    # get important data for iterating\n",
    "    example_size = examples.shape[0]\n",
    "    og_shape = examples[0].shape\n",
    "    rand_img = np.random.choice(example_size, size=n_sample*n_sample, replace=False) \n",
    "    # (0, example_size, size=n_sample*n_sample)\n",
    "\n",
    "    # prep the figure\n",
    "    fig, ax = plt.subplots(n_sample,n_sample, figsize=(20,20))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # plot images\n",
    "    for i in range(n_sample*n_sample):\n",
    "        # define subplot\n",
    "        plt.subplot(n_sample, n_sample, 1+i)\n",
    "\n",
    "        # getting the images from sample\n",
    "        rand_i = rand_img[i]\n",
    "        gimg = examples[rand_i]\n",
    "        gimg = inv_std_img(gimg, 0, 255, \"ctr\")\n",
    "\n",
    "        # turn off axis\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(gimg) #, interpolation=\"nearest\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"GENERATED IMAGES\", fontsize=50)\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-Gen-img-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_metrics(disr_hist, disf_hist, gan_hist, report_fp_name, epoch):\n",
    "\n",
    "    # reporting results\n",
    "    disr_hist = np.array(disr_hist)\n",
    "    disf_hist = np.array(disf_hist)\n",
    "    gan_hist = np.array(gan_hist)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\n",
    "    fig.patch.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "    # loss\n",
    "    ax1.plot(disr_hist[:,1], \"royalblue\", label=\"Loss: R-Dis\")\n",
    "    ax1.plot(disf_hist[:,1], \"crimson\", label=\"Loss: F-Dis\")\n",
    "    ax1.plot(gan_hist[:,1], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "    # ax1.plot(gan_hist[:], \"blueviolet\", label=\"Loss: GAN/Gen\")\n",
    "\n",
    "    # acc_\n",
    "    ax2.plot(disr_hist[:,0], \"royalblue\", label=\"Acc: R-Dis\")\n",
    "    ax2.plot(disf_hist[:,0], \"crimson\", label=\"Acc: F-Dis\")\n",
    "    ax2.plot(gan_hist[:,0], \"blueviolet\", label=\"Acc: GAN/Gem\")\n",
    "\n",
    "    # plot leyend\n",
    "    fig.suptitle(\"LEARNING BEHAVIOR\", fontsize=20)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax1.set(xlabel = \"Epoch [cycle]\", ylabel = \"Loss\")\n",
    "    ax2.set(xlabel = \"Epoch [cycle]\", ylabel = \"Acc\")\n",
    "    fig.legend()\n",
    "\n",
    "    # save plot to file\n",
    "    plot_name = \"GAN-learn-curve-epoch%03d\" % int(epoch)\n",
    "    plot_name = plot_name + \".png\"\n",
    "    fpn = os.path.join(report_fp_name, plot_name)\n",
    "    plt.savefig(fpn)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the loss and accuracy avg in multiple batchs of an epoch\n",
    "def epoch_avg(log):\n",
    "    loss, acc = None, None\n",
    "\n",
    "    # if acc and loss are present to avg\n",
    "    if type(log[0]) is list:\n",
    "        if len(log) > 0:\n",
    "\n",
    "            acc_list = list()\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                ta = l[0]\n",
    "                tl = l[1]\n",
    "\n",
    "                acc_list.append(ta)\n",
    "                loss_list.append(tl)\n",
    "\n",
    "            loss, acc = mean(loss_list), mean(acc_list)\n",
    "        return loss, acc\n",
    "    \n",
    "    else:\n",
    "        # if only loss is present\n",
    "        if len(log) > 0:\n",
    "\n",
    "            loss_list = list()\n",
    "\n",
    "            for l in log:\n",
    "                loss_list.append(l)\n",
    "\n",
    "            loss = mean(loss_list)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model, needs the dirpath, the name and the datetime to save\n",
    "def export_model(model, models_fp_name, filename, datetime):\n",
    "\n",
    "    ss = True\n",
    "    sln = True\n",
    "    fext = \"png\"\n",
    "    fpn = filename + \"-\" + datetime\n",
    "    fpn = filename + \".\" + fext\n",
    "    fpn = os.path.join(models_fp_name, fpn)\n",
    "    plot_model(model, to_file=fpn, show_shapes=ss, show_layer_names=sln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format data to save in file\n",
    "def format_metrics(disr_history, disf_history, gan_history):\n",
    "\n",
    "    headers, data = None, None\n",
    "\n",
    "    disr_hist = np.array(disr_history)\n",
    "    disf_hist = np.array(disf_history)\n",
    "    gan_hist = np.array(gan_history)\n",
    "\n",
    "    # formating file headers\n",
    "    headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\", \"gen_gan_acc\"]\n",
    "    # headers = [\"dis_loss_real\", \"dis_acc_real\", \"dis_loss_fake\", \"dis_acc_fake\", \"gen_gan_loss\",] # \"gen_gan_acc\"]\n",
    "\n",
    "    # formating fake discriminator train data\n",
    "    drhl = disr_hist[:,1]\n",
    "    drha = disr_hist[:,0]\n",
    "\n",
    "    # formating real discrimintator train data\n",
    "    dfhl = disf_hist[:,1]\n",
    "    dfha = disf_hist[:,0]\n",
    "\n",
    "    # formating gan/gen train data\n",
    "    # gghl = gan_hist[:]# .flatten()\n",
    "    gghl = gan_hist[:,1]\n",
    "    ggha = gan_hist[:,0]\n",
    "\n",
    "    # adding all formatted data into list\n",
    "    data = np.column_stack((drhl, drha, dfhl, dfha, gghl, ggha))\n",
    "    # data = np.column_stack((drhl, drha, dfhl, dfha, gghl)) #, ggha))\n",
    "\n",
    "    return data, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write data in csv file\n",
    "def write_metrics(data, headers, report_fp_name, filename):\n",
    "\n",
    "    # print(report_fp_name, filename)\n",
    "    fpn = filename + \"-train-history.csv\"\n",
    "    fpn = os.path.join(report_fp_name, fpn)\n",
    "\n",
    "    history_df = pd.DataFrame(data, columns=headers)\n",
    "    tdata = history_df.to_csv(\n",
    "                            fpn,\n",
    "                            sep=\",\",\n",
    "                            index=False,\n",
    "                            encoding=\"utf-8\",\n",
    "                            mode=\"w\",\n",
    "                            quoting=csv.QUOTE_ALL\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to safe the loss/acc logs in training for the gan/gen/dis models\n",
    "def save_metrics(disr_history, disf_history, gan_history, report_fp_name, filename):\n",
    "\n",
    "    data, headers = format_metrics(disr_history, disf_history, gan_history)\n",
    "    write_metrics(data, headers, report_fp_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to know the time between epochs or batchs it return the new time for a new calculation\n",
    "def lapse_time(last_time, epoch):\n",
    "\n",
    "    now_time = datetime.datetime.now()\n",
    "    deltatime = now_time - last_time\n",
    "    deltatime = deltatime.total_seconds()\n",
    "    deltatime = \"%.2f\" % deltatime\n",
    "    msg = \"Epoch:%3d \" % int(epoch+1)\n",
    "    msg = msg + \"elapsed time: \" + str(deltatime) + \" [s]\"\n",
    "    print(msg)\n",
    "    return now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# def train(gen_model, dis_model, gan_model, X_img, X_txt, y, labels, epochs, batch_size, save_intervas, fn_config):\n",
    "def train_gan(gen_model, dis_model, gan_model, data, train_cfg): # epochs, batch_size, save_intervas, fn_config\n",
    "\"\"\"\n",
    "# function to test the model while training\n",
    "def test_model(gen_model, dis_model, data, data_shape, test_cfg): #batch_size, synth_batch, report_fn_path): #train_cfg)\n",
    "\n",
    "    dataset_size = test_cfg.get(\"dataset_size\")\n",
    "    batch_size = test_cfg.get(\"batch_size\")\n",
    "    synth_batch = test_cfg.get(\"synth_batch\")\n",
    "    epoch = int(test_cfg.get(\"current_epoch\"))\n",
    "    report_fn_path = test_cfg.get(\"report_fn_path\")\n",
    "    gen_samples = test_cfg.get(\"gen_sample_size\") \n",
    "\n",
    "    # select real txt2img for discrimintator\n",
    "    real_data = gen_real_samples(data, dataset_size, batch_size)\n",
    "    # create false txt for txt2img for generator\n",
    "    fake_data = gen_fake_samples(gen_model, data_shape, batch_size)\n",
    "\n",
    "    # expand the training sample for the discriminator\n",
    "    if synth_batch > 1:\n",
    "        real_data = expand_samples(real_data, synth_batch)\n",
    "        fake_data = expand_samples(fake_data, synth_batch)\n",
    "\n",
    "    # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "    # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "\n",
    "    # gen data\n",
    "    X_test = fake_data[0]\n",
    "    split_batch = int(batch_size/2)\n",
    "\n",
    "    # plotting gen images\n",
    "    # plot_gen_ideas(fake_data, epoch, report_fn_path, gen_samples)\n",
    "    plot_gen_images(X_test, epoch, report_fn_path, gen_samples)\n",
    "\n",
    "    test_real, test_fake = None, None\n",
    "\n",
    "    if len(data) == 2:\n",
    "        test_real, test_fake = test_gan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    elif len(data) == 3 and data_shape.get(\"conditioned\") == True:\n",
    "        test_real, test_fake = test_cgan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    elif len(data) == 3 and data_shape.get(\"conditioned\") == False:\n",
    "        test_real, test_fake = test_multi_gan(dis_model, real_data, fake_data, batch_size)\n",
    "\n",
    "    elif len(data) == 4:\n",
    "        test_real, test_fake = test_multi_cgan(dis_model, real_data, fake_data, data_shape, test_cfg)\n",
    "\n",
    "    # summarize discriminator performance\n",
    "    print(\"Batch Size %d -> Samples: Fake: %d & Real: %d\" % (batch_size*synth_batch, split_batch, split_batch))\n",
    "    print(\">>> Test Fake -> Acc: %.3f || Loss: %.3f\" % (test_fake[1], test_fake[0]))\n",
    "    print(\">>> Test Real -> Acc: %.3f || Loss: %.3f\" % (test_real[1], test_real[0]))\n",
    "    # print(\">>> Test Gen -> Acc: %.3f || Loss: %.3f\" % (test_cgen[1], test_cgen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special function to train the GAN\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "# def train(gen_model, dis_model, gan_model, X_img, X_txt, y, labels, epochs, batch_size, save_intervas, fn_config):\n",
    "def training_gan(gen_model, dis_model, gan_model, data, train_cfg): # epochs, batch_size, save_intervas, fn_config\n",
    "\n",
    "    # sample size\n",
    "    dataset_size = train_cfg.get(\"dataset_size\")\n",
    "\n",
    "    # data shape for the generator\n",
    "    data_shape = {\n",
    "        \"latent_shape\": train_cfg.get(\"latent_shape\"),\n",
    "        \"cat_shape\": train_cfg.get(\"cat_shape\"),\n",
    "        \"txt_shape\": train_cfg.get(\"txt_shape\"),\n",
    "        \"label_shape\": train_cfg.get(\"label_shape\"),\n",
    "        \"conditioned\": train_cfg.get(\"conditioned\"),\n",
    "        \"data_cols\": train_cfg.get(\"data_cols\"),\n",
    "        }\n",
    "\n",
    "    # augmentation factor\n",
    "    synth_batch = train_cfg.get(\"synth_batch\")\n",
    "    n = train_cfg.get(\"gen_sample_size\")\n",
    "\n",
    "    epochs = train_cfg.get(\"epochs\")\n",
    "    batch_size = train_cfg.get(\"batch_size\")\n",
    "    half_batch = int(batch_size/2)\n",
    "    batch_per_epoch = int(dataset_size/batch_size)\n",
    "    # fake/real batch division\n",
    "    real_batch = int((batch_size*synth_batch)/2)\n",
    "\n",
    "    # train config\n",
    "    model_fn_path = train_cfg.get(\"models_fn_path\")\n",
    "    report_fn_path = train_cfg.get(\"report_fn_path\")\n",
    "    dis_model_name = train_cfg.get(\"dis_model_name\")\n",
    "    gen_model_name = train_cfg.get(\"gen_model_name\")\n",
    "    gan_model_name = train_cfg.get(\"gan_model_name\")\n",
    "    check_intervas = train_cfg.get(\"check_epochs\")\n",
    "    save_intervas = train_cfg.get(\"save_epochs\")\n",
    "    max_models = train_cfg.get(\"max_models\")\n",
    "    pretrain = train_cfg.get(\"pretrained\")\n",
    "\n",
    "\t# prepare lists for storing stats each epoch\n",
    "    disf_hist, disr_hist, gan_hist = list(), list(), list()\n",
    "    train_time = None\n",
    "\n",
    "    # train dict config\n",
    "    test_cfg = {\n",
    "        \"report_fn_path\": report_fn_path,\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"synth_batch\": synth_batch,\n",
    "        \"gen_sample_size\": train_cfg.get(\"gen_sample_size\"),\n",
    "        \"last_epoch\": None,\n",
    "    }\n",
    "\n",
    "    # iterating in training epochs:\n",
    "    for ep in range(epochs+1):\n",
    "        # epoch logs\n",
    "        ep_disf_hist, ep_disr_hist, ep_gan_hist = list(), list(), list()\n",
    "        train_time = datetime.datetime.now()\n",
    "\n",
    "        # iterating over training batchs\n",
    "        for batch in range(batch_per_epoch):\n",
    "\n",
    "            # select real txt2img for discrimintator\n",
    "            real_data = gen_real_samples(data, dataset_size, half_batch)\n",
    "            # create false txt for txt2img for generator\n",
    "            fake_data = gen_fake_samples(gen_model, data_shape, half_batch)\n",
    "\n",
    "            # expand the training sample for the discriminator\n",
    "            if synth_batch > 1:\n",
    "                real_data = expand_samples(real_data, synth_batch)\n",
    "                fake_data = expand_samples(fake_data, synth_batch)\n",
    "\n",
    "            # print(Xt_real.shape, Xi_real.shape, y_real.shape, yl_real.shape)\n",
    "            # print(Xt_fake.shape, Xi_fake.shape, y_fake.shape, yl_fake.shape)\n",
    "            # print(real_data[0].shape, fake_data[0].shape)\n",
    "            # print(real_data[1].shape, fake_data[1].shape)\n",
    "            # drift labels to confuse the model\n",
    "            real_data, fake_data = drift_labels(real_data, fake_data, half_batch, 0.05)\n",
    "\n",
    "            dhf, dhr, gh = None, None, None\n",
    "\n",
    "            if len(data) == 2:\n",
    "                dhf, dhr, gh = train_gan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 3 and data_shape.get(\"conditioned\") == True:\n",
    "                dhf, dhr, gh = train_cgan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 3 and data_shape.get(\"conditioned\") == False:\n",
    "                dhf, dhr, gh = train_multi_gan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            elif len(data) == 4:\n",
    "                dhf, dhr, gh = train_multi_cgan(dis_model, gan_model, real_data, fake_data, batch_size, data_shape)\n",
    "\n",
    "            # epoch log\n",
    "            ep_disr_hist.append(dhf)\n",
    "            ep_disf_hist.append(dhr)\n",
    "            ep_gan_hist.append(gh)\n",
    "\n",
    "\t\t\t# print('>%d, %d/%d, dis_=%.3f, gen=%.3f' % (ep+1, batch+1, bat_per_epo, dis_history, gen_history))\n",
    "            log_msg = \">>> Epoch: %d, B/Ep: %d/%d, Batch S: %d\" % (ep+1, batch+1, batch_per_epoch, batch_size*synth_batch)\n",
    "            log_msg = \"%s -> [R-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhr[0], dhr[1])\n",
    "            log_msg = \"%s || [F-Dis loss: %.3f, acc: %.3f]\" % (log_msg, dhf[0], dhf[1])\n",
    "            log_msg = \"%s || [Gen loss: %.3f, acc: %.3f]\" % (log_msg, gh[0], gh[1])\n",
    "            print(log_msg)\n",
    "\n",
    "        # record history for epoch\n",
    "        disr_hist.append(epoch_avg(ep_disr_hist))\n",
    "        disf_hist.append(epoch_avg(ep_disf_hist))\n",
    "        gan_hist.append(epoch_avg(ep_gan_hist))\n",
    "\n",
    "\t\t# evaluate the model performance sometimes\n",
    "        if (ep) % check_intervas == 0:\n",
    "            print(\"Epoch:\", ep+1, \"Saving the training progress...\")\n",
    "            test_cfg[\"current_epoch\"] = ep\n",
    "            test_model(gen_model, dis_model, data, data_shape, test_cfg) #, synth_batch)\n",
    "            plot_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, ep)\n",
    "            save_metrics(disr_hist, disf_hist, gan_hist, report_fn_path, gan_model_name)\n",
    "\n",
    "\t\t# saving the model sometimes\n",
    "        if (ep) % save_intervas == 0:\n",
    "            epoch_sufix = \"-epoch%d\" % int(ep)\n",
    "            # epoch_sufix = \"-last\"\n",
    "            epoch_sufix = str(epoch_sufix)\n",
    "            dis_mn = dis_model_name + epoch_sufix\n",
    "            gen_mn = gen_model_name + epoch_sufix\n",
    "            gan_mn = gan_model_name + epoch_sufix\n",
    "\n",
    "            dis_path = os.path.join(model_fn_path, \"Dis\")\n",
    "            gen_path = os.path.join(model_fn_path, \"Gen\")\n",
    "            gan_path = os.path.join(model_fn_path, \"GAN\")\n",
    "\n",
    "            save_model(dis_model, dis_path, dis_mn)\n",
    "            save_model(gen_model, gen_path, gen_mn)\n",
    "            save_model(gan_model, gan_path, gan_mn)\n",
    "        \n",
    "        train_time = lapse_time(train_time, ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape):\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    y_real = real_data[1]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    y_fake = fake_data[1]\n",
    "\n",
    "    # train for real samples batch\n",
    "    dhr = dis_model.train_on_batch(Xi_real, y_real)\n",
    "    # train for fake samples batch\n",
    "    dhf = dis_model.train_on_batch(Xi_fake, y_fake)\n",
    "\n",
    "    # prepare text and inverted categories from the latent space as input for the generator\n",
    "    latent_gen, y_gen = gen_latent_data(dataset_shape, batch_size)\n",
    "\n",
    "    # update the generator via the discriminator's error\n",
    "    gh = gan_model.train_on_batch(latent_gen, y_gen)\n",
    "\n",
    "    return dhf, dhr, gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gan(dis_model, real_data, fake_data, batch_size):\n",
    "    \n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, batch_size, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    y_real = real_data[1]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    y_fake = fake_data[1]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate(Xi_real, y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate(Xi_fake, y_fake, verbose=0)\n",
    "\n",
    "    return test_real, test_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cgan(dis_model, real_data, fake_data, batch_size):\n",
    "    \n",
    "    # drift labels to confuse the model\n",
    "    real_data, fake_data = drift_labels(real_data, fake_data, batch_size, 0.05)\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    Xl_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    xL_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "\n",
    "    # evaluate model\n",
    "    test_real = dis_model.evaluate([Xi_real, Xl_real], y_real, verbose=0)\n",
    "    test_fake = dis_model.evaluate([Xi_fake, xL_fake], y_fake, verbose=0)\n",
    "\n",
    "    return test_real, test_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan(dis_model, gan_model, real_data, fake_data, batch_size, dataset_shape):\n",
    "\n",
    "    # real data asignation\n",
    "    Xi_real = real_data[0]\n",
    "    yl_real = real_data[1]\n",
    "    y_real = real_data[2]\n",
    "\n",
    "    # fake data asignation\n",
    "    Xi_fake = fake_data[0]\n",
    "    yl_fake = fake_data[1]\n",
    "    y_fake = fake_data[2]\n",
    "\n",
    "    # train for real samples batch\n",
    "    dhr = dis_model.train_on_batch([Xi_real, yl_real], y_real)\n",
    "    # train for fake samples batch\n",
    "    dhf = dis_model.train_on_batch([Xi_fake, yl_fake], y_fake)\n",
    "\n",
    "    # prepare text and inverted categories from the latent space as input for the generator\n",
    "    latent_gen, yl_gen, y_gen = gen_latent_data(dataset_shape, batch_size)\n",
    "\n",
    "    # update the generator via the discriminator's error\n",
    "    gh = gan_model.train_on_batch([latent_gen, yl_gen], y_gen)\n",
    "\n",
    "    return dhf, dhr, gh"
   ]
  },
  {
   "source": [
    "# EXEC SCRIPT\n",
    "\n",
    "## Dataset prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== working files ===\n\n std-VVG-Gallery-Text-Data-Small.csv \n std-VVG-Gallery-Img-Data-Small.csv \n Small \n rgb \n Validation-GAN-Text-Data-Small.csv\n"
     ]
    }
   ],
   "source": [
    "# variable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with predictions txt data\n",
    "imagef = \"Img\"\n",
    "\n",
    "# report subfolder\n",
    "reportf = \"Reports\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "trainf = \"Train\"\n",
    "testf = \"Test\"\n",
    "\n",
    "# subfolder for model IO\n",
    "modelf = \"Models\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "\n",
    "imgf = \"jpg\"\n",
    "\n",
    "rgb_sufix = \"rgb\"\n",
    "bw_sufix = \"bw\"\n",
    "\n",
    "# standard sufix\n",
    "stdprefix = \"std-\"\n",
    "\n",
    "# ml model useful data\n",
    "mltprefix = \"ml-\"\n",
    "\n",
    "# report names\n",
    "# timestamp = datetime.date.today().strftime(\"%d-%b-%Y\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "sample_sufix = \"Small\"\n",
    "# sample_sufix = \"Large\"\n",
    "# sample_sufix = \"Paintings\"\n",
    "imgf_sufix = \"Img-Data-\"\n",
    "text_sufix = \"Text-Data-\"\n",
    "\n",
    "# std-VVG-Gallery-Text-Data-Paintings\n",
    "gallery_prefix = \"VVG-Gallery-\"\n",
    "\n",
    "# dataframe file name\n",
    "text_fn = stdprefix + gallery_prefix + text_sufix + sample_sufix + \".\" + fext\n",
    "imgf_fn = stdprefix + gallery_prefix + imgf_sufix + sample_sufix + \".\" + fext\n",
    "valt_fn = \"Validation-GAN-\" + text_sufix + sample_sufix + \".\" + fext\n",
    "\n",
    "# model names\n",
    "dis_model_name = \"VVG-Text2Img-CDiscriminator\"\n",
    "gen_model_name = \"VVG-Text2Img-CGenerator\"\n",
    "gan_model_name = \"VVG-Text2Img-CGAN\"\n",
    "\n",
    "# to continue training after stoping script\n",
    "continue_training = True\n",
    "\n",
    "# ramdom seed\n",
    "randseed = 42\n",
    "\n",
    "# sample distribution train vs test sample size\n",
    "train_split = 0.80\n",
    "test_split = 1.0 - train_split\n",
    "\n",
    "# regex to know that column Im interested in\n",
    "keeper_regex = r\"(^ID$)|(^std_)\"\n",
    "\n",
    "imgt = rgb_sufix\n",
    "# imgt = bw_sufix\n",
    "\n",
    "# woring values for code\n",
    "work_txtf, work_imgf, work_sufix, work_imgt = text_fn, imgf_fn, sample_sufix, imgt\n",
    "\n",
    "print(\"=== working files ===\")\n",
    "print(\"\\n\", work_txtf, \"\\n\", work_imgf, \"\\n\", work_sufix, \"\\n\", work_imgt, \"\\n\", valt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\n"
     ]
    }
   ],
   "source": [
    "root_folder = os.getcwd()\n",
    "root_folder = os.path.split(root_folder)[0]\n",
    "root_folder = os.path.normpath(root_folder)\n",
    "print(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Text-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Train\\std-VVG-Gallery-Img-Data-Small.csv True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Test\\Validation-GAN-Text-Data-Small.csv False\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Models True\nc:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-MLModel-Trainer\\Data\\Reports True\n"
     ]
    }
   ],
   "source": [
    "# variable reading\n",
    "# dataframe filepath for texttual data\n",
    "text_fn_path = os.path.join(root_folder, dataf, trainf, work_txtf)\n",
    "print(text_fn_path, os.path.exists(text_fn_path))\n",
    "\n",
    "# dataframe filepath for img data\n",
    "img_fn_path = os.path.join(root_folder, dataf, trainf, work_imgf)\n",
    "print(img_fn_path, os.path.exists(img_fn_path))\n",
    "\n",
    "# dataframe filepath form GAN data\n",
    "val_fn_path = os.path.join(root_folder, dataf, testf, valt_fn)\n",
    "print(val_fn_path, os.path.exists(val_fn_path))\n",
    "\n",
    "# filepath for the models\n",
    "model_fn_path = os.path.join(root_folder, dataf, modelf)\n",
    "print(model_fn_path, os.path.exists(model_fn_path))\n",
    "\n",
    "# filepath for the reports\n",
    "report_fn_path = os.path.join(root_folder, dataf, reportf)\n",
    "print(report_fn_path, os.path.exists(report_fn_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rading training data\n",
    "# loading textual file\n",
    "text_df = pd.read_csv(\n",
    "                text_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "text_cols = text_df.columns.values\n",
    "\n",
    "# loading image file\n",
    "img_df = pd.read_csv(\n",
    "                img_fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "img_cols = img_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ID', 'F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "idx_cols = list()\n",
    "\n",
    "for tcol in text_cols:\n",
    "    if tcol in img_cols:\n",
    "        idx_cols.append(tcol)\n",
    "print(idx_cols)\n",
    "\n",
    "source_df = pd.merge(text_df, img_df, how=\"inner\", on=idx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 59 entries, 0 to 58\nData columns (total 22 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   ID                     59 non-null     object\n 1   F-number               59 non-null     object\n 2   JH-number              59 non-null     object\n 3   creator-date           59 non-null     object\n 4   creator-place          59 non-null     object\n 5   Dimensions             59 non-null     object\n 6   details                59 non-null     object\n 7   MUS_TEXT               59 non-null     object\n 8   std_cat_creator-date   59 non-null     object\n 9   std_cat_creator-place  59 non-null     object\n 10  std_cat_Dimensions     59 non-null     object\n 11  std_cat_details        59 non-null     object\n 12  clr_tokens             59 non-null     object\n 13  lemmas                 59 non-null     object\n 14  bows_tokens            59 non-null     object\n 15  idxs_tokens            59 non-null     object\n 16  tfidf_tokens           59 non-null     object\n 17  std_dvec_tokens        59 non-null     object\n 18  rgb_img                59 non-null     object\n 19  bw_img                 59 non-null     object\n 20  rgb_shape              59 non-null     object\n 21  bw_shape               59 non-null     object\ndtypes: object(22)\nmemory usage: 10.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking everything is allrigth\n",
    "img_df = None\n",
    "text_df = None\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = source_df.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and loading images into df\n",
    "# working variables\n",
    "src_col = work_imgt + \"_img\"\n",
    "tgt_col = work_imgt + \"_img\" + \"_data\"\n",
    "work_shape = work_imgt + \"_shape\"\n",
    "scale = 50\n",
    "print(src_col, tgt_col)\n",
    "source_df = get_images(root_folder, source_df, src_col, tgt_col, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update image shape\n",
    "source_df = update_shape(source_df, tgt_col, work_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_shape\n(400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# searching the biggest shape in the image files\n",
    "print(work_shape)\n",
    "shape_data = source_df[work_shape]\n",
    "max_shape = get_mshape(shape_data, work_imgt)\n",
    "print(max_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rgb_img_data pad_cnn_rgb_img_data rgb\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max shape of the images in gallery\n",
    "pad_prefix = \"pad_\"\n",
    "conv_prefix = \"cnn_\"\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = pad_prefix + conv_prefix + src_col\n",
    "\n",
    "print(src_col, tgt_col, work_imgt)\n",
    "source_df = padding_images(source_df, src_col, tgt_col, max_shape, work_imgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing regular images...\n"
     ]
    }
   ],
   "source": [
    "# reading images from folder and stadarizing images into df\n",
    "# working variables\n",
    "print(\"standarizing regular images...\")\n",
    "src_col = work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "\n",
    "# source_df = standarize_images(source_df, src_col, tgt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "standarizing padded images...\npad_cnn_rgb_img_data std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "print(\"standarizing padded images...\")\n",
    "src_col = pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "tgt_col = \"std_\" + src_col\n",
    "print(src_col, tgt_col)\n",
    "\n",
    "# std_opt = \"std\"\n",
    "std_opt = \"ctr\"\n",
    "source_df = standarize_images(source_df, src_col, tgt_col, work_imgt, std_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, s0004V1962r to d1125S2005\nData columns (total 24 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   F-number                  59 non-null     object\n 1   JH-number                 59 non-null     object\n 2   creator-date              59 non-null     object\n 3   creator-place             59 non-null     object\n 4   Dimensions                59 non-null     object\n 5   details                   59 non-null     object\n 6   MUS_TEXT                  59 non-null     object\n 7   std_cat_creator-date      59 non-null     object\n 8   std_cat_creator-place     59 non-null     object\n 9   std_cat_Dimensions        59 non-null     object\n 10  std_cat_details           59 non-null     object\n 11  clr_tokens                59 non-null     object\n 12  lemmas                    59 non-null     object\n 13  bows_tokens               59 non-null     object\n 14  idxs_tokens               59 non-null     object\n 15  tfidf_tokens              59 non-null     object\n 16  std_dvec_tokens           59 non-null     object\n 17  rgb_img                   59 non-null     object\n 18  bw_img                    59 non-null     object\n 19  rgb_shape                 59 non-null     object\n 20  bw_shape                  59 non-null     object\n 21  rgb_img_data              59 non-null     object\n 22  pad_cnn_rgb_img_data      59 non-null     object\n 23  std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(24)\nmemory usage: 11.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# shuffle the DataFrame rows\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# cleaning memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find a name of column names according to a regex\n",
    "def get_keeper_cols(col_names, search_regex):\n",
    "    ans = [i for i in col_names if re.search(search_regex, i)]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the disperse columns in the df\n",
    "def get_disperse_categories(src_df, keep_cols, max_dis, check_cols, ignore_col):\n",
    "\n",
    "    ans = list()\n",
    "\n",
    "    max_dis = 2\n",
    "    tcount = 0\n",
    "\n",
    "    while tcount < max_dis:\n",
    "        for label_col in keep_columns:\n",
    "\n",
    "            if label_col != ignore_col:\n",
    "\n",
    "                label_count = src_df[label_col].value_counts(normalize=False)\n",
    "\n",
    "                if tcount < label_count.shape[0] and (check_cols in label_col):\n",
    "                    tcount = label_count.shape[0]\n",
    "                    ans.append(label_col)\n",
    "                # print(\"count values of\", label_col, \":=\", label_count.shape)#.__dict__)\n",
    "        tcount = tcount + 1\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the disperse columns from the interesting ones\n",
    "def remove_disperse_categories(keep_columns, too_disperse):\n",
    "    for too in too_disperse:\n",
    "        keep_columns.remove(too)\n",
    "    return keep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_corpus(train_df, dvector_col, pad_prefix):\n",
    "    # getting the corpus dense vectors\n",
    "    work_corpus = np.asarray(train_df[dvector_col], dtype=\"object\")\n",
    "\n",
    "    # converting list of list to array of array\n",
    "    print(\"Original txt shape\", work_corpus.shape)\n",
    "\n",
    "    # padding the representation\n",
    "    work_corpus = pad_sequences(work_corpus, dtype='object', padding=\"post\")\n",
    "    # print(\"Padded txt shape\", work_corpus.shape)\n",
    "\n",
    "    # creating the new column and saving padded data\n",
    "    padded_col_dvector = pad_prefix + dvector_col\n",
    "\n",
    "    # print(padded_col)\n",
    "    train_df[padded_col_dvector] = list(work_corpus)\n",
    "    print(\"Padded txt shape\", work_corpus.shape)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_categories(train_df, cat_cols, tgt_col):\n",
    "\n",
    "    labels_data = train_df[cat_cols]\n",
    "    labels_concat = list()\n",
    "\n",
    "    # concatenating all category labels from dataframe\n",
    "    for index, row in labels_data.iterrows():\n",
    "        row = concat_labels(row, labels_cols)\n",
    "        labels_concat.append(row)\n",
    "\n",
    "    # print(len(labels_concat[0]), type(labels_concat[0]))\n",
    "    # updating dataframe\n",
    "    tcat_label_col = \"std_cat_labels\"\n",
    "    train_df[tgt_col] = labels_concat\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to adjust the textual data for the LSTM layers in the model\n",
    "def format_corpus(corpus, timesteps, features):\n",
    "\n",
    "    # preparation for reshape lstm model\n",
    "    corpus = temporalize(corpus, timesteps)\n",
    "    print(corpus.shape)\n",
    "\n",
    "    corpus = corpus.reshape((corpus.shape[0], timesteps, features))\n",
    "    print(corpus.shape)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ original input/interested columns ------\n['F-number', 'JH-number', 'creator-date', 'creator-place', 'Dimensions', 'details', 'MUS_TEXT', 'std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'clr_tokens', 'lemmas', 'bows_tokens', 'idxs_tokens', 'tfidf_tokens', 'std_dvec_tokens', 'rgb_img', 'bw_img', 'rgb_shape', 'bw_shape', 'rgb_img_data', 'pad_cnn_rgb_img_data', 'std_pad_cnn_rgb_img_data']\n\n\n------ Interesting columns ------\n['std_cat_creator-date', 'std_cat_creator-place', 'std_cat_Dimensions', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# selecting data to train\n",
    "# want to keep the columns starting with STD_\n",
    "keep_columns = list(source_df.columns)\n",
    "print(\"------ original input/interested columns ------\")\n",
    "print(keep_columns)\n",
    "\n",
    "# create the columns Im interesting in\n",
    "keep_columns = get_keeper_cols(keep_columns, keeper_regex)\n",
    "# keep_columns = [i for i in df_columns if re.search(keeper_regex, i)]\n",
    "\n",
    "print(\"\\n\\n------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-date', 'std_cat_Dimensions']\n"
     ]
    }
   ],
   "source": [
    "too_disperse = get_disperse_categories(source_df, keep_columns, 2, \"std_cat_\", \"std_pad_cnn_rgb_img_data\")\n",
    "print(too_disperse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ Interesting columns ------\n['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\n"
     ]
    }
   ],
   "source": [
    "# creating the training dataframe\n",
    "keep_columns = remove_disperse_categories(keep_columns, too_disperse)\n",
    "# keep_columns.remove(\"ID\")\n",
    "print(\"------ Interesting columns ------\")\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training dataframe\n",
    "train_df = pd.DataFrame(source_df, columns=keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the stuff\n",
    "train_df = train_df.sample(frac = 1)\n",
    "source_df = None\n",
    "df_columns = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, d0014V1962v to d0159V1962r\nData columns (total 4 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\ndtypes: object(4)\nmemory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Padded image column in dataframe:  std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "pad_regex = u\"^std_pad_\"\n",
    "padimg_col = get_keeper_cols(df_columns, pad_regex)\n",
    "padimg_col = padimg_col[0]\n",
    "print(\"Padded image column in dataframe: \", str(padimg_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dense vector column in dataframe:  std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "# getting the column with the relevant data to train\n",
    "dvec_regex = u\"^std_dvec\"\n",
    "dvector_col = get_keeper_cols(df_columns, dvec_regex)\n",
    "dvector_col = dvector_col[0]\n",
    "print(\"Dense vector column in dataframe: \", str(dvector_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix column data type\n",
    "work_corpus = train_df[dvector_col]\n",
    "work_corpus = format_dvector(work_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing type in dataframe\n",
    "train_df[dvector_col] = work_corpus\n",
    "work_corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original txt shape (59,)\nPadded txt shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# padding training data according to max length of text corpus\n",
    "pad_prefix = \"pad_\"\n",
    "recurrent_prefix = \"lstm_\"\n",
    "\n",
    "train_df = padding_corpus(train_df, dvector_col, pad_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_img_col = \"std_\" + work_imgt + \"_img\" + \"_data\"\n",
    "padded_img_col = \"std_\" + pad_prefix + conv_prefix + work_imgt + \"_img\" + \"_data\"\n",
    "padded_col_dvector = pad_prefix + dvector_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\nClassifier trainable labels in dataframe:  ['std_cat_creator-place', 'std_cat_details']\ncategories heat column: std_cat_labels\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "cat_regex = u\"^std_cat_\"\n",
    "labels_cols = get_keeper_cols(keep_columns, cat_regex)\n",
    "print(\"Classifier trainable labels in dataframe: \", str(labels_cols))\n",
    "\n",
    "# updating dataframe with hot/concatenated categories\n",
    "tcat_label_col = \"std_cat_labels\"\n",
    "print(\"categories heat column:\", tcat_label_col)\n",
    "train_df = heat_categories(train_df, labels_cols, tcat_label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['std_cat_creator-place', 'std_cat_details', 'std_dvec_tokens', 'std_pad_cnn_rgb_img_data']\nTrainable labels columns in dataframe:  ['std_cat_creator-place', 'std_cat_details']\n"
     ]
    }
   ],
   "source": [
    "# getting the columns with the relevant labels to predict\n",
    "print(keep_columns)\n",
    "labels_cols = [i for i in keep_columns if re.search(u\"^std_cat_\", i)]\n",
    "print(\"Trainable labels columns in dataframe: \", str(labels_cols))\n",
    "\n",
    "labels_data = train_df[labels_cols]\n",
    "labels_concat = list()\n",
    "\n",
    "# concatenating all category labels from dataframe\n",
    "for index, row in labels_data.iterrows():\n",
    "    row = concat_labels(row, labels_cols)\n",
    "    labels_concat.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pad_std_dvec_tokens\n"
     ]
    }
   ],
   "source": [
    "text_lstm_col = padded_col_dvector\n",
    "print(text_lstm_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "std_pad_cnn_rgb_img_data\n"
     ]
    }
   ],
   "source": [
    "working_img_col = padded_img_col\n",
    "# working_img_col = regular_img_col\n",
    "print(working_img_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nIndex: 59 entries, d0014V1962v to d0159V1962r\nData columns (total 6 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   std_cat_creator-place     59 non-null     object\n 1   std_cat_details           59 non-null     object\n 2   std_dvec_tokens           59 non-null     object\n 3   std_pad_cnn_rgb_img_data  59 non-null     object\n 4   pad_std_dvec_tokens       59 non-null     object\n 5   std_cat_labels            59 non-null     object\ndtypes: object(6)\nmemory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 (400, 400, 3)\n",
      "final X_img shape (59, 400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "og_shape = train_df[working_img_col][0].shape# y[0].shape\n",
    "X_img_len = train_df[working_img_col].shape[0] #y.shape[0]\n",
    "print(X_img_len, og_shape)\n",
    "\n",
    "X_img = None\n",
    "\n",
    "for img in train_df[working_img_col]:\n",
    "\n",
    "    if X_img is None:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = img\n",
    "    else:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        X_img = np.concatenate((X_img, img), axis=0)\n",
    "\n",
    "print(\"final X_img shape\", X_img.shape)\n",
    "# y.shape = (1899, 800, 800, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_img[0]))\n",
    "print(type(X_img[0][0]))\n",
    "print(X_img[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_img.shape) == 3:\n",
    "    X_img = X_img[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 16)\n"
     ]
    }
   ],
   "source": [
    "# y = train_df[working_img_col]\n",
    "# y = np.expand_dims(y, axis=0)\n",
    "y_labels = np.asarray([np.asarray(j, dtype=\"object\") for j in train_df[tcat_label_col]], dtype=\"object\")\n",
    "print(\"y shape\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y shape (59, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.ones((y_labels.shape[0],1)).astype(\"float32\")\n",
    "print(\"y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y classification category\n<class 'numpy.ndarray'>\n<class 'numpy.float32'>\n(1,)\ny labels category\n<class 'numpy.ndarray'>\n<class 'float'>\n(16,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y classification category\")\n",
    "print(type(y[0]))\n",
    "print(type(y[0][0]))\n",
    "print(y[1].shape)\n",
    "\n",
    "print(\"y labels category\")\n",
    "print(type(y_labels[0]))\n",
    "print(type(y_labels[0][0]))\n",
    "print(y_labels[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "final X_lstm shape (59, 142)\n"
     ]
    }
   ],
   "source": [
    "# creating Train/Test sample\n",
    "# getting the X, y to train, as is autoencoder both are the same\n",
    "X_txt = np.asarray([np.asarray(i, dtype=\"object\") for i in train_df[text_lstm_col]], dtype=\"object\")\n",
    "# X = np.array(train_df[text_lstm_col]).astype(\"object\")\n",
    "# X = train_df[text_lstm_col]\n",
    "print(\"final X_lstm shape\", X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'float'>\n(142,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_txt[0]))\n",
    "print(type(X_txt[0][0]))\n",
    "print(X_txt[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 142\n"
     ]
    }
   ],
   "source": [
    "# timestep is the memory of what i read, this is the longest sentence I can remember in the short term\n",
    "# neet to look for the best option, in small the max is 15\n",
    "timesteps = 15\n",
    "\n",
    "# features is the max length in the corpus, after padding!!!!\n",
    "features = X_txt[0].shape[0]\n",
    "print(timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 1, 142)\n(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "X_txt = format_corpus(X_txt, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "diff_txt = y.shape[0] - X_txt.shape[0]\n",
    "print(diff_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n"
     ]
    }
   ],
   "source": [
    "Xa = X_txt[-diff_txt:]\n",
    "X_txt = np.append(X_txt, Xa, axis=0)\n",
    "print(X_txt.shape)\n",
    "Xa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(59, 15, 142)\n(59, 400, 400, 3)\n(59, 1)\n(59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt.shape)\n",
    "print(X_img.shape)\n",
    "print(y.shape)\n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n(400, 400, 3)\n(1,)\n(16,)\n"
     ]
    }
   ],
   "source": [
    "print(X_txt[0].shape)\n",
    "print(X_img[0].shape)\n",
    "print(y[0].shape)\n",
    "print(y_labels[0].shape)\n",
    "txt_og_shape = X_txt[0].shape\n",
    "img_og_shape = X_img[0].shape\n",
    "cat_og_shape = y[0].shape\n",
    "lab_og_shape = y_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xt = X_txt # np.array(X).astype(\"object\")\n",
    "# Xi = X_img\n",
    "# yt = y # np.array(y).astype(\"object\")\n",
    "# # ya = y[0:timesteps]\n",
    "# train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# ML Model Definition\n",
    "\n",
    "## Image GAN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional generator for images\n",
    "def create_img_generator(latent_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    latent_n = model_cfg.get(\"latent_img_size\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    # latent img shape\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    rs = model_cfg.get(\"return_sequences\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_shape, name=\"ImgGenIn\")\n",
    "\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=latent_shape, \n",
    "                    name = \"ImgGenMask_1\")(in_latent) # concat1\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr2 = LSTM(lstm_units, activation=in_lyr_act, \n",
    "                    input_shape=latent_shape, \n",
    "                    return_sequences=rs, name=\"ImgGenLSTM_2\")(lyr1)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr3 = Flatten(name=\"ImgGenFlat_3\")(lyr2)\n",
    "\n",
    "    # dense layer\n",
    "    lyr4 = Dense(latent_n, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgGenDense_4\")(lyr3)\n",
    "    \n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr5 = Reshape(latent_img_shape, name=\"ImgGenReshape_5\")(lyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr6 = Conv2DTranspose(int(filters/8), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgGenBN_7\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgGenDrop_8\")(lyr7)\n",
    "\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_9\")(lyr8)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr10 = Conv2DTranspose(int(filters/2), kernel_size=ksize, \n",
    "                            strides=out_stsize, activation=out_lyr_act, \n",
    "                            padding=out_pad, name=\"ImgGenConv2D_10\")(lyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr11 = BatchNormalization(name=\"ImgGenBN_11\")(lyr10)\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgGenDrop_12\")(lyr11)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr13 = Conv2DTranspose(filters, kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgGenConv2D_13\")(lyr12)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize, \n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgGenOut\")(lyr13)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_latent, outputs=out_img)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional discriminator for images\n",
    "def create_img_discriminator(img_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # input layer config, image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"DisImgIn\")\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer\n",
    "    lyr1 = Conv2D(in_filters, kernel_size=in_ksize, \n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgDisConv2D_1\")(in_img)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr2 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"ImgDisBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"ImgDisDrop_4\")(lyr3)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr5 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_4\")(lyr4)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr6 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgDisConv2D_5\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgDisBN_6\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgDisDrop_7\")(lyr7)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr9 = Flatten(name=\"ImgDisFlat_8\")(lyr8)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr10 = Dense(int(mid_disn), activation=hid_cls_act, name=\"ImgDisDense_9\")(lyr9)\n",
    "    lyr11 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"ImgDisDense_10\")(lyr10)\n",
    "    # drop layer\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgDisDrop_11\")(lyr11)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr13 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"ImgDisDense_12\")(lyr12)\n",
    "    lyr14 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"ImgDisDense_13\")(lyr13)\n",
    "    # drop layer\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"ImgDisDrop_14\")(lyr14)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr16 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"ImgDisDense_15\")(lyr15)\n",
    "    lyr17 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"ImgDisDense_16\")(lyr16)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"ImgDisOut\")(lyr17)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_img, outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "\t# make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "\t# get noise and label inputs from generator model\n",
    "    gen_noise = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model(gen_output)\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model(gen_noise, gan_output)\n",
    "    # compile model\n",
    "    model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # model.compile(loss=ls, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Text GAN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM generator for text\n",
    "def create_txt_generator(latent_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # hidden layer config\n",
    "    latent_n = model_cfg.get(\"mid_gen_neurons\")\n",
    "    latent_reshape = model_cfg.get(\"latent_lstm_reshape\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # output layer condig\n",
    "    txt_shape = model_cfg.get(\"output_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_shape, name=\"TxtGenIn\")\n",
    "\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=latent_shape, \n",
    "                    name = \"TxtGenMask_1\")(in_latent) # concat1\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr2 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    input_shape=latent_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtGenLSTM_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"TxtGenBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"TxtGenDrop_4\")(lyr3)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr5 = Flatten(name=\"TxtGenFlat_5\")(lyr4)\n",
    "\n",
    "    # dense layer\n",
    "    lyr6 = Dense(latent_n, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"TxtGenDense_6\")(lyr5)\n",
    "\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr7 = Reshape(latent_reshape, name=\"TxtGenReshape_7\")(lyr6)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr8 = BatchNormalization(name=\"TxtGenBN_8\")(lyr7)\n",
    "    lyr9 = Dropout(hid_ldrop, name=\"TxtGenDrop_9\")(lyr8)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr10 = LSTM(int(lstm_units/4), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_10\")(lyr9)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr11 = LSTM(int(lstm_units/2), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_11\")(lyr10)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr12 = BatchNormalization(name=\"TxtGenBN_12\")(lyr11)\n",
    "    lyr13 = Dropout(hid_ldrop, name=\"TxtGenDrop_13\")(lyr12)\n",
    "\n",
    "    # output layer, dense time sequential layer.\n",
    "    lyr14 = LSTM(lstm_units, activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenDrop_14\")(lyr13)\n",
    "\n",
    "    out_txt = TimeDistributed(Dense(txt_shape, activation=out_lyr_act), name = \"GenTxtOut\")(lyr14)\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=in_latent, outputs=out_txt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM discriminator for text\n",
    "def create_txt_discriminator(txt_shape, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # hidden layer config\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_txt = Input(shape=txt_shape, name=\"DisTxtIn\")\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=txt_shape, \n",
    "                    name = \"TxtDisMask_1\")(in_txt) # concat1\n",
    "\n",
    "    # input LSTM layer\n",
    "    lyr2 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    input_shape=txt_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtDisLSTM_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"TxtDisBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"TxtDisDrop_4\")(lyr3)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr5 = LSTM(int(lstm_units/2), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtDisLSTM_5\")(lyr4)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr6 = LSTM(int(lstm_units/4), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtDisLSTM_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"TxtDisBN_7\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"TxtDisDrop_8\")(lyr7)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr9 = Flatten(name=\"TxtDisFlat_9\")(lyr8)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr10 = Dense(int(mid_disn), activation=hid_cls_act, name=\"TxtDisDense_10\")(lyr9)\n",
    "    lyr11 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"TxtDisDense_11\")(lyr10)\n",
    "    # drop layer\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"TxtDisDrop_12\")(lyr11)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr13 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"TxtDisDense_13\")(lyr12)\n",
    "    lyr14 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"TxtDisDense_14\")(lyr13)\n",
    "    # drop layer\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"TxtDisDrop_15\")(lyr14)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr16 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"TxtDisDense_16\")(lyr15)\n",
    "    lyr17 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"TxtDisDense_17\")(lyr16)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"TxtDisOut\")(lyr17)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=in_txt, outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model(gen_output)\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model(gen_noise, gan_output)\n",
    "    # compile model\n",
    "    model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Conditional Img GAN: CGAN-img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional generator for images\n",
    "def create_img_cgenerator(latent_shape, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    memory = latent_shape[0]\n",
    "    features = latent_shape[1]\n",
    "    lbl_neurons = model_cfg.get(\"labels_neurons\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    latent_n = model_cfg.get(\"latent_img_size\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    # latent img shape\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    rs = model_cfg.get(\"return_sequences\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # CONDITIONAL LABELS LAYERS\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"ImgCGenLblIn\")\n",
    "    # embedding categorical textual input\n",
    "    cond1 = Embedding(memory, features, input_length=n_labels, name=\"ImgCGenLblEmb_1\")(in_labels)\n",
    "\n",
    "    # flat layer\n",
    "    cond2 = Flatten(name=\"ImgCGenLblFlat_2\")(cond1)\n",
    "    # dense layers\n",
    "    cond3 = Dense(int(lbl_neurons/2), activation=lbl_ly_actf, name=\"ImgCGenLblDense_3\")(cond2)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    cond4 = BatchNormalization(name=\"ImgCGenLblBN_4\")(cond3)\n",
    "    cond5 = Dropout(hid_ldrop, name=\"ImgCGenLblDrop_5\")(cond4)\n",
    "\n",
    "    cond6 = Dense(lbl_neurons, activation=lbl_ly_actf, name=\"ImgCGenLblDense_6\")(cond5)\n",
    "    # reshape layer\n",
    "    cond7 = Reshape(latent_shape, name=\"ImgCGenLblOut\")(cond6)\n",
    "\n",
    "    # GENERATOR DEFINITION\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_shape, name=\"ImgCGenIn\")\n",
    "\n",
    "    # concat generator layers + label layers\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"ImgCGenConcat\")([in_latent, cond7])\n",
    "\n",
    "    # masking input text\n",
    "    lyr1 = Masking(mask_value=mval, input_shape=latent_shape, \n",
    "                    name = \"ImgCGenMask_1\")(lbl_concat) # contat!!!!\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    lyr2 = LSTM(lstm_units, activation=in_lyr_act, \n",
    "                    input_shape=latent_shape, \n",
    "                    return_sequences=rs, name=\"ImgCGenLSTM_2\")(lyr1)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr3 = Flatten(name=\"ImgCGenFlat_3\")(lyr2)\n",
    "\n",
    "    # dense layer\n",
    "    lyr4 = Dense(latent_n, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgCGenDense_4\")(lyr3)\n",
    "    \n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    lyr5 = Reshape(latent_img_shape, name=\"ImgCGenReshape_5\")(lyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr6 = Conv2DTranspose(int(filters/8), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_6\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgCGenBN_7\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgCGenDrop_8\")(lyr7)\n",
    "\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_9\")(lyr8)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr10 = Conv2DTranspose(int(filters/2), kernel_size=ksize, \n",
    "                            strides=out_stsize, activation=out_lyr_act, \n",
    "                            padding=out_pad, name=\"ImgCGenConv2D_10\")(lyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr11 = BatchNormalization(name=\"ImgCGenBN_11\")(lyr10)\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgCGenDrop_12\")(lyr11)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    lyr13 = Conv2DTranspose(filters, kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_13\")(lyr12)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize, \n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgCGenOut\")(lyr13)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_latent, in_labels], outputs=out_img)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional discriminator for images\n",
    "def create_img_cdiscriminator(img_shape, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    memory = model_cfg.get(\"timesteps\")\n",
    "    features = model_cfg.get(\"max_features\")\n",
    "    lbl_neurons = model_cfg.get(\"labels_neurons\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    lbl_filters = model_cfg.get(\"labels_filters\")\n",
    "    lbl_ksize = model_cfg.get(\"labels_kernel_size\")\n",
    "    lbl_stsize = model_cfg.get(\"labels_stride\")\n",
    "    gen_reshape = model_cfg.get(\"labels_reshape\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "\n",
    "    # input layer config, image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "    # mid neuron size\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # LABEL IMG LAYERS\n",
    "    # label inpuy\n",
    "    in_labels = Input(shape=(n_labels,), \n",
    "                        name=\"ImgCDisLblIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    cond1 = Embedding(memory, features, \n",
    "                        input_length=n_labels, \n",
    "                        name=\"ImgCDisLblEmb_1\")(in_labels)\n",
    "\n",
    "    # flat layer\n",
    "    cond2 = Flatten(name=\"ImgCDisLblFlat_2\")(cond1)\n",
    "    cond3 = Dense(lbl_neurons, activation=lbl_ly_actf, \n",
    "                    name=\"ImgCDisLblDense_3\")(cond2)\n",
    "    \n",
    "    # reshape layer\n",
    "    cond4 = Reshape(gen_reshape, name=\"ImgCDisLblReshape_4\")(cond3)\n",
    "\n",
    "    # transpose conv2D layers\n",
    "    cond5 = Conv2DTranspose(int(lbl_filters/8), kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblConv2D_5\")(cond4)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    cond6 = BatchNormalization(name=\"ImgCDisLblBN_6\")(cond5)\n",
    "    cond7 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_7\")(cond6)\n",
    "\n",
    "    # trnaspose conv2D layers\n",
    "    cond8 = Conv2DTranspose(int(lbl_filters/4), kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblConv2D_8\")(cond7)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    cond9 = BatchNormalization(name=\"ImgCDisLblBN_9\")(cond8)\n",
    "    cond10 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_10\")(cond9)\n",
    "\n",
    "    # conditional layer output\n",
    "    cond11 = Conv2DTranspose(img_shape[2], kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblOut\")(cond10)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"DisImgIn\")\n",
    "\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"CDisConcat_1\")([in_img, cond11])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer\n",
    "    lyr1 = Conv2D(in_filters, kernel_size=in_ksize, \n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgCDisConv2D_1\")(lbl_concat)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr2 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_2\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"ImgCDisBN_3\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"ImgCDisDrop_4\")(lyr3)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr5 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_4\")(lyr4)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr6 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_5\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgCDisBN_6\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgCDisDrop_7\")(lyr7)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr9 = Flatten(name=\"ImgCDisFlat_8\")(lyr8)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr10 = Dense(int(mid_disn), activation=hid_cls_act, name=\"ImgCDisDense_9\")(lyr9)\n",
    "    lyr11 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"ImgCDisDense_10\")(lyr10)\n",
    "    # drop layer\n",
    "    lyr12 = Dropout(hid_ldrop, name=\"ImgCDisDrop_11\")(lyr11)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr13 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"ImgCDisDense_12\")(lyr12)\n",
    "    lyr14 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"ImgCDisDense_13\")(lyr13)\n",
    "    # drop layer\n",
    "    lyr15 = Dropout(hid_ldrop, name=\"ImgCDisDrop_14\")(lyr14)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr16 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"ImgCDisDense_15\")(lyr15)\n",
    "    lyr17 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"ImgCDisDense_16\")(lyr16)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"ImgCDisOut\")(lyr17)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_img, in_labels], outputs=out_cls)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_cgan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_output, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "source": [
    "## Multi GAN txt2img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Conv discriminator for image and text\n",
    "def create_multi_discriminator(img_shape, txt_shape, model_cfg):\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=[in_img, in_txt], outputs=out_cls)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_generator(img_shape, txt_shape, model_cfg):\n",
    "\n",
    "    # model definition\n",
    "    gen_model = Model(inputs=in_latent, outputs=[out_img, out_txt])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_gan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    cdis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = cgen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = cgen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_output, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1], metrics=gan_cfg[2])\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "source": [
    "## Multi CGAN txt2img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cgenerator(latent_shape, img_shape, txt_shape, n_labels, model_cfg):\n",
    "    # MODEL CONFIG\n",
    "    # config for conditional labels\n",
    "    # print(\"=======================\\n\",model_cfg, \"=====================\")\n",
    "    memory = latent_shape[0]\n",
    "    features = latent_shape[1]\n",
    "    lbl_neurons = model_cfg.get(\"labels_neurons\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"gen_dropout_rate\")\n",
    "\n",
    "    # def of the latent space size for the input\n",
    "    # input layer config, latent txt space\n",
    "    latent_nimg = model_cfg.get(\"latent_img_size\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    latent_img_shape = model_cfg.get(\"latent_img_shape\")\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "\n",
    "    # hidden layer config\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    rs = model_cfg.get(\"return_sequences\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    latent_ntxt = model_cfg.get(\"mid_gen_neurons\")\n",
    "    latent_txt_shape = model_cfg.get(\"latent_lstm_reshape\")\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # output layer condig\n",
    "    out_filters = model_cfg.get(\"output_filters\")\n",
    "    out_ksize = model_cfg.get(\"output_kernel_size\")\n",
    "    out_stsize = model_cfg.get(\"output_stride\")\n",
    "    out_pad = model_cfg.get(\"output_padding\")\n",
    "    img_shape = model_cfg.get(\"output_shape\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "    txt_shape = model_cfg.get(\"output_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # MODEL DEF\n",
    "    # CONDITIONAL LABELS LAYERS FOR IMG + TXT\n",
    "    # label input\n",
    "    in_labels = Input(shape=(n_labels,), name=\"ImgTxtCGenLblIn\")\n",
    "    # embedding categorical textual input\n",
    "    cond1 = Embedding(memory, features, input_length=n_labels, name=\"ImgTxtCGenLblEmb_1\")(in_labels)\n",
    "\n",
    "    # flat layer\n",
    "    cond2 = Flatten(name=\"ImgTxtCGenLblFlat_2\")(cond1)\n",
    "    # dense layers\n",
    "    cond3 = Dense(int(lbl_neurons/2), activation=lbl_ly_actf, name=\"ImgTxtCGenLblDense_3\")(cond2)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    cond4 = BatchNormalization(name=\"ImgTxtCGenLblBN_4\")(cond3)\n",
    "    cond5 = Dropout(hid_ldrop, name=\"ImgTxtCGenLblDrop_5\")(cond4)\n",
    "\n",
    "    cond6 = Dense(lbl_neurons, activation=lbl_ly_actf, name=\"ImgTxtCGenLblDense_6\")(cond5)\n",
    "    # reshape layer\n",
    "    cond7 = Reshape(latent_shape, name=\"ImgTxtCGenLblOut\")(cond6)\n",
    "\n",
    "    # GENERATOR DEFINITION\n",
    "    #LATENT LAYER CREATION\n",
    "    # input layer\n",
    "    in_latent = Input(shape=latent_shape, name=\"ImgTxtCGenIn\")\n",
    "\n",
    "    # concat generator layers + label layers\n",
    "    lbl_concat = Concatenate(axis=-1, name=\"ImgTxtCGenConcat\")([in_latent, cond7])\n",
    "\n",
    "    # masking input text\n",
    "    olyr1 = Masking(mask_value=mval, input_shape=latent_shape, \n",
    "                    name = \"ImgTxtCGenMask_1\")(lbl_concat) # contat!!!!\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    olyr2 = LSTM(lstm_units, activation=in_lyr_act, \n",
    "                    input_shape=latent_shape, \n",
    "                    return_sequences=rs, name=\"ImgTxtCGenLSTM_2\")(olyr1)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    olyr3 = Flatten(name=\"ImgTxtCGenFlat_3\")(olyr2)\n",
    "\n",
    "    # dense layer\n",
    "    olyr4 = Dense(latent_nimg, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgTxtCGenDense_4\")(olyr3)\n",
    "    \n",
    "    olyr5 = Dense(latent_ntxt, \n",
    "                activation=hid_lyr_act, \n",
    "                name=\"ImgTxtCGenDense_5\")(olyr3)\n",
    "\n",
    "    # IMG GENERATOR\n",
    "    # reshape layer 1D-> 2D (rbg image)\n",
    "    ilyr5 = Reshape(latent_img_shape, name=\"ImgCGenReshape_5\")(olyr4)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr6 = Conv2DTranspose(int(filters/8), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_6\")(ilyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr7 = BatchNormalization(name=\"ImgCGenBN_7\")(ilyr6)\n",
    "    ilyr8 = Dropout(hid_ldrop, name=\"ImgCGenDrop_8\")(ilyr7)\n",
    "\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr9 = Conv2DTranspose(int(filters/4), kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_9\")(ilyr8)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr10 = Conv2DTranspose(int(filters/2), kernel_size=ksize, \n",
    "                            strides=out_stsize, activation=out_lyr_act, \n",
    "                            padding=out_pad, name=\"ImgCGenConv2D_10\")(ilyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    ilyr11 = BatchNormalization(name=\"ImgCGenBN_11\")(ilyr10)\n",
    "    ilyr12 = Dropout(hid_ldrop, name=\"ImgCGenDrop_12\")(ilyr11)\n",
    "\n",
    "    # transpose conv2D layer\n",
    "    ilyr13 = Conv2DTranspose(filters, kernel_size=ksize, \n",
    "                            strides=stsize, activation=hid_lyr_act, \n",
    "                            padding=pad, name=\"ImgCGenConv2D_13\")(ilyr12)\n",
    "\n",
    "    # output layer\n",
    "    out_img = Conv2D(out_filters, kernel_size=out_ksize, \n",
    "                        strides=out_stsize, activation=out_lyr_act, \n",
    "                        padding=out_pad, input_shape=img_shape, \n",
    "                        name=\"ImgCGenOut\")(ilyr13)\n",
    "\n",
    "    # TXT GENERATOR\n",
    "    # reshape layer 1D-> 2D (descriptive txt)\n",
    "    tlyr6 = Reshape(latent_txt_shape, name=\"TxtGenReshape_6\")(olyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr7 = BatchNormalization(name=\"TxtGenBN_7\")(tlyr7)\n",
    "    tlyr8 = Dropout(hid_ldrop, name=\"TxtGenDrop_8\")(tlyr7)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tlyr9 = LSTM(int(lstm_units/4), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_10\")(tlyr8)\n",
    "\n",
    "    # intermediate recurrent layer\n",
    "    tlyr10 = LSTM(int(lstm_units/2), activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenLSTM_11\")(tlyr9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    tlyr11 = BatchNormalization(name=\"TxtGenBN_12\")(tlyr10)\n",
    "    tlyr12 = Dropout(hid_ldrop, name=\"TxtGenDrop_12\")(tlyr11)\n",
    "\n",
    "    # output layer, dense time sequential layer.\n",
    "    tlyr13 = LSTM(lstm_units, activation=hid_lyr_act, \n",
    "                    input_shape=mem_shape, \n",
    "                    return_sequences=rs, \n",
    "                    name=\"TxtGenDrop_13\")(tlyr12)\n",
    "\n",
    "    out_txt = TimeDistributed(Dense(txt_shape, activation=out_lyr_act), name = \"GenTxtOut\")(tlyr14)\n",
    "\n",
    "    # MODEL DEFINITION\n",
    "    model = Model(inputs=[in_latent, in_labels], outputs=[out_img, out_txt])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Conv conditianal discriminator for text and images\n",
    "def create_multi_cdiscriminator(img_shape, txt_shape, n_labels, model_cfg):\n",
    "\n",
    "    # MODEL CONFIG\n",
    "    # config for txt + img conditional labels\n",
    "    memory = model_cfg.get(\"timesteps\")\n",
    "    features = model_cfg.get(\"max_features\")\n",
    "    lbl_neurons = model_cfg.get(\"labels_neurons\")\n",
    "    lbl_ly_actf = model_cfg.get(\"labels_lyr_activation\")\n",
    "    lbl_filters = model_cfg.get(\"labels_filters\")\n",
    "    lbl_ksize = model_cfg.get(\"labels_kernel_size\")\n",
    "    lbl_stsize = model_cfg.get(\"labels_stride\")\n",
    "    lbl_lstm = model_cfg.get(\"labels_lstm_neurons\")\n",
    "    lbl_rs = model_cfg.get(\"labels_return_sequences\")\n",
    "    dis_img_reshape = model_cfg.get(\"labels_img_reshape\")\n",
    "    dis_txt_reshape = model_cfg.get(\"labels_txt_reshape\")\n",
    "\n",
    "    # input layer config for image classification\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "    in_filters = model_cfg.get(\"input_filters\")\n",
    "    in_ksize = model_cfg.get(\"input_kernel_size\")\n",
    "    in_stsize = model_cfg.get(\"input_stride\")\n",
    "    in_pad = model_cfg.get(\"input_padding\")\n",
    "\n",
    "    # input layer config for txt classification\n",
    "    mval = model_cfg.get(\"mask_value\")\n",
    "    in_rs = model_cfg.get(\"input_return_sequences\")\n",
    "    in_lstm = model_cfg.get(\"input_lstm_neurons\")\n",
    "    in_lyr_act = model_cfg.get(\"input_lyr_activation\")\n",
    "\n",
    "    # encoding hidden layer config for image classification\n",
    "    filters = model_cfg.get(\"filters\")\n",
    "    ksize = model_cfg.get(\"kernel_size\")\n",
    "    stsize = model_cfg.get(\"stride\")\n",
    "    pad = model_cfg.get(\"padding\")\n",
    "    hid_lyr_act = model_cfg.get(\"hidden_lyr_activation\")\n",
    "    hid_ldrop = model_cfg.get(\"dis_dropout_rate\")\n",
    "\n",
    "    # encoding hidden layer config for text classification\n",
    "    lstm_units = model_cfg.get(\"lstm_neurons\")\n",
    "    mem_shape = model_cfg.get(\"memory_shape\")\n",
    "    rs = model_cfg.get(\"hidden_return_sequences\")\n",
    "\n",
    "    # mid classification config\n",
    "    mid_disn = model_cfg.get(\"mid_dis_neurons\")\n",
    "    hid_cls_act = model_cfg.get(\"dense_cls_activation\")\n",
    "\n",
    "    # output layer config\n",
    "    out_nsize = model_cfg.get(\"output_dis_neurons\")\n",
    "    out_lyr_act = model_cfg.get(\"output_lyr_activation\")\n",
    "\n",
    "    # IMG/TXT LABELS CONDITIONAL LAYERS\n",
    "    # labels input\n",
    "    in_labels = Input(shape=(n_labels,), \n",
    "                        name=\"MultiCDisLblIn\")\n",
    "\n",
    "    # embedding categorical textual input\n",
    "    cond1 = Embedding(memory, features, \n",
    "                        input_length=n_labels, \n",
    "                        name=\"MultiCDisLblEmb_1\")(in_labels)\n",
    "\n",
    "    # flat layer\n",
    "    cond2 = Flatten(name=\"MultiCDisLblFlat_2\")(cond1)\n",
    "    # img dense layer\n",
    "    cond3 = Dense(lbl_neurons, activation=lbl_ly_actf, \n",
    "                    name=\"MultiCDisLblDense_3\")(cond2)\n",
    "    \n",
    "    # image reshape layer\n",
    "    cond4i = Reshape(dis_img_reshape, name=\"ImgCDisLblReshape_4\")(cond3)\n",
    "\n",
    "    # txt dense layer\n",
    "    cond5 = Dense(int(memory*features), activation=lbl_ly_actf, \n",
    "                    name=\"MultiCDisLblDense_5\")(cond2)\n",
    "    # txt reshape layer\n",
    "    cond6t = Reshape(dis_txt_reshape, name=\"TxtCDisLblReshape_6\")(cond5)\n",
    "\n",
    "    # transpose conv2D layer for img\n",
    "    cond7 = Conv2DTranspose(int(lbl_filters/8), kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblConv2D_7\")(cond4i)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit for img\n",
    "    cond8 = BatchNormalization(name=\"ImgCDisLblBN_8\")(cond7)\n",
    "    cond9 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_9\")(cond8)\n",
    "\n",
    "    # transpose conv2D layers for img\n",
    "    cond10 = Conv2DTranspose(int(lbl_filters/4), kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblConv2D_10\")(cond9)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit for img\n",
    "    cond11 = BatchNormalization(name=\"ImgCDisLblBN_11\")(cond10)\n",
    "    cond12 = Dropout(hid_ldrop, name=\"ImgCDisLblDrop_12\")(cond11)\n",
    "\n",
    "    # conditional layer output for img\n",
    "    cond13 = Conv2DTranspose(img_shape[2], kernel_size=ksize, \n",
    "                                strides=stsize, activation=lbl_ly_actf, \n",
    "                                padding=pad, name=\"ImgCDisLblOut\")(cond12)\n",
    "\n",
    "    # intermediate LSTM layer for text\n",
    "    cond14 = LSTM(int(lbl_lstm/2), \n",
    "                activation=lbl_ly_actf, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=lbl_rs, \n",
    "                name=\"TxtCDisLblLSTM_14\")(cond6t)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit for img\n",
    "    cond15 = BatchNormalization(name=\"TxtCDisLblBN_15\")(cond14)\n",
    "    cond16 = Dropout(hid_ldrop, name=\"TxtCDisLblDrop_16\")(cond15)\n",
    "\n",
    "    # intermediate LSTM layer for text\n",
    "    cond17 = LSTM(lbl_lstm, \n",
    "                activation=lbl_ly_actf, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=lbl_rs, \n",
    "                name=\"TxtCDisLblLSTM_17\")(cond16)\n",
    "\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_img = Input(shape=img_shape, name=\"CDisImgIn\")\n",
    "\n",
    "    concat_img = Concatenate(axis=-1, name=\"ImgCDisConcat_19\")([in_img, cond13])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # intermediate conv layer\n",
    "    lyr1 = Conv2D(in_filters, kernel_size=in_ksize, \n",
    "                    padding=in_pad, activation=in_lyr_act, \n",
    "                    strides=in_stsize, name=\"ImgCDisConv2D_20\")(concat_img)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr2 = Conv2D(int(filters/2), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_21\")(lyr1)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr3 = BatchNormalization(name=\"ImgCDisBN_22\")(lyr2)\n",
    "    lyr4 = Dropout(hid_ldrop, name=\"ImgCDisDrop_23\")(lyr3)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr5 = Conv2D(int(filters/4), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_24\")(lyr4)\n",
    "\n",
    "    # intermediate conv layer\n",
    "    lyr6 = Conv2D(int(filters/8), kernel_size=ksize, \n",
    "                    padding=pad, activation=hid_lyr_act, \n",
    "                    strides=stsize, name=\"ImgCDisConv2D_25\")(lyr5)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr7 = BatchNormalization(name=\"ImgCDisBN_26\")(lyr6)\n",
    "    lyr8 = Dropout(hid_ldrop, name=\"ImgCDisDrop_27\")(lyr7)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr9 = Flatten(name=\"ImgCDisFlat_28\")(lyr8)\n",
    "\n",
    "    #TXT DISCRIMINATOR\n",
    "    # LAYER CREATION\n",
    "    # input layer\n",
    "    in_txt = Input(shape=txt_shape, name=\"CDisTxtIn\")\n",
    "\n",
    "    # concat txt input with labels conditional\n",
    "    concat_txt = Concatenate(axis=-1, name=\"TxtCDisConcat_30\")([in_txt, cond17])\n",
    "\n",
    "    # DISCRIMINATOR LAYERS\n",
    "    # masking input text\n",
    "    lyr10 = Masking(mask_value=mval, input_shape=txt_shape, \n",
    "                    name = \"TxtCDisMask_31\")(concat_txt) # concat1\n",
    "\n",
    "    # input LSTM layer\n",
    "    lyr11 = LSTM(in_lstm, activation=in_lyr_act, \n",
    "                    input_shape=txt_shape, \n",
    "                    return_sequences=in_rs, \n",
    "                    name=\"TxtCDisLSTM_32\")(lyr10)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr12 = BatchNormalization(name=\"TxtCDisBN_33\")(lyr11)\n",
    "    lyr13 = Dropout(hid_ldrop, name=\"TxCtDisDrop_34\")(lyr12)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr14 = LSTM(int(lstm_units/2), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtCDisLSTM_35\")(lyr13)\n",
    "\n",
    "    # intermediate LSTM layer\n",
    "    lyr15 = LSTM(int(lstm_units/4), \n",
    "                activation=hid_lyr_act, \n",
    "                input_shape=mem_shape, \n",
    "                return_sequences=rs, \n",
    "                name=\"TxtCDisLSTM_36\")(lyr14)\n",
    "\n",
    "    # batch normalization + drop layers to avoid overfit\n",
    "    lyr16 = BatchNormalization(name=\"TxtCDisBN_37\")(lyr15)\n",
    "    lyr17 = Dropout(hid_ldrop, name=\"TxtCDisDrop_38\")(lyr16)\n",
    "\n",
    "    # flatten from 2D to 1D\n",
    "    lyr18 = Flatten(name=\"TxtCDisFlat_39\")(lyr17)\n",
    "\n",
    "    # concat img encoding + txt encoding\n",
    "    concat_encoding = Concatenate(axis=-1, name=\"MultiCDisConcat_40\")([lyr18, lyr9])\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr19 = Dense(int(mid_disn), activation=hid_cls_act, name=\"MultiCDisDense_41\")(concat_encoding)\n",
    "    lyr20 = Dense(int(mid_disn/2), activation=hid_cls_act, name=\"MultiCDisDense_42\")(lyr19)\n",
    "    # drop layer\n",
    "    lyr21 = Dropout(hid_ldrop, name=\"MultiCDisDrop_43\")(lyr20)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr22 = Dense(int(mid_disn/4), activation=hid_cls_act, name=\"MultiCDisDense_44\")(lyr21)\n",
    "    lyr23 = Dense(int(mid_disn/8), activation=hid_cls_act, name=\"MultiCDisDense_45\")(lyr22)\n",
    "    # drop layer\n",
    "    lyr24 = Dropout(hid_ldrop, name=\"MultiCDisDrop_46\")(lyr23)\n",
    "\n",
    "    # dense classifier layers\n",
    "    lyr25 = Dense(int(mid_disn/16), activation=hid_cls_act, name=\"MultiCDisDense_47\")(lyr24)\n",
    "    lyr26 = Dense(int(mid_disn/32), activation=hid_cls_act, name=\"MultiCDisDense_48\")(lyr25)\n",
    "\n",
    "    # output layer\n",
    "    out_cls = Dense(out_nsize, activation=out_lyr_act, name=\"MultiCDisOut\")(lyr26)\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=[in_img, in_txt, in_labels], outputs=out_cls)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cgan(gen_model, dis_model, gan_cfg):\n",
    "\n",
    "    # getting GAN Config\n",
    "    ls = gan_cfg.get(\"loss\")\n",
    "    opt = gan_cfg.get(\"optimizer\")\n",
    "    met = gan_cfg.get(\"metrics\")\n",
    "\n",
    "    # make weights in the discriminator not trainable\n",
    "    dis_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_labels = gen_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_img, gen_txt = gen_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = dis_model([gen_img, gen_txt, gen_labels])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    gan_model = Model([gen_noise, gen_labels], gan_output)\n",
    "    # compile model\n",
    "    gan_model.compile(loss=ls, optimizer=opt, metrics=met)\n",
    "    # cgan_model.compile(loss=gan_cfg[0], optimizer=gan_cfg[1])#, metrics=gan_cfg[2])\n",
    "    return gan_model"
   ]
  },
  {
   "source": [
    "## ML Models Configuration\n",
    "### GAN-img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Generator Config:\n {'mask_value': 0.0, 'return_sequences': True, 'lstm_neurons': 400, 'latent_img_size': 7500, 'input_lyr_activation': 'relu', 'latent_img_shape': (50, 50, 3), 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B987267CD0>, 'gen_dropout_rate': 0.3, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (400, 400, 3), 'output_lyr_activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "# img generator config\n",
    "img_gen_cfg = {\n",
    "    \"mask_value\": 0.0,\n",
    "    \"return_sequences\": True,\n",
    "    \"lstm_neurons\": 400,\n",
    "    \"latent_img_size\": 50*50*3,\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"latent_img_shape\": (50,50,3),\n",
    "    \"filters\": 64, \n",
    "    \"kernel_size\": (4,4),\n",
    "    \"stride\": (2,2),\n",
    "    \"padding\": \"same\",\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"gen_dropout_rate\": 0.3,\n",
    "    \"output_filters\": img_og_shape[2],\n",
    "    \"output_kernel_size\": (3,3),\n",
    "    \"output_stride\": (1,1),\n",
    "    \"output_padding\": \"same\",\n",
    "    \"output_shape\": X_img[0].shape,\n",
    "    \"output_lyr_activation\": \"tanh\",\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Generator Config:\\n\", img_gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Discriminator Config:\n {'input_lyr_activation': 'relu', 'input_filters': 64, 'input_kernel_size': (4, 4), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B987918D00>, 'dis_dropout_rate': 0.2, 'mid_dis_neurons': 5000, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B987918280>, 'output_dis_neurons': 1, 'output_lyr_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B9879183D0>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# img discriminator config\n",
    "img_dis_cfg = {\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"input_filters\": 64,\n",
    "    \"input_kernel_size\": (4,4),\n",
    "    \"input_stride\": (2,2),\n",
    "    \"input_padding\": \"same\",\n",
    "    \"filters\": 64,\n",
    "    \"kernel_size\": (4,4),\n",
    "    \"stride\": (2,2),\n",
    "    \"padding\": \"same\",\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"dis_dropout_rate\": 0.2,\n",
    "    \"mid_dis_neurons\": 50*50*2,\n",
    "    \"dense_cls_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"output_dis_neurons\": 1,\n",
    "    \"output_lyr_activation\": \"sigmoid\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.0004, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Discriminator Config:\\n\", img_dis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B9860FE820>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# img GAN config\n",
    "gan_cfg = {\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"GAN-img Config:\\n\", gan_cfg)"
   ]
  },
  {
   "source": [
    "### GAN-txt definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Generator Config:\n {'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'input_lyr_activation': 'relu', 'mid_gen_neurons': 2130, 'lstm_neurons': 400, 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9E760>, 'hidden_return_sequences': True, 'gen_dropout_rate': 0.3, 'latent_lstm_reshape': (15, 142), 'memory_shape': (15, 142), 'output_neurons': 142, 'output_shape': (15, 142), 'output_lyr_activation': 'tanh', 'output_return_sequences': True}\n"
     ]
    }
   ],
   "source": [
    "# txt generator config\n",
    "txt_gen_cfg = {\n",
    "    \"mask_value\": 0.0,\n",
    "    \"input_return_sequences\": True,\n",
    "    \"input_lstm_neurons\": 400,\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"mid_gen_neurons\": timesteps*X_txt.shape[2],\n",
    "    \"lstm_neurons\": 400,\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"hidden_return_sequences\": True,\n",
    "    \"gen_dropout_rate\": 0.3,\n",
    "    \"latent_lstm_reshape\": X_txt[0].shape,\n",
    "    \"memory_shape\": X_txt[0].shape,\n",
    "    \"output_neurons\": X_txt.shape[2],\n",
    "    \"output_shape\": X_txt[0].shape,\n",
    "    \"output_lyr_activation\": \"tanh\",\n",
    "    \"output_return_sequences\": True,\n",
    "    }\n",
    "\n",
    "print(\"GAN-txt Generator Config:\\n\", txt_gen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Discriminator Config:\n {'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'input_lyr_activation': 'relu', 'lstm_neurons': 400, 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9878162E0>, 'hidden_return_sequences': True, 'memory_shape': (15, 142), 'dis_dropout_rate': 0.2, 'mid_dis_neurons': 2130, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B987816610>, 'output_dis_neurons': 1, 'output_lyr_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B9878167C0>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt discriminator config\n",
    "txt_dis_cfg = {\n",
    "    \"mask_value\": 0.0,\n",
    "    \"input_return_sequences\": True,\n",
    "    \"input_lstm_neurons\": 400,\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"lstm_neurons\": 400,\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"hidden_return_sequences\": True,\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"memory_shape\": X_txt[0].shape,\n",
    "    \"dis_dropout_rate\": 0.2,\n",
    "    \"mid_dis_neurons\": timesteps*X_txt.shape[2],\n",
    "    \"dense_cls_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"output_dis_neurons\": 1,\n",
    "    \"output_lyr_activation\": \"sigmoid\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.0004, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"GAN-txt Discriminator Config:\\n\", txt_dis_cfg)"
   ]
  },
  {
   "source": [
    "### CGAN-img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Generator Config:\n {'mask_value': 0.0, 'return_sequences': True, 'lstm_neurons': 400, 'latent_img_size': 7500, 'input_lyr_activation': 'relu', 'latent_img_shape': (50, 50, 3), 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9E100>, 'gen_dropout_rate': 0.3, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (400, 400, 3), 'output_lyr_activation': 'tanh', 'labels_neurons': 2130, 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9879185B0>}\n"
     ]
    }
   ],
   "source": [
    "img_cgen_cfg = {\n",
    "    \"mask_value\": 0.0,\n",
    "    \"return_sequences\": True,\n",
    "    \"lstm_neurons\": 400,\n",
    "    \"latent_img_size\": 50*50*3,\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"latent_img_shape\": (50,50,3),\n",
    "    \"filters\": 64, \n",
    "    \"kernel_size\": (4,4),\n",
    "    \"stride\": (2,2),\n",
    "    \"padding\": \"same\",\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"gen_dropout_rate\": 0.3,\n",
    "    \"output_filters\": img_og_shape[2],\n",
    "    \"output_kernel_size\": (3,3),\n",
    "    \"output_stride\": (1,1),\n",
    "    \"output_padding\": \"same\",\n",
    "    \"output_shape\": X_img[0].shape,\n",
    "    \"output_lyr_activation\": \"tanh\",\n",
    "    \"labels_neurons\": timesteps*X_txt.shape[2],\n",
    "    \"labels_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Generator Config:\\n\", img_cgen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Generator Config:\n {'input_lyr_activation': 'relu', 'input_filters': 64, 'input_kernel_size': (4, 4), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9860FE9D0>, 'dis_dropout_rate': 0.2, 'mid_dis_neurons': 5000, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9860FEFD0>, 'output_dis_neurons': 1, 'output_lyr_activation': 'sigmoid', 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9EA60>, 'timesteps': 15, 'max_features': 142, 'labels_neurons': 7500, 'labels_filters': 64, 'labels_kernel_size': (4, 4), 'labels_stride': (2, 2), 'labels_reshape': (50, 50, 3), 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B959C9E610>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "img_cdis_cfg = {\n",
    "    \"input_lyr_activation\": \"relu\",\n",
    "    \"input_filters\": 64,\n",
    "    \"input_kernel_size\": (4,4),\n",
    "    \"input_stride\": (2,2),\n",
    "    \"input_padding\": \"same\",\n",
    "    \"filters\": 64,\n",
    "    \"kernel_size\": (4,4),\n",
    "    \"stride\": (2,2),\n",
    "    \"padding\": \"same\",\n",
    "    \"hidden_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"dis_dropout_rate\": 0.2,\n",
    "    \"mid_dis_neurons\": 50*50*2,\n",
    "    \"dense_cls_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"output_dis_neurons\": 1,\n",
    "    \"output_lyr_activation\": \"sigmoid\",\n",
    "    \"labels_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"timesteps\": timesteps,\n",
    "    \"max_features\": X_txt.shape[2],\n",
    "    \"labels_neurons\": 50*50*3,\n",
    "    \"labels_lyr_activation\": LeakyReLU(alpha=0.2),\n",
    "    \"labels_filters\": 64,\n",
    "    \"labels_kernel_size\": (4,4),\n",
    "    \"labels_stride\": (2,2),\n",
    "    \"labels_reshape\": (50,50,3),\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.0004, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Generator Config:\\n\", img_cdis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B959C9EE50>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt GAN config\n",
    "img_cgan_cfg = {\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"CGAN-img Config:\\n\", img_cgan_cfg)"
   ]
  },
  {
   "source": [
    "### CGAN-txt2img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGen-txt2img Config:\n {'mask_value': 0.0, 'return_sequences': True, 'lstm_neurons': 400, 'latent_img_size': 7500, 'input_lyr_activation': 'relu', 'latent_img_shape': (50, 50, 3), 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9E760>, 'gen_dropout_rate': 0.3, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (15, 142), 'output_lyr_activation': 'tanh', 'labels_neurons': 2130, 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9879185B0>, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'mid_gen_neurons': 2130, 'hidden_return_sequences': True, 'latent_lstm_reshape': (15, 142), 'memory_shape': (15, 142), 'output_neurons': 142, 'output_return_sequences': True}\n"
     ]
    }
   ],
   "source": [
    "multi_cgen_cfg = dict()\n",
    "multi_cgen_cfg.update(img_cgen_cfg)\n",
    "multi_cgen_cfg.update(txt_gen_cfg)\n",
    "\n",
    "print(\"Multi CGen-txt2img Config:\\n\", multi_cgen_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CDis-txt2img Config:\n {'input_lyr_activation': 'relu', 'input_filters': 64, 'input_kernel_size': (4, 4), 'input_stride': (2, 2), 'input_padding': 'same', 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9878162E0>, 'dis_dropout_rate': 0.2, 'mid_dis_neurons': 2130, 'dense_cls_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B987816610>, 'output_dis_neurons': 1, 'output_lyr_activation': 'sigmoid', 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9EA60>, 'timesteps': 15, 'max_features': 142, 'labels_neurons': 7500, 'labels_filters': 64, 'labels_kernel_size': (4, 4), 'labels_stride': (2, 2), 'labels_reshape': (50, 50, 3), 'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B9878167C0>, 'metrics': ['accuracy'], 'mask_value': 0.0, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'lstm_neurons': 400, 'hidden_return_sequences': True, 'memory_shape': (15, 142), 'labels_lstm_neurons': 400, 'labels_return_sequences': True, 'labels_img_reshape': (50, 50, 3), 'labels_txt_reshape': (15, 142)}\n"
     ]
    }
   ],
   "source": [
    "multi_cdis_cfg = dict()\n",
    "multi_cdis_cfg.update(img_cdis_cfg)\n",
    "multi_cdis_cfg.update(txt_dis_cfg)\n",
    "\n",
    "mcdis_cfg_update = {\n",
    "    \"labels_lstm_neurons\": 400,\n",
    "    \"labels_return_sequences\": True,\n",
    "    \"labels_img_reshape\": (50,50,3),\n",
    "    \"labels_txt_reshape\": X_txt[0].shape,\n",
    "    }\n",
    "\n",
    "multi_cdis_cfg.update(mcdis_cfg_update)\n",
    "\n",
    "print(\"Multi CDis-txt2img Config:\\n\", multi_cdis_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Config:\n {'loss': 'binary_crossentropy', 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001B9DDC11D30>, 'metrics': ['accuracy']}\n"
     ]
    }
   ],
   "source": [
    "# txt2img CGAN config\n",
    "multi_cgan_cfg = {\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": Adam(learning_rate=0.00015, beta_1=0.5),\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"Multi CGAN-txt2img Config:\\n\", multi_cgan_cfg)"
   ]
  },
  {
   "source": [
    "## ML Model Creation\n",
    "### GAN img definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Generator Definition\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nImgGenIn (InputLayer)        [(None, 15, 142)]         0         \n_________________________________________________________________\nImgGenMask_1 (Masking)       (None, 15, 142)           0         \n_________________________________________________________________\nImgGenLSTM_2 (LSTM)          (None, 15, 400)           868800    \n_________________________________________________________________\nImgGenFlat_3 (Flatten)       (None, 6000)              0         \n_________________________________________________________________\nImgGenDense_4 (Dense)        (None, 7500)              45007500  \n_________________________________________________________________\nImgGenReshape_5 (Reshape)    (None, 50, 50, 3)         0         \n_________________________________________________________________\nImgGenConv2D_6 (Conv2DTransp (None, 100, 100, 8)       392       \n_________________________________________________________________\nImgGenBN_7 (BatchNormalizati (None, 100, 100, 8)       32        \n_________________________________________________________________\nImgGenDrop_8 (Dropout)       (None, 100, 100, 8)       0         \n_________________________________________________________________\nImgGenConv2D_9 (Conv2DTransp (None, 200, 200, 16)      2064      \n_________________________________________________________________\nImgGenConv2D_10 (Conv2DTrans (None, 200, 200, 32)      8224      \n_________________________________________________________________\nImgGenBN_11 (BatchNormalizat (None, 200, 200, 32)      128       \n_________________________________________________________________\nImgGenDrop_12 (Dropout)      (None, 200, 200, 32)      0         \n_________________________________________________________________\nImgGenConv2D_13 (Conv2DTrans (None, 400, 400, 64)      32832     \n_________________________________________________________________\nImgGenOut (Conv2D)           (None, 400, 400, 3)       1731      \n=================================================================\nTotal params: 45,921,703\nTrainable params: 45,921,623\nNon-trainable params: 80\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_shape = X_txt[0].shape\n",
    "gen_model = create_img_generator(latent_shape, img_gen_cfg)\n",
    "print(\"GAN-img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "gen_model.model_name = \"GAN-img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Discriminator Definition\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nDisImgIn (InputLayer)        [(None, 400, 400, 3)]     0         \n_________________________________________________________________\nImgDisConv2D_1 (Conv2D)      (None, 200, 200, 64)      3136      \n_________________________________________________________________\nImgDisConv2D_2 (Conv2D)      (None, 100, 100, 32)      32800     \n_________________________________________________________________\nImgDisBN_3 (BatchNormalizati (None, 100, 100, 32)      128       \n_________________________________________________________________\nImgDisDrop_4 (Dropout)       (None, 100, 100, 32)      0         \n_________________________________________________________________\nImgDisConv2D_4 (Conv2D)      (None, 50, 50, 16)        8208      \n_________________________________________________________________\nImgDisConv2D_5 (Conv2D)      (None, 25, 25, 8)         2056      \n_________________________________________________________________\nImgDisBN_6 (BatchNormalizati (None, 25, 25, 8)         32        \n_________________________________________________________________\nImgDisDrop_7 (Dropout)       (None, 25, 25, 8)         0         \n_________________________________________________________________\nImgDisFlat_8 (Flatten)       (None, 5000)              0         \n_________________________________________________________________\nImgDisDense_9 (Dense)        (None, 5000)              25005000  \n_________________________________________________________________\nImgDisDense_10 (Dense)       (None, 2500)              12502500  \n_________________________________________________________________\nImgDisDrop_11 (Dropout)      (None, 2500)              0         \n_________________________________________________________________\nImgDisDense_12 (Dense)       (None, 1250)              3126250   \n_________________________________________________________________\nImgDisDense_13 (Dense)       (None, 625)               781875    \n_________________________________________________________________\nImgDisDrop_14 (Dropout)      (None, 625)               0         \n_________________________________________________________________\nImgDisDense_15 (Dense)       (None, 312)               195312    \n_________________________________________________________________\nImgDisDense_16 (Dense)       (None, 156)               48828     \n_________________________________________________________________\nImgDisOut (Dense)            (None, 1)                 157       \n=================================================================\nTotal params: 41,706,282\nTrainable params: 41,706,202\nNon-trainable params: 80\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape = X_img[0].shape\n",
    "dis_model = create_img_discriminator(img_shape, img_dis_cfg)\n",
    "print(\"GAN-img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "dis_model.model_name = \"GAN-img Discriminator\"\n",
    "\n",
    "# compile model\n",
    "dis_model.compile(loss=img_dis_cfg[\"loss\"], \n",
    "                    optimizer=img_dis_cfg[\"optimizer\"], \n",
    "                    metrics=img_dis_cfg[\"metrics\"])\n",
    "\n",
    "# cdis_model.trainable = False\n",
    "dis_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-img Model definition\nModel: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nImgGenIn (InputLayer)        [(None, 15, 142)]         0         \n_________________________________________________________________\nImgGenMask_1 (Masking)       (None, 15, 142)           0         \n_________________________________________________________________\nImgGenLSTM_2 (LSTM)          (None, 15, 400)           868800    \n_________________________________________________________________\nImgGenFlat_3 (Flatten)       (None, 6000)              0         \n_________________________________________________________________\nImgGenDense_4 (Dense)        (None, 7500)              45007500  \n_________________________________________________________________\nImgGenReshape_5 (Reshape)    (None, 50, 50, 3)         0         \n_________________________________________________________________\nImgGenConv2D_6 (Conv2DTransp (None, 100, 100, 8)       392       \n_________________________________________________________________\nImgGenBN_7 (BatchNormalizati (None, 100, 100, 8)       32        \n_________________________________________________________________\nImgGenDrop_8 (Dropout)       (None, 100, 100, 8)       0         \n_________________________________________________________________\nImgGenConv2D_9 (Conv2DTransp (None, 200, 200, 16)      2064      \n_________________________________________________________________\nImgGenConv2D_10 (Conv2DTrans (None, 200, 200, 32)      8224      \n_________________________________________________________________\nImgGenBN_11 (BatchNormalizat (None, 200, 200, 32)      128       \n_________________________________________________________________\nImgGenDrop_12 (Dropout)      (None, 200, 200, 32)      0         \n_________________________________________________________________\nImgGenConv2D_13 (Conv2DTrans (None, 400, 400, 64)      32832     \n_________________________________________________________________\nImgGenOut (Conv2D)           (None, 400, 400, 3)       1731      \n_________________________________________________________________\nmodel_1 (Functional)         (None, 1)                 41706282  \n=================================================================\nTotal params: 87,627,985\nTrainable params: 45,921,623\nNon-trainable params: 41,706,362\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"GAN-img Model definition\")\n",
    "gan_model = create_img_gan(gen_model, dis_model, gan_cfg)\n",
    "gan_model.model_name = \"GAN-img\"\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-03 08:27:26\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(gen_model, model_fn_path, gen_model.model_name, timestamp)\n",
    "export_model(dis_model, model_fn_path, dis_model.model_name, timestamp)\n",
    "export_model(gan_model, model_fn_path, gan_model.model_name, timestamp)"
   ]
  },
  {
   "source": [
    "### GAN txt definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Generator Definition\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTxtGenIn (InputLayer)        [(None, 15, 142)]         0         \n_________________________________________________________________\nTxtGenMask_1 (Masking)       (None, 15, 142)           0         \n_________________________________________________________________\nTxtGenLSTM_2 (LSTM)          (None, 15, 400)           868800    \n_________________________________________________________________\nTxtGenBN_3 (BatchNormalizati (None, 15, 400)           1600      \n_________________________________________________________________\nTxtGenDrop_4 (Dropout)       (None, 15, 400)           0         \n_________________________________________________________________\nTxtGenFlat_5 (Flatten)       (None, 6000)              0         \n_________________________________________________________________\nTxtGenDense_6 (Dense)        (None, 2130)              12782130  \n_________________________________________________________________\nTxtGenReshape_7 (Reshape)    (None, 15, 142)           0         \n_________________________________________________________________\nTxtGenBN_8 (BatchNormalizati (None, 15, 142)           568       \n_________________________________________________________________\nTxtGenDrop_9 (Dropout)       (None, 15, 142)           0         \n_________________________________________________________________\nTxtGenLSTM_10 (LSTM)         (None, 15, 100)           97200     \n_________________________________________________________________\nTxtGenLSTM_11 (LSTM)         (None, 15, 200)           240800    \n_________________________________________________________________\nTxtGenBN_12 (BatchNormalizat (None, 15, 200)           800       \n_________________________________________________________________\nTxtGenDrop_13 (Dropout)      (None, 15, 200)           0         \n_________________________________________________________________\nTxtGenDrop_14 (LSTM)         (None, 15, 400)           961600    \n_________________________________________________________________\nGenTxtOut (TimeDistributed)  (None, 15, 142)           56942     \n=================================================================\nTotal params: 15,010,440\nTrainable params: 15,008,956\nNon-trainable params: 1,484\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_txt_model = create_txt_generator(latent_shape, txt_gen_cfg)\n",
    "print(\"GAN-txt Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "gen_txt_model.model_name = \"GAN-txt Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "gen_txt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15, 142)\n",
      "GAN-txt Discriminator Definition\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "DisTxtIn (InputLayer)        [(None, 15, 142)]         0         \n",
      "_________________________________________________________________\n",
      "TxtDisMask_1 (Masking)       (None, 15, 142)           0         \n",
      "_________________________________________________________________\n",
      "TxtDisLSTM_2 (LSTM)          (None, 15, 400)           868800    \n",
      "_________________________________________________________________\n",
      "TxtDisBN_3 (BatchNormalizati (None, 15, 400)           1600      \n",
      "_________________________________________________________________\n",
      "TxtDisDrop_4 (Dropout)       (None, 15, 400)           0         \n",
      "_________________________________________________________________\n",
      "TxtDisLSTM_5 (LSTM)          (None, 15, 200)           480800    \n",
      "_________________________________________________________________\n",
      "TxtDisLSTM_6 (LSTM)          (None, 15, 100)           120400    \n",
      "_________________________________________________________________\n",
      "TxtDisBN_7 (BatchNormalizati (None, 15, 100)           400       \n",
      "_________________________________________________________________\n",
      "TxtDisDrop_8 (Dropout)       (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "TxtDisFlat_9 (Flatten)       (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "TxtDisDense_10 (Dense)       (None, 2130)              3197130   \n",
      "_________________________________________________________________\n",
      "TxtDisDense_11 (Dense)       (None, 1065)              2269515   \n",
      "_________________________________________________________________\n",
      "TxtDisDrop_12 (Dropout)      (None, 1065)              0         \n",
      "_________________________________________________________________\n",
      "TxtDisDense_13 (Dense)       (None, 532)               567112    \n",
      "_________________________________________________________________\n",
      "TxtDisDense_14 (Dense)       (None, 266)               141778    \n",
      "_________________________________________________________________\n",
      "TxtDisDrop_15 (Dropout)      (None, 266)               0         \n",
      "_________________________________________________________________\n",
      "TxtDisDense_16 (Dense)       (None, 133)               35511     \n",
      "_________________________________________________________________\n",
      "TxtDisDense_17 (Dense)       (None, 66)                8844      \n",
      "_________________________________________________________________\n",
      "TxtDisOut (Dense)            (None, 1)                 67        \n",
      "=================================================================\n",
      "Total params: 7,691,957\n",
      "Trainable params: 7,690,957\n",
      "Non-trainable params: 1,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "txt_shape = X_txt[0].shape\n",
    "print(txt_shape)\n",
    "dis_txt_model = create_txt_discriminator(txt_shape, txt_dis_cfg)\n",
    "print(\"GAN-txt Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "dis_txt_model.model_name = \"GAN-txt Discriminator\"\n",
    "\n",
    "# compile model\n",
    "dis_txt_model.compile(loss=txt_dis_cfg[\"loss\"], \n",
    "                    optimizer=txt_dis_cfg[\"optimizer\"], \n",
    "                    metrics=txt_dis_cfg[\"metrics\"])\n",
    "\n",
    "# cdis_model.trainable = False\n",
    "dis_txt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GAN-txt Model definition\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "TxtGenIn (InputLayer)        [(None, 15, 142)]         0         \n",
      "_________________________________________________________________\n",
      "TxtGenMask_1 (Masking)       (None, 15, 142)           0         \n",
      "_________________________________________________________________\n",
      "TxtGenLSTM_2 (LSTM)          (None, 15, 400)           868800    \n",
      "_________________________________________________________________\n",
      "TxtGenBN_3 (BatchNormalizati (None, 15, 400)           1600      \n",
      "_________________________________________________________________\n",
      "TxtGenDrop_4 (Dropout)       (None, 15, 400)           0         \n",
      "_________________________________________________________________\n",
      "TxtGenFlat_5 (Flatten)       (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "TxtGenDense_6 (Dense)        (None, 2130)              12782130  \n",
      "_________________________________________________________________\n",
      "TxtGenReshape_7 (Reshape)    (None, 15, 142)           0         \n",
      "_________________________________________________________________\n",
      "TxtGenBN_8 (BatchNormalizati (None, 15, 142)           568       \n",
      "_________________________________________________________________\n",
      "TxtGenDrop_9 (Dropout)       (None, 15, 142)           0         \n",
      "_________________________________________________________________\n",
      "TxtGenLSTM_10 (LSTM)         (None, 15, 100)           97200     \n",
      "_________________________________________________________________\n",
      "TxtGenLSTM_11 (LSTM)         (None, 15, 200)           240800    \n",
      "_________________________________________________________________\n",
      "TxtGenBN_12 (BatchNormalizat (None, 15, 200)           800       \n",
      "_________________________________________________________________\n",
      "TxtGenDrop_13 (Dropout)      (None, 15, 200)           0         \n",
      "_________________________________________________________________\n",
      "TxtGenDrop_14 (LSTM)         (None, 15, 400)           961600    \n",
      "_________________________________________________________________\n",
      "GenTxtOut (TimeDistributed)  (None, 15, 142)           56942     \n",
      "_________________________________________________________________\n",
      "model_4 (Functional)         (None, 1)                 7691957   \n",
      "=================================================================\n",
      "Total params: 22,702,397\n",
      "Trainable params: 15,008,956\n",
      "Non-trainable params: 7,693,441\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"GAN-txt Model definition\")\n",
    "gan_txt_model = create_img_gan(gen_txt_model, dis_txt_model, gan_cfg)\n",
    "gan_txt_model.summary()\n",
    "gan_txt_model.model_name = \"GAN-txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-03 08:27:26\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(gen_txt_model, model_fn_path, gen_txt_model.model_name, timestamp)\n",
    "export_model(dis_txt_model, model_fn_path, dis_txt_model.model_name, timestamp)\n",
    "export_model(gan_txt_model, model_fn_path, gan_txt_model.model_name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n",
      "CGAN-img Generator Definition\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgCGenLblIn (InputLayer)       [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblEmb_1 (Embedding)     (None, 16, 142)      2130        ImgCGenLblIn[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblFlat_2 (Flatten)      (None, 2272)         0           ImgCGenLblEmb_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDense_3 (Dense)       (None, 1065)         2420745     ImgCGenLblFlat_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblBN_4 (BatchNormalizat (None, 1065)         4260        ImgCGenLblDense_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDrop_5 (Dropout)      (None, 1065)         0           ImgCGenLblBN_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblDense_6 (Dense)       (None, 2130)         2270580     ImgCGenLblDrop_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenIn (InputLayer)          [(None, 15, 142)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLblOut (Reshape)         (None, 15, 142)      0           ImgCGenLblDense_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConcat (Concatenate)     (None, 15, 284)      0           ImgCGenIn[0][0]                  \n",
      "                                                                 ImgCGenLblOut[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenMask_1 (Masking)         (None, 15, 284)      0           ImgCGenConcat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenLSTM_2 (LSTM)            (None, 15, 400)      1096000     ImgCGenMask_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenFlat_3 (Flatten)         (None, 6000)         0           ImgCGenLSTM_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDense_4 (Dense)          (None, 7500)         45007500    ImgCGenFlat_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenReshape_5 (Reshape)      (None, 50, 50, 3)    0           ImgCGenDense_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_6 (Conv2DTranspos (None, 100, 100, 8)  392         ImgCGenReshape_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_7 (BatchNormalization (None, 100, 100, 8)  32          ImgCGenConv2D_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_8 (Dropout)         (None, 100, 100, 8)  0           ImgCGenBN_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_9 (Conv2DTranspos (None, 200, 200, 16) 2064        ImgCGenDrop_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_10 (Conv2DTranspo (None, 200, 200, 32) 8224        ImgCGenConv2D_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_11 (BatchNormalizatio (None, 200, 200, 32) 128         ImgCGenConv2D_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_12 (Dropout)        (None, 200, 200, 32) 0           ImgCGenBN_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_13 (Conv2DTranspo (None, 400, 400, 64) 32832       ImgCGenDrop_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenOut (Conv2D)             (None, 400, 400, 3)  1731        ImgCGenConv2D_13[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 50,846,618\n",
      "Trainable params: 50,844,408\n",
      "Non-trainable params: 2,210\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_labels = y_labels[0].shape[0]\n",
    "print(n_labels)\n",
    "cgen_img_model = create_img_cgenerator(latent_shape, n_labels, img_cgen_cfg)\n",
    "print(\"CGAN-img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "cgen_img_model.model_name = \"CGAN-img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "cgen_img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Discriminator Definition\nModel: \"model_7\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nImgCDisLblIn (InputLayer)       [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nImgCDisLblEmb_1 (Embedding)     (None, 16, 142)      2130        ImgCDisLblIn[0][0]               \n__________________________________________________________________________________________________\nImgCDisLblFlat_2 (Flatten)      (None, 2272)         0           ImgCDisLblEmb_1[0][0]            \n__________________________________________________________________________________________________\nImgCDisLblDense_3 (Dense)       (None, 7500)         17047500    ImgCDisLblFlat_2[0][0]           \n__________________________________________________________________________________________________\nImgCDisLblReshape_4 (Reshape)   (None, 50, 50, 3)    0           ImgCDisLblDense_3[0][0]          \n__________________________________________________________________________________________________\nImgCDisLblConv2D_5 (Conv2DTrans (None, 100, 100, 8)  392         ImgCDisLblReshape_4[0][0]        \n__________________________________________________________________________________________________\nImgCDisLblBN_6 (BatchNormalizat (None, 100, 100, 8)  32          ImgCDisLblConv2D_5[0][0]         \n__________________________________________________________________________________________________\nImgCDisLblDrop_7 (Dropout)      (None, 100, 100, 8)  0           ImgCDisLblBN_6[0][0]             \n__________________________________________________________________________________________________\nImgCDisLblConv2D_8 (Conv2DTrans (None, 200, 200, 16) 2064        ImgCDisLblDrop_7[0][0]           \n__________________________________________________________________________________________________\nImgCDisLblBN_9 (BatchNormalizat (None, 200, 200, 16) 64          ImgCDisLblConv2D_8[0][0]         \n__________________________________________________________________________________________________\nImgCDisLblDrop_10 (Dropout)     (None, 200, 200, 16) 0           ImgCDisLblBN_9[0][0]             \n__________________________________________________________________________________________________\nDisImgIn (InputLayer)           [(None, 400, 400, 3) 0                                            \n__________________________________________________________________________________________________\nImgCDisLblOut (Conv2DTranspose) (None, 400, 400, 3)  771         ImgCDisLblDrop_10[0][0]          \n__________________________________________________________________________________________________\nCDisConcat_1 (Concatenate)      (None, 400, 400, 6)  0           DisImgIn[0][0]                   \n                                                                 ImgCDisLblOut[0][0]              \n__________________________________________________________________________________________________\nImgCDisConv2D_1 (Conv2D)        (None, 200, 200, 64) 6208        CDisConcat_1[0][0]               \n__________________________________________________________________________________________________\nImgCDisConv2D_2 (Conv2D)        (None, 100, 100, 32) 32800       ImgCDisConv2D_1[0][0]            \n__________________________________________________________________________________________________\nImgCDisBN_3 (BatchNormalization (None, 100, 100, 32) 128         ImgCDisConv2D_2[0][0]            \n__________________________________________________________________________________________________\nImgCDisDrop_4 (Dropout)         (None, 100, 100, 32) 0           ImgCDisBN_3[0][0]                \n__________________________________________________________________________________________________\nImgCDisConv2D_4 (Conv2D)        (None, 50, 50, 16)   8208        ImgCDisDrop_4[0][0]              \n__________________________________________________________________________________________________\nImgCDisConv2D_5 (Conv2D)        (None, 25, 25, 8)    2056        ImgCDisConv2D_4[0][0]            \n__________________________________________________________________________________________________\nImgCDisBN_6 (BatchNormalization (None, 25, 25, 8)    32          ImgCDisConv2D_5[0][0]            \n__________________________________________________________________________________________________\nImgCDisDrop_7 (Dropout)         (None, 25, 25, 8)    0           ImgCDisBN_6[0][0]                \n__________________________________________________________________________________________________\nImgCDisFlat_8 (Flatten)         (None, 5000)         0           ImgCDisDrop_7[0][0]              \n__________________________________________________________________________________________________\nImgCDisDense_9 (Dense)          (None, 5000)         25005000    ImgCDisFlat_8[0][0]              \n__________________________________________________________________________________________________\nImgCDisDense_10 (Dense)         (None, 2500)         12502500    ImgCDisDense_9[0][0]             \n__________________________________________________________________________________________________\nImgCDisDrop_11 (Dropout)        (None, 2500)         0           ImgCDisDense_10[0][0]            \n__________________________________________________________________________________________________\nImgCDisDense_12 (Dense)         (None, 1250)         3126250     ImgCDisDrop_11[0][0]             \n__________________________________________________________________________________________________\nImgCDisDense_13 (Dense)         (None, 625)          781875      ImgCDisDense_12[0][0]            \n__________________________________________________________________________________________________\nImgCDisDrop_14 (Dropout)        (None, 625)          0           ImgCDisDense_13[0][0]            \n__________________________________________________________________________________________________\nImgCDisDense_15 (Dense)         (None, 312)          195312      ImgCDisDrop_14[0][0]             \n__________________________________________________________________________________________________\nImgCDisDense_16 (Dense)         (None, 156)          48828       ImgCDisDense_15[0][0]            \n__________________________________________________________________________________________________\nImgCDisOut (Dense)              (None, 1)            157         ImgCDisDense_16[0][0]            \n==================================================================================================\nTotal params: 58,762,307\nTrainable params: 58,762,179\nNon-trainable params: 128\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_shape = X_img[0].shape\n",
    "cdis_img_model = create_img_cdiscriminator(img_shape, n_labels, img_cdis_cfg)\n",
    "print(\"CGAN-img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "cdis_img_model.model_name = \"CGAN-img Discriminator\"\n",
    "\n",
    "# compile model\n",
    "cdis_img_model.compile(loss=img_cdis_cfg[\"loss\"], \n",
    "                    optimizer=img_cdis_cfg[\"optimizer\"], \n",
    "                    metrics=img_cdis_cfg[\"metrics\"])\n",
    "\n",
    "# cdis_model.trainable = False\n",
    "cdis_img_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CGAN-img Model definition\nModel: \"model_8\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nImgCGenLblIn (InputLayer)       [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nImgCGenLblEmb_1 (Embedding)     (None, 16, 142)      2130        ImgCGenLblIn[0][0]               \n__________________________________________________________________________________________________\nImgCGenLblFlat_2 (Flatten)      (None, 2272)         0           ImgCGenLblEmb_1[0][0]            \n__________________________________________________________________________________________________\nImgCGenLblDense_3 (Dense)       (None, 1065)         2420745     ImgCGenLblFlat_2[0][0]           \n__________________________________________________________________________________________________\nImgCGenLblBN_4 (BatchNormalizat (None, 1065)         4260        ImgCGenLblDense_3[0][0]          \n__________________________________________________________________________________________________\nImgCGenLblDrop_5 (Dropout)      (None, 1065)         0           ImgCGenLblBN_4[0][0]             \n__________________________________________________________________________________________________\nImgCGenLblDense_6 (Dense)       (None, 2130)         2270580     ImgCGenLblDrop_5[0][0]           \n__________________________________________________________________________________________________\nImgCGenIn (InputLayer)          [(None, 15, 142)]    0                                            \n__________________________________________________________________________________________________\nImgCGenLblOut (Reshape)         (None, 15, 142)      0           ImgCGenLblDense_6[0][0]          \n__________________________________________________________________________________________________\nImgCGenConcat (Concatenate)     (None, 15, 284)      0           ImgCGenIn[0][0]                  \n                                                                 ImgCGenLblOut[0][0]              \n__________________________________________________________________________________________________\nImgCGenMask_1 (Masking)         (None, 15, 284)      0           ImgCGenConcat[0][0]              \n__________________________________________________________________________________________________\nImgCGenLSTM_2 (LSTM)            (None, 15, 400)      1096000     ImgCGenMask_1[0][0]              \n__________________________________________________________________________________________________\nImgCGenFlat_3 (Flatten)         (None, 6000)         0           ImgCGenLSTM_2[0][0]              \n__________________________________________________________________________________________________\nImgCGenDense_4 (Dense)          (None, 7500)         45007500    ImgCGenFlat_3[0][0]              \n__________________________________________________________________________________________________\nImgCGenReshape_5 (Reshape)      (None, 50, 50, 3)    0           ImgCGenDense_4[0][0]             \n__________________________________________________________________________________________________\nImgCGenConv2D_6 (Conv2DTranspos (None, 100, 100, 8)  392         ImgCGenReshape_5[0][0]           \n__________________________________________________________________________________________________\nImgCGenBN_7 (BatchNormalization (None, 100, 100, 8)  32          ImgCGenConv2D_6[0][0]            \n__________________________________________________________________________________________________\nImgCGenDrop_8 (Dropout)         (None, 100, 100, 8)  0           ImgCGenBN_7[0][0]                \n__________________________________________________________________________________________________\nImgCGenConv2D_9 (Conv2DTranspos (None, 200, 200, 16) 2064        ImgCGenDrop_8[0][0]              \n__________________________________________________________________________________________________\nImgCGenConv2D_10 (Conv2DTranspo (None, 200, 200, 32) 8224        ImgCGenConv2D_9[0][0]            \n__________________________________________________________________________________________________\nImgCGenBN_11 (BatchNormalizatio (None, 200, 200, 32) 128         ImgCGenConv2D_10[0][0]           \n__________________________________________________________________________________________________\nImgCGenDrop_12 (Dropout)        (None, 200, 200, 32) 0           ImgCGenBN_11[0][0]               \n__________________________________________________________________________________________________\nImgCGenConv2D_13 (Conv2DTranspo (None, 400, 400, 64) 32832       ImgCGenDrop_12[0][0]             \n__________________________________________________________________________________________________\nImgCGenOut (Conv2D)             (None, 400, 400, 3)  1731        ImgCGenConv2D_13[0][0]           \n__________________________________________________________________________________________________\nmodel_7 (Functional)            (None, 1)            58762307    ImgCGenOut[0][0]                 \n                                                                 ImgCGenLblIn[0][0]               \n==================================================================================================\nTotal params: 109,608,925\nTrainable params: 50,844,408\nNon-trainable params: 58,764,517\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"CGAN-img Model definition\")\n",
    "cgan_img_model = create_img_cgan(cgen_img_model, cdis_img_model, gan_cfg)\n",
    "cgan_img_model.summary()\n",
    "cgan_img_model.model_name = \"CGAN-img\""
   ]
  },
  {
   "source": [
    "### Multi CGAN-txt2img"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================\n",
      " {'mask_value': 0.0, 'return_sequences': True, 'lstm_neurons': 400, 'latent_img_size': 7500, 'input_lyr_activation': 'relu', 'latent_img_shape': (50, 50, 3), 'filters': 64, 'kernel_size': (4, 4), 'stride': (2, 2), 'padding': 'same', 'hidden_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B959C9E760>, 'gen_dropout_rate': 0.3, 'output_filters': 3, 'output_kernel_size': (3, 3), 'output_stride': (1, 1), 'output_padding': 'same', 'output_shape': (15, 142), 'output_lyr_activation': 'tanh', 'labels_neurons': 2130, 'labels_lyr_activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B9879185B0>, 'input_return_sequences': True, 'input_lstm_neurons': 400, 'mid_gen_neurons': 2130, 'hidden_return_sequences': True, 'latent_lstm_reshape': (15, 142), 'memory_shape': (15, 142), 'output_neurons': 142, 'output_return_sequences': True} =====================\n",
      "Multi CGAN-txt2img Generator Definition\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgTxtCGenLblIn (InputLayer)    [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblEmb_1 (Embedding)  (None, 16, 142)      2130        ImgTxtCGenLblIn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblFlat_2 (Flatten)   (None, 2272)         0           ImgTxtCGenLblEmb_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDense_3 (Dense)    (None, 1065)         2420745     ImgTxtCGenLblFlat_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblBN_4 (BatchNormali (None, 1065)         4260        ImgTxtCGenLblDense_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDrop_5 (Dropout)   (None, 1065)         0           ImgTxtCGenLblBN_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDense_6 (Dense)    (None, 2130)         2270580     ImgTxtCGenLblDrop_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenIn (InputLayer)       [(None, 15, 142)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblOut (Reshape)      (None, 15, 142)      0           ImgTxtCGenLblDense_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenConcat (Concatenate)  (None, 15, 284)      0           ImgTxtCGenIn[0][0]               \n",
      "                                                                 ImgTxtCGenLblOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenMask_1 (Masking)      (None, 15, 284)      0           ImgTxtCGenConcat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLSTM_2 (LSTM)         (None, 15, 400)      1096000     ImgTxtCGenMask_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenFlat_3 (Flatten)      (None, 6000)         0           ImgTxtCGenLSTM_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenDense_4 (Dense)       (None, 7500)         45007500    ImgTxtCGenFlat_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenReshape_5 (Reshape)      (None, 50, 50, 3)    0           ImgTxtCGenDense_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenDense_5 (Dense)       (None, 2130)         12782130    ImgTxtCGenFlat_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_6 (Conv2DTranspos (None, 100, 100, 8)  392         ImgCGenReshape_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenReshape_7 (Reshape)       (None, 15, 142)      0           ImgTxtCGenDense_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_7 (BatchNormalization (None, 100, 100, 8)  32          ImgCGenConv2D_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenBN_8 (BatchNormalization) (None, 15, 142)      568         TxtGenReshape_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_8 (Dropout)         (None, 100, 100, 8)  0           ImgCGenBN_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_9 (Dropout)          (None, 15, 142)      0           TxtGenBN_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_9 (Conv2DTranspos (None, 200, 200, 16) 2064        ImgCGenDrop_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenLSTM_10 (LSTM)            (None, 15, 100)      97200       TxtGenDrop_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_10 (Conv2DTranspo (None, 200, 200, 32) 8224        ImgCGenConv2D_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenLSTM_11 (LSTM)            (None, 15, 200)      240800      TxtGenLSTM_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_11 (BatchNormalizatio (None, 200, 200, 32) 128         ImgCGenConv2D_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenBN_12 (BatchNormalization (None, 15, 200)      800         TxtGenLSTM_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_12 (Dropout)        (None, 200, 200, 32) 0           ImgCGenBN_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_13 (Dropout)         (None, 15, 200)      0           TxtGenBN_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_13 (Conv2DTranspo (None, 400, 400, 64) 32832       ImgCGenDrop_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_14 (LSTM)            (None, 15, 400)      961600      TxtGenDrop_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenOut (Conv2D)             (None, 400, 400, 3)  1731        ImgCGenConv2D_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "GenTxtOut (TimeDistributed)     (None, 15, 142)      56942       TxtGenDrop_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 64,986,658\n",
      "Trainable params: 64,983,764\n",
      "Non-trainable params: 2,894\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multi_cgen_model = create_multi_cgenerator(latent_shape, img_shape, txt_shape, n_labels, multi_cgen_cfg)\n",
    "print(\"Multi CGAN-txt2img Generator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "multi_cgen_model.model_name = \"Multi CGAN-txt2img Generator\"\n",
    "\n",
    "# DONT compile model\n",
    "# cdis_model.trainable = False\n",
    "multi_cgen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Discriminator Definition\nModel: \"model_16\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nMultiCDisLblIn (InputLayer)     [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nMultiCDisLblEmb_1 (Embedding)   (None, 16, 142)      2130        MultiCDisLblIn[0][0]             \n__________________________________________________________________________________________________\nMultiCDisLblFlat_2 (Flatten)    (None, 2272)         0           MultiCDisLblEmb_1[0][0]          \n__________________________________________________________________________________________________\nMultiCDisLblDense_3 (Dense)     (None, 7500)         17047500    MultiCDisLblFlat_2[0][0]         \n__________________________________________________________________________________________________\nImgCDisLblReshape_4 (Reshape)   (None, 50, 50, 3)    0           MultiCDisLblDense_3[0][0]        \n__________________________________________________________________________________________________\nImgCDisLblConv2D_7 (Conv2DTrans (None, 100, 100, 8)  392         ImgCDisLblReshape_4[0][0]        \n__________________________________________________________________________________________________\nMultiCDisLblDense_5 (Dense)     (None, 2130)         4841490     MultiCDisLblFlat_2[0][0]         \n__________________________________________________________________________________________________\nImgCDisLblBN_8 (BatchNormalizat (None, 100, 100, 8)  32          ImgCDisLblConv2D_7[0][0]         \n__________________________________________________________________________________________________\nTxtCDisLblReshape_6 (Reshape)   (None, 15, 142)      0           MultiCDisLblDense_5[0][0]        \n__________________________________________________________________________________________________\nImgCDisLblDrop_9 (Dropout)      (None, 100, 100, 8)  0           ImgCDisLblBN_8[0][0]             \n__________________________________________________________________________________________________\nTxtCDisLblLSTM_14 (LSTM)        (None, 15, 200)      274400      TxtCDisLblReshape_6[0][0]        \n__________________________________________________________________________________________________\nImgCDisLblConv2D_10 (Conv2DTran (None, 200, 200, 16) 2064        ImgCDisLblDrop_9[0][0]           \n__________________________________________________________________________________________________\nTxtCDisLblBN_15 (BatchNormaliza (None, 15, 200)      800         TxtCDisLblLSTM_14[0][0]          \n__________________________________________________________________________________________________\nImgCDisLblBN_11 (BatchNormaliza (None, 200, 200, 16) 64          ImgCDisLblConv2D_10[0][0]        \n__________________________________________________________________________________________________\nTxtCDisLblDrop_16 (Dropout)     (None, 15, 200)      0           TxtCDisLblBN_15[0][0]            \n__________________________________________________________________________________________________\nImgCDisLblDrop_12 (Dropout)     (None, 200, 200, 16) 0           ImgCDisLblBN_11[0][0]            \n__________________________________________________________________________________________________\nCDisTxtIn (InputLayer)          [(None, 15, 142)]    0                                            \n__________________________________________________________________________________________________\nTxtCDisLblLSTM_17 (LSTM)        (None, 15, 400)      961600      TxtCDisLblDrop_16[0][0]          \n__________________________________________________________________________________________________\nCDisImgIn (InputLayer)          [(None, 400, 400, 3) 0                                            \n__________________________________________________________________________________________________\nImgCDisLblOut (Conv2DTranspose) (None, 400, 400, 3)  771         ImgCDisLblDrop_12[0][0]          \n__________________________________________________________________________________________________\nTxtCDisConcat_30 (Concatenate)  (None, 15, 542)      0           CDisTxtIn[0][0]                  \n                                                                 TxtCDisLblLSTM_17[0][0]          \n__________________________________________________________________________________________________\nImgCDisConcat_19 (Concatenate)  (None, 400, 400, 6)  0           CDisImgIn[0][0]                  \n                                                                 ImgCDisLblOut[0][0]              \n__________________________________________________________________________________________________\nTxtCDisMask_31 (Masking)        (None, 15, 542)      0           TxtCDisConcat_30[0][0]           \n__________________________________________________________________________________________________\nImgCDisConv2D_20 (Conv2D)       (None, 200, 200, 64) 6208        ImgCDisConcat_19[0][0]           \n__________________________________________________________________________________________________\nTxtCDisLSTM_32 (LSTM)           (None, 15, 400)      1508800     TxtCDisMask_31[0][0]             \n__________________________________________________________________________________________________\nImgCDisConv2D_21 (Conv2D)       (None, 100, 100, 32) 32800       ImgCDisConv2D_20[0][0]           \n__________________________________________________________________________________________________\nTxtCDisBN_33 (BatchNormalizatio (None, 15, 400)      1600        TxtCDisLSTM_32[0][0]             \n__________________________________________________________________________________________________\nImgCDisBN_22 (BatchNormalizatio (None, 100, 100, 32) 128         ImgCDisConv2D_21[0][0]           \n__________________________________________________________________________________________________\nTxCtDisDrop_34 (Dropout)        (None, 15, 400)      0           TxtCDisBN_33[0][0]               \n__________________________________________________________________________________________________\nImgCDisDrop_23 (Dropout)        (None, 100, 100, 32) 0           ImgCDisBN_22[0][0]               \n__________________________________________________________________________________________________\nTxtCDisLSTM_35 (LSTM)           (None, 15, 200)      480800      TxCtDisDrop_34[0][0]             \n__________________________________________________________________________________________________\nImgCDisConv2D_24 (Conv2D)       (None, 50, 50, 16)   8208        ImgCDisDrop_23[0][0]             \n__________________________________________________________________________________________________\nTxtCDisLSTM_36 (LSTM)           (None, 15, 100)      120400      TxtCDisLSTM_35[0][0]             \n__________________________________________________________________________________________________\nImgCDisConv2D_25 (Conv2D)       (None, 25, 25, 8)    2056        ImgCDisConv2D_24[0][0]           \n__________________________________________________________________________________________________\nTxtCDisBN_37 (BatchNormalizatio (None, 15, 100)      400         TxtCDisLSTM_36[0][0]             \n__________________________________________________________________________________________________\nImgCDisBN_26 (BatchNormalizatio (None, 25, 25, 8)    32          ImgCDisConv2D_25[0][0]           \n__________________________________________________________________________________________________\nTxtCDisDrop_38 (Dropout)        (None, 15, 100)      0           TxtCDisBN_37[0][0]               \n__________________________________________________________________________________________________\nImgCDisDrop_27 (Dropout)        (None, 25, 25, 8)    0           ImgCDisBN_26[0][0]               \n__________________________________________________________________________________________________\nTxtCDisFlat_39 (Flatten)        (None, 1500)         0           TxtCDisDrop_38[0][0]             \n__________________________________________________________________________________________________\nImgCDisFlat_28 (Flatten)        (None, 5000)         0           ImgCDisDrop_27[0][0]             \n__________________________________________________________________________________________________\nMultiCDisConcat_40 (Concatenate (None, 6500)         0           TxtCDisFlat_39[0][0]             \n                                                                 ImgCDisFlat_28[0][0]             \n__________________________________________________________________________________________________\nMultiCDisDense_41 (Dense)       (None, 2130)         13847130    MultiCDisConcat_40[0][0]         \n__________________________________________________________________________________________________\nMultiCDisDense_42 (Dense)       (None, 1065)         2269515     MultiCDisDense_41[0][0]          \n__________________________________________________________________________________________________\nMultiCDisDrop_43 (Dropout)      (None, 1065)         0           MultiCDisDense_42[0][0]          \n__________________________________________________________________________________________________\nMultiCDisDense_44 (Dense)       (None, 532)          567112      MultiCDisDrop_43[0][0]           \n__________________________________________________________________________________________________\nMultiCDisDense_45 (Dense)       (None, 266)          141778      MultiCDisDense_44[0][0]          \n__________________________________________________________________________________________________\nMultiCDisDrop_46 (Dropout)      (None, 266)          0           MultiCDisDense_45[0][0]          \n__________________________________________________________________________________________________\nMultiCDisDense_47 (Dense)       (None, 133)          35511       MultiCDisDrop_46[0][0]           \n__________________________________________________________________________________________________\nMultiCDisDense_48 (Dense)       (None, 66)           8844        MultiCDisDense_47[0][0]          \n__________________________________________________________________________________________________\nMultiCDisOut (Dense)            (None, 1)            67          MultiCDisDense_48[0][0]          \n==================================================================================================\nTotal params: 42,162,632\nTrainable params: 42,161,104\nNon-trainable params: 1,528\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multi_cdis_model = create_multi_cdiscriminator(img_shape, txt_shape, n_labels, multi_cdis_cfg)\n",
    "print(\"Multi CGAN-txt2img Discriminator Definition\")\n",
    "# dis_model = Sequential(slim_dis_layers)\n",
    "multi_cdis_model.model_name = \"Multi CGAN-txt2img Discriminator\"\n",
    "# compile model\n",
    "\n",
    "multi_cdis_model.compile(loss=multi_cdis_cfg[\"loss\"], \n",
    "                    optimizer=multi_cdis_cfg[\"optimizer\"], \n",
    "                    metrics=multi_cdis_cfg[\"metrics\"])\n",
    "\n",
    "# compile model\n",
    "multi_cdis_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi CGAN-txt2img Model definition\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ImgTxtCGenLblIn (InputLayer)    [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblEmb_1 (Embedding)  (None, 16, 142)      2130        ImgTxtCGenLblIn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblFlat_2 (Flatten)   (None, 2272)         0           ImgTxtCGenLblEmb_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDense_3 (Dense)    (None, 1065)         2420745     ImgTxtCGenLblFlat_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblBN_4 (BatchNormali (None, 1065)         4260        ImgTxtCGenLblDense_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDrop_5 (Dropout)   (None, 1065)         0           ImgTxtCGenLblBN_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblDense_6 (Dense)    (None, 2130)         2270580     ImgTxtCGenLblDrop_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenIn (InputLayer)       [(None, 15, 142)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLblOut (Reshape)      (None, 15, 142)      0           ImgTxtCGenLblDense_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenConcat (Concatenate)  (None, 15, 284)      0           ImgTxtCGenIn[0][0]               \n",
      "                                                                 ImgTxtCGenLblOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenMask_1 (Masking)      (None, 15, 284)      0           ImgTxtCGenConcat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenLSTM_2 (LSTM)         (None, 15, 400)      1096000     ImgTxtCGenMask_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenFlat_3 (Flatten)      (None, 6000)         0           ImgTxtCGenLSTM_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenDense_4 (Dense)       (None, 7500)         45007500    ImgTxtCGenFlat_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenReshape_5 (Reshape)      (None, 50, 50, 3)    0           ImgTxtCGenDense_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgTxtCGenDense_5 (Dense)       (None, 2130)         12782130    ImgTxtCGenFlat_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_6 (Conv2DTranspos (None, 100, 100, 8)  392         ImgCGenReshape_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenReshape_7 (Reshape)       (None, 15, 142)      0           ImgTxtCGenDense_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_7 (BatchNormalization (None, 100, 100, 8)  32          ImgCGenConv2D_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenBN_8 (BatchNormalization) (None, 15, 142)      568         TxtGenReshape_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_8 (Dropout)         (None, 100, 100, 8)  0           ImgCGenBN_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_9 (Dropout)          (None, 15, 142)      0           TxtGenBN_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_9 (Conv2DTranspos (None, 200, 200, 16) 2064        ImgCGenDrop_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenLSTM_10 (LSTM)            (None, 15, 100)      97200       TxtGenDrop_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_10 (Conv2DTranspo (None, 200, 200, 32) 8224        ImgCGenConv2D_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenLSTM_11 (LSTM)            (None, 15, 200)      240800      TxtGenLSTM_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenBN_11 (BatchNormalizatio (None, 200, 200, 32) 128         ImgCGenConv2D_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenBN_12 (BatchNormalization (None, 15, 200)      800         TxtGenLSTM_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenDrop_12 (Dropout)        (None, 200, 200, 32) 0           ImgCGenBN_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_13 (Dropout)         (None, 15, 200)      0           TxtGenBN_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenConv2D_13 (Conv2DTranspo (None, 400, 400, 64) 32832       ImgCGenDrop_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TxtGenDrop_14 (LSTM)            (None, 15, 400)      961600      TxtGenDrop_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ImgCGenOut (Conv2D)             (None, 400, 400, 3)  1731        ImgCGenConv2D_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "GenTxtOut (TimeDistributed)     (None, 15, 142)      56942       TxtGenDrop_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "model_16 (Functional)           (None, 1)            42162632    ImgCGenOut[0][0]                 \n",
      "                                                                 GenTxtOut[0][0]                  \n",
      "                                                                 ImgTxtCGenLblIn[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 107,149,290\n",
      "Trainable params: 64,983,764\n",
      "Non-trainable params: 42,165,526\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi CGAN-txt2img Model definition\")\n",
    "multi_cgan_model = create_multi_cgan(multi_cgen_model, multi_cdis_model, gan_cfg)\n",
    "multi_cgan_model.summary()\n",
    "multi_cgan_model.model_name = \"Multi CGAN-txt2img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(multi_cgan_model, model_fn_path, multi_cgan_model.model_name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-03 08:27:26\n"
     ]
    }
   ],
   "source": [
    "# saving model topology into png files\n",
    "print(timestamp)\n",
    "export_model(cgen_img_model, model_fn_path, cgen_img_model.model_name, timestamp)\n",
    "export_model(cdis_img_model, model_fn_path, cdis_img_model.model_name, timestamp)\n",
    "export_model(cgan_img_model, model_fn_path, cgan_img_model.model_name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-Images: (59, 400, 400, 3) \n-Text: (59, 15, 142) \n-Real/Fake: (59, 1) \n-txt&img Labels: (59, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"-Images:\", X_img.shape, \"\\n-Text:\", X_txt.shape, \"\\n-Real/Fake:\", y.shape, \"\\n-txt&img Labels:\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Training Config:\n {'epochs': 500, 'batch_size': 32, 'synth_batch': 1, 'gen_sample_size': 3, 'models_fn_path': 'c:\\\\Users\\\\Felipe\\\\Documents\\\\GitHub\\\\sa-artea\\\\VVG-MLModel-Trainer\\\\Data\\\\Models', 'report_fn_path': 'c:\\\\Users\\\\Felipe\\\\Documents\\\\GitHub\\\\sa-artea\\\\VVG-MLModel-Trainer\\\\Data\\\\Reports', 'dis_model_name': 'VVG-Text2Img-CDiscriminator', 'gen_model_name': 'VVG-Text2Img-CGenerator', 'gan_model_name': 'VVG-Text2Img-CGAN', 'check_epochs': 10, 'save_epochs': 50, 'max_models': 5, 'latent_shape': (15, 142), 'pretrained': False, 'conditioned': True, 'dataset_size': 59, 'cat_shape': (1,), 'txt_shape': (15, 142), 'label_shape': (16,), 'img_shape': (400, 400, 3), 'data_cols': 3}\n"
     ]
    }
   ],
   "source": [
    "# training and batch size\n",
    "gan_train_cfg = {\n",
    "    \"epochs\":500,\n",
    "    \"batch_size\":32,\n",
    "    \"synth_batch\": 1,\n",
    "    \"gen_sample_size\": 3,\n",
    "    \"models_fn_path\": model_fn_path,\n",
    "    \"report_fn_path\": report_fn_path,\n",
    "    \"dis_model_name\": dis_model_name,\n",
    "    \"gen_model_name\": gen_model_name,\n",
    "    \"gan_model_name\": gan_model_name,\n",
    "    \"check_epochs\": 10,\n",
    "    \"save_epochs\": 50,\n",
    "    \"max_save_models\": 5,\n",
    "    \"latent_shape\": X_txt[0].shape,\n",
    "    \"pretrained\": False,\n",
    "    \"conditioned\": True,\n",
    "    \"dataset_size\": X_img.shape[0],\n",
    "    \"cat_shape\": y[0].shape,\n",
    "    \"txt_shape\": X_txt[0].shape,\n",
    "    \"label_shape\": y_labels[0].shape,\n",
    "    \"img_shape\": X_img[0].shape,\n",
    "    # \"data_cols\": 2,\n",
    "    \"data_cols\": 3,\n",
    "    # \"data_cols\": 4,\n",
    "    }\n",
    "\n",
    "print(\"Model Training Config:\\n\", gan_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# gan_data = (X_img, y)\n",
    "gan_data = (X_img, y_labels, y)\n",
    "print(len(gan_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".719, acc: 0.000]\n",
      "Epoch:250 elapsed time: 16.35 [s]\n",
      ">>> Epoch: 251, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 1.772, acc: 0.000]\n",
      "Epoch: 251 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 1.868\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.122\n",
      "Epoch:251 elapsed time: 22.90 [s]\n",
      ">>> Epoch: 252, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 1.256, acc: 0.354] || [F-Dis loss: 0.451, acc: 0.062] || [Gen loss: 1.606, acc: 0.000]\n",
      "Epoch:252 elapsed time: 15.08 [s]\n",
      ">>> Epoch: 253, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 1.775, acc: 0.000]\n",
      "Epoch:253 elapsed time: 14.55 [s]\n",
      ">>> Epoch: 254, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 1.714, acc: 0.000]\n",
      "Epoch:254 elapsed time: 14.64 [s]\n",
      ">>> Epoch: 255, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 1.710, acc: 0.000]\n",
      "Epoch:255 elapsed time: 15.00 [s]\n",
      ">>> Epoch: 256, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.370, acc: 0.062] || [Gen loss: 1.726, acc: 0.000]\n",
      "Epoch:256 elapsed time: 14.92 [s]\n",
      ">>> Epoch: 257, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 1.902, acc: 0.000]\n",
      "Epoch:257 elapsed time: 14.63 [s]\n",
      ">>> Epoch: 258, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 2.055, acc: 0.000]\n",
      "Epoch:258 elapsed time: 14.41 [s]\n",
      ">>> Epoch: 259, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.386, acc: 0.062] || [Gen loss: 1.885, acc: 0.000]\n",
      "Epoch:259 elapsed time: 14.18 [s]\n",
      ">>> Epoch: 260, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.451, acc: 0.062] || [Gen loss: 1.739, acc: 0.000]\n",
      "Epoch:260 elapsed time: 14.20 [s]\n",
      ">>> Epoch: 261, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 1.681, acc: 0.000]\n",
      "Epoch: 261 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.450\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.029\n",
      "Epoch:261 elapsed time: 18.09 [s]\n",
      ">>> Epoch: 262, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.310, acc: 0.354] || [F-Dis loss: 0.515, acc: 0.062] || [Gen loss: 1.579, acc: 0.000]\n",
      "Epoch:262 elapsed time: 14.55 [s]\n",
      ">>> Epoch: 263, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.414, acc: 0.062] || [Gen loss: 1.571, acc: 0.000]\n",
      "Epoch:263 elapsed time: 16.05 [s]\n",
      ">>> Epoch: 264, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.398, acc: 0.062] || [Gen loss: 1.645, acc: 0.000]\n",
      "Epoch:264 elapsed time: 15.61 [s]\n",
      ">>> Epoch: 265, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 1.619, acc: 0.000]\n",
      "Epoch:265 elapsed time: 15.39 [s]\n",
      ">>> Epoch: 266, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.340, acc: 0.062] || [Gen loss: 1.682, acc: 0.000]\n",
      "Epoch:266 elapsed time: 17.99 [s]\n",
      ">>> Epoch: 267, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 1.586, acc: 0.000]\n",
      "Epoch:267 elapsed time: 18.84 [s]\n",
      ">>> Epoch: 268, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 1.357, acc: 0.000]\n",
      "Epoch:268 elapsed time: 25.13 [s]\n",
      ">>> Epoch: 269, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 1.238, acc: 0.000]\n",
      "Epoch:269 elapsed time: 27.27 [s]\n",
      ">>> Epoch: 270, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.388, acc: 0.062] || [Gen loss: 1.311, acc: 0.000]\n",
      "Epoch:270 elapsed time: 24.50 [s]\n",
      ">>> Epoch: 271, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 1.290, acc: 0.000]\n",
      "Epoch: 271 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.385\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.018\n",
      "Epoch:271 elapsed time: 38.35 [s]\n",
      ">>> Epoch: 272, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.263, acc: 0.354] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 1.237, acc: 0.000]\n",
      "Epoch:272 elapsed time: 28.24 [s]\n",
      ">>> Epoch: 273, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 1.134, acc: 0.000]\n",
      "Epoch:273 elapsed time: 26.06 [s]\n",
      ">>> Epoch: 274, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 1.317, acc: 0.000]\n",
      "Epoch:274 elapsed time: 25.08 [s]\n",
      ">>> Epoch: 275, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 1.214, acc: 0.000]\n",
      "Epoch:275 elapsed time: 31.57 [s]\n",
      ">>> Epoch: 276, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 1.255, acc: 0.000]\n",
      "Epoch:276 elapsed time: 34.30 [s]\n",
      ">>> Epoch: 277, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.057, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 1.643, acc: 0.000]\n",
      "Epoch:277 elapsed time: 30.23 [s]\n",
      ">>> Epoch: 278, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.369, acc: 0.062] || [Gen loss: 1.526, acc: 0.000]\n",
      "Epoch:278 elapsed time: 27.48 [s]\n",
      ">>> Epoch: 279, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 1.773, acc: 0.000]\n",
      "Epoch:279 elapsed time: 24.48 [s]\n",
      ">>> Epoch: 280, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.344, acc: 0.062] || [Gen loss: 1.694, acc: 0.000]\n",
      "Epoch:280 elapsed time: 24.30 [s]\n",
      ">>> Epoch: 281, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.056, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.493, acc: 0.000]\n",
      "Epoch: 281 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.390\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.019\n",
      "Epoch:281 elapsed time: 33.21 [s]\n",
      ">>> Epoch: 282, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.274, acc: 0.354] || [F-Dis loss: 0.432, acc: 0.062] || [Gen loss: 1.439, acc: 0.000]\n",
      "Epoch:282 elapsed time: 24.55 [s]\n",
      ">>> Epoch: 283, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.375, acc: 0.062] || [Gen loss: 1.379, acc: 0.000]\n",
      "Epoch:283 elapsed time: 24.63 [s]\n",
      ">>> Epoch: 284, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.379, acc: 0.062] || [Gen loss: 1.570, acc: 0.000]\n",
      "Epoch:284 elapsed time: 21.36 [s]\n",
      ">>> Epoch: 285, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 1.456, acc: 0.000]\n",
      "Epoch:285 elapsed time: 21.76 [s]\n",
      ">>> Epoch: 286, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.407, acc: 0.062] || [Gen loss: 1.510, acc: 0.000]\n",
      "Epoch:286 elapsed time: 22.55 [s]\n",
      ">>> Epoch: 287, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 1.404, acc: 0.000]\n",
      "Epoch:287 elapsed time: 23.42 [s]\n",
      ">>> Epoch: 288, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 1.522, acc: 0.000]\n",
      "Epoch:288 elapsed time: 18.27 [s]\n",
      ">>> Epoch: 289, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 1.524, acc: 0.000]\n",
      "Epoch:289 elapsed time: 17.23 [s]\n",
      ">>> Epoch: 290, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 1.635, acc: 0.000]\n",
      "Epoch:290 elapsed time: 15.91 [s]\n",
      ">>> Epoch: 291, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 1.598, acc: 0.000]\n",
      "Epoch: 291 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.417\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.027\n",
      "Epoch:291 elapsed time: 20.07 [s]\n",
      ">>> Epoch: 292, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.283, acc: 0.354] || [F-Dis loss: 0.353, acc: 0.062] || [Gen loss: 1.564, acc: 0.000]\n",
      "Epoch:292 elapsed time: 14.54 [s]\n",
      ">>> Epoch: 293, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 1.515, acc: 0.000]\n",
      "Epoch:293 elapsed time: 14.31 [s]\n",
      ">>> Epoch: 294, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.384, acc: 0.062] || [Gen loss: 1.520, acc: 0.000]\n",
      "Epoch:294 elapsed time: 15.12 [s]\n",
      ">>> Epoch: 295, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 1.809, acc: 0.000]\n",
      "Epoch:295 elapsed time: 13.79 [s]\n",
      ">>> Epoch: 296, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.374, acc: 0.062] || [Gen loss: 1.812, acc: 0.000]\n",
      "Epoch:296 elapsed time: 13.57 [s]\n",
      ">>> Epoch: 297, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.383, acc: 0.062] || [Gen loss: 1.817, acc: 0.000]\n",
      "Epoch:297 elapsed time: 17.16 [s]\n",
      ">>> Epoch: 298, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 1.893, acc: 0.000]\n",
      "Epoch:298 elapsed time: 16.80 [s]\n",
      ">>> Epoch: 299, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.090, acc: 0.875] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 1.264, acc: 0.000]\n",
      "Epoch:299 elapsed time: 14.94 [s]\n",
      ">>> Epoch: 300, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 0.678, acc: 0.000]\n",
      "Epoch:300 elapsed time: 15.87 [s]\n",
      ">>> Epoch: 301, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 1.261, acc: 0.000]\n",
      "Epoch: 301 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 6.102\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.433\n",
      "Epoch:301 elapsed time: 23.99 [s]\n",
      ">>> Epoch: 302, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 4.082, acc: 0.354] || [F-Dis loss: 0.405, acc: 0.062] || [Gen loss: 1.247, acc: 0.000]\n",
      "Epoch:302 elapsed time: 14.93 [s]\n",
      ">>> Epoch: 303, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.459, acc: 0.062] || [Gen loss: 0.546, acc: 0.000]\n",
      "Epoch:303 elapsed time: 14.87 [s]\n",
      ">>> Epoch: 304, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.419, acc: 0.062] || [Gen loss: 0.323, acc: 0.000]\n",
      "Epoch:304 elapsed time: 14.73 [s]\n",
      ">>> Epoch: 305, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.334, acc: 0.062] || [Gen loss: 0.378, acc: 0.000]\n",
      "Epoch:305 elapsed time: 14.38 [s]\n",
      ">>> Epoch: 306, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.320, acc: 0.000]\n",
      "Epoch:306 elapsed time: 14.24 [s]\n",
      ">>> Epoch: 307, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 0.454, acc: 0.000]\n",
      "Epoch:307 elapsed time: 14.22 [s]\n",
      ">>> Epoch: 308, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.039, acc: 0.938] || [F-Dis loss: 0.415, acc: 0.062] || [Gen loss: 0.293, acc: 0.000]\n",
      "Epoch:308 elapsed time: 14.79 [s]\n",
      ">>> Epoch: 309, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 0.226, acc: 0.000]\n",
      "Epoch:309 elapsed time: 17.36 [s]\n",
      ">>> Epoch: 310, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.460, acc: 0.062] || [Gen loss: 0.207, acc: 0.000]\n",
      "Epoch:310 elapsed time: 17.16 [s]\n",
      ">>> Epoch: 311, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 0.148, acc: 0.000]\n",
      "Epoch: 311 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 7.348\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.463\n",
      "Epoch:311 elapsed time: 21.57 [s]\n",
      ">>> Epoch: 312, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 4.903, acc: 0.354] || [F-Dis loss: 0.446, acc: 0.062] || [Gen loss: 0.238, acc: 0.000]\n",
      "Epoch:312 elapsed time: 16.91 [s]\n",
      ">>> Epoch: 313, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.410, acc: 0.062] || [Gen loss: 0.367, acc: 0.000]\n",
      "Epoch:313 elapsed time: 17.98 [s]\n",
      ">>> Epoch: 314, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.357, acc: 0.062] || [Gen loss: 0.259, acc: 0.000]\n",
      "Epoch:314 elapsed time: 18.14 [s]\n",
      ">>> Epoch: 315, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.345, acc: 0.062] || [Gen loss: 0.168, acc: 0.000]\n",
      "Epoch:315 elapsed time: 17.17 [s]\n",
      ">>> Epoch: 316, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.449, acc: 0.062] || [Gen loss: 0.231, acc: 0.000]\n",
      "Epoch:316 elapsed time: 16.67 [s]\n",
      ">>> Epoch: 317, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.365, acc: 0.062] || [Gen loss: 0.228, acc: 0.000]\n",
      "Epoch:317 elapsed time: 17.52 [s]\n",
      ">>> Epoch: 318, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 0.289, acc: 0.000]\n",
      "Epoch:318 elapsed time: 16.10 [s]\n",
      ">>> Epoch: 319, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 0.464, acc: 0.000]\n",
      "Epoch:319 elapsed time: 16.06 [s]\n",
      ">>> Epoch: 320, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.342, acc: 0.062] || [Gen loss: 0.502, acc: 0.000]\n",
      "Epoch:320 elapsed time: 18.20 [s]\n",
      ">>> Epoch: 321, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 0.612, acc: 0.000]\n",
      "Epoch: 321 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 6.614\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.428\n",
      "Epoch:321 elapsed time: 22.37 [s]\n",
      ">>> Epoch: 322, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 4.424, acc: 0.354] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 0.407, acc: 0.000]\n",
      "Epoch:322 elapsed time: 15.48 [s]\n",
      ">>> Epoch: 323, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 0.569, acc: 0.000]\n",
      "Epoch:323 elapsed time: 13.82 [s]\n",
      ">>> Epoch: 324, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.359, acc: 0.062] || [Gen loss: 0.563, acc: 0.000]\n",
      "Epoch:324 elapsed time: 13.90 [s]\n",
      ">>> Epoch: 325, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.501, acc: 0.000]\n",
      "Epoch:325 elapsed time: 13.91 [s]\n",
      ">>> Epoch: 326, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 0.438, acc: 0.000]\n",
      "Epoch:326 elapsed time: 14.79 [s]\n",
      ">>> Epoch: 327, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 0.636, acc: 0.000]\n",
      "Epoch:327 elapsed time: 15.48 [s]\n",
      ">>> Epoch: 328, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 0.897, acc: 0.000]\n",
      "Epoch:328 elapsed time: 15.79 [s]\n",
      ">>> Epoch: 329, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.350, acc: 0.062] || [Gen loss: 0.938, acc: 0.000]\n",
      "Epoch:329 elapsed time: 16.74 [s]\n",
      ">>> Epoch: 330, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 1.047, acc: 0.000]\n",
      "Epoch:330 elapsed time: 16.38 [s]\n",
      ">>> Epoch: 331, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.347, acc: 0.062] || [Gen loss: 1.032, acc: 0.000]\n",
      "Epoch: 331 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 5.944\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.374\n",
      "Epoch:331 elapsed time: 18.73 [s]\n",
      ">>> Epoch: 332, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 3.972, acc: 0.354] || [F-Dis loss: 0.388, acc: 0.062] || [Gen loss: 1.065, acc: 0.000]\n",
      "Epoch:332 elapsed time: 14.31 [s]\n",
      ">>> Epoch: 333, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 0.972, acc: 0.000]\n",
      "Epoch:333 elapsed time: 13.96 [s]\n",
      ">>> Epoch: 334, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.379, acc: 0.062] || [Gen loss: 0.972, acc: 0.000]\n",
      "Epoch:334 elapsed time: 14.30 [s]\n",
      ">>> Epoch: 335, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.012, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.116, acc: 0.000]\n",
      "Epoch:335 elapsed time: 14.35 [s]\n",
      ">>> Epoch: 336, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 1.164, acc: 0.000]\n",
      "Epoch:336 elapsed time: 14.71 [s]\n",
      ">>> Epoch: 337, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 0.969, acc: 0.000]\n",
      "Epoch:337 elapsed time: 15.70 [s]\n",
      ">>> Epoch: 338, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.368, acc: 0.062] || [Gen loss: 0.990, acc: 0.000]\n",
      "Epoch:338 elapsed time: 15.83 [s]\n",
      ">>> Epoch: 339, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 1.257, acc: 0.000]\n",
      "Epoch:339 elapsed time: 15.70 [s]\n",
      ">>> Epoch: 340, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.356, acc: 0.062] || [Gen loss: 1.388, acc: 0.000]\n",
      "Epoch:340 elapsed time: 15.35 [s]\n",
      ">>> Epoch: 341, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.007, acc: 0.938] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 1.246, acc: 0.000]\n",
      "Epoch: 341 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 5.168\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.385\n",
      "Epoch:341 elapsed time: 19.39 [s]\n",
      ">>> Epoch: 342, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 3.453, acc: 0.354] || [F-Dis loss: 0.461, acc: 0.062] || [Gen loss: 1.409, acc: 0.000]\n",
      "Epoch:342 elapsed time: 15.15 [s]\n",
      ">>> Epoch: 343, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.376, acc: 0.062] || [Gen loss: 1.398, acc: 0.000]\n",
      "Epoch:343 elapsed time: 14.53 [s]\n",
      ">>> Epoch: 344, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.453, acc: 0.000]\n",
      "Epoch:344 elapsed time: 14.71 [s]\n",
      ">>> Epoch: 345, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.420, acc: 0.062] || [Gen loss: 1.295, acc: 0.000]\n",
      "Epoch:345 elapsed time: 15.09 [s]\n",
      ">>> Epoch: 346, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.007, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 1.417, acc: 0.000]\n",
      "Epoch:346 elapsed time: 14.79 [s]\n",
      ">>> Epoch: 347, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.362, acc: 0.062] || [Gen loss: 1.547, acc: 0.000]\n",
      "Epoch:347 elapsed time: 14.86 [s]\n",
      ">>> Epoch: 348, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.453, acc: 0.062] || [Gen loss: 1.455, acc: 0.000]\n",
      "Epoch:348 elapsed time: 15.83 [s]\n",
      ">>> Epoch: 349, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 1.160, acc: 0.000]\n",
      "Epoch:349 elapsed time: 18.93 [s]\n",
      ">>> Epoch: 350, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.421, acc: 0.062] || [Gen loss: 1.254, acc: 0.000]\n",
      "Epoch:350 elapsed time: 18.17 [s]\n",
      ">>> Epoch: 351, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 1.134, acc: 0.000]\n",
      "Epoch: 351 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 5.355\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.341\n",
      "Epoch:351 elapsed time: 26.84 [s]\n",
      ">>> Epoch: 352, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 3.577, acc: 0.354] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 1.146, acc: 0.000]\n",
      "Epoch:352 elapsed time: 18.30 [s]\n",
      ">>> Epoch: 353, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 1.326, acc: 0.000]\n",
      "Epoch:353 elapsed time: 17.52 [s]\n",
      ">>> Epoch: 354, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.050, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 1.460, acc: 0.000]\n",
      "Epoch:354 elapsed time: 16.30 [s]\n",
      ">>> Epoch: 355, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 1.820, acc: 0.000]\n",
      "Epoch:355 elapsed time: 15.40 [s]\n",
      ">>> Epoch: 356, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 1.915, acc: 0.000]\n",
      "Epoch:356 elapsed time: 14.83 [s]\n",
      ">>> Epoch: 357, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.426, acc: 0.062] || [Gen loss: 1.772, acc: 0.000]\n",
      "Epoch:357 elapsed time: 14.40 [s]\n",
      ">>> Epoch: 358, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 1.785, acc: 0.000]\n",
      "Epoch:358 elapsed time: 13.92 [s]\n",
      ">>> Epoch: 359, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 1.715, acc: 0.000]\n",
      "Epoch:359 elapsed time: 13.94 [s]\n",
      ">>> Epoch: 360, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.021, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 1.872, acc: 0.000]\n",
      "Epoch:360 elapsed time: 13.68 [s]\n",
      ">>> Epoch: 361, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.443, acc: 0.062] || [Gen loss: 1.632, acc: 0.000]\n",
      "Epoch: 361 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 3.942\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.283\n",
      "Epoch:361 elapsed time: 17.87 [s]\n",
      ">>> Epoch: 362, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 2.635, acc: 0.354] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.672, acc: 0.000]\n",
      "Epoch:362 elapsed time: 14.62 [s]\n",
      ">>> Epoch: 363, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.356, acc: 0.062] || [Gen loss: 1.736, acc: 0.000]\n",
      "Epoch:363 elapsed time: 14.95 [s]\n",
      ">>> Epoch: 364, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.661, acc: 0.000]\n",
      "Epoch:364 elapsed time: 14.78 [s]\n",
      ">>> Epoch: 365, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.011, acc: 0.938] || [F-Dis loss: 0.430, acc: 0.062] || [Gen loss: 1.657, acc: 0.000]\n",
      "Epoch:365 elapsed time: 14.67 [s]\n",
      ">>> Epoch: 366, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.414, acc: 0.062] || [Gen loss: 1.675, acc: 0.000]\n",
      "Epoch:366 elapsed time: 15.00 [s]\n",
      ">>> Epoch: 367, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.429, acc: 0.062] || [Gen loss: 1.554, acc: 0.000]\n",
      "Epoch:367 elapsed time: 15.17 [s]\n",
      ">>> Epoch: 368, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 1.617, acc: 0.000]\n",
      "Epoch:368 elapsed time: 16.57 [s]\n",
      ">>> Epoch: 369, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.033, acc: 0.938] || [F-Dis loss: 0.416, acc: 0.062] || [Gen loss: 1.733, acc: 0.000]\n",
      "Epoch:369 elapsed time: 18.66 [s]\n",
      ">>> Epoch: 370, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.743, acc: 0.000]\n",
      "Epoch:370 elapsed time: 15.93 [s]\n",
      ">>> Epoch: 371, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 1.697, acc: 0.000]\n",
      "Epoch: 371 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 2.526\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.174\n",
      "Epoch:371 elapsed time: 18.73 [s]\n",
      ">>> Epoch: 372, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 1.692, acc: 0.354] || [F-Dis loss: 0.359, acc: 0.062] || [Gen loss: 1.863, acc: 0.000]\n",
      "Epoch:372 elapsed time: 15.89 [s]\n",
      ">>> Epoch: 373, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 1.967, acc: 0.000]\n",
      "Epoch:373 elapsed time: 14.95 [s]\n",
      ">>> Epoch: 374, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.044, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 1.935, acc: 0.000]\n",
      "Epoch:374 elapsed time: 15.31 [s]\n",
      ">>> Epoch: 375, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 1.745, acc: 0.000]\n",
      "Epoch:375 elapsed time: 14.81 [s]\n",
      ">>> Epoch: 376, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.374, acc: 0.062] || [Gen loss: 1.754, acc: 0.000]\n",
      "Epoch:376 elapsed time: 14.76 [s]\n",
      ">>> Epoch: 377, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.006, acc: 0.938] || [F-Dis loss: 0.419, acc: 0.062] || [Gen loss: 1.800, acc: 0.000]\n",
      "Epoch:377 elapsed time: 14.79 [s]\n",
      ">>> Epoch: 378, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 1.755, acc: 0.000]\n",
      "Epoch:378 elapsed time: 14.16 [s]\n",
      ">>> Epoch: 379, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.391, acc: 0.062] || [Gen loss: 1.683, acc: 0.000]\n",
      "Epoch:379 elapsed time: 14.35 [s]\n",
      ">>> Epoch: 380, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 1.668, acc: 0.000]\n",
      "Epoch:380 elapsed time: 14.70 [s]\n",
      ">>> Epoch: 381, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.424, acc: 0.062] || [Gen loss: 1.655, acc: 0.000]\n",
      "Epoch: 381 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.434\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.024\n",
      "Epoch:381 elapsed time: 19.31 [s]\n",
      ">>> Epoch: 382, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.298, acc: 0.354] || [F-Dis loss: 0.437, acc: 0.062] || [Gen loss: 1.726, acc: 0.000]\n",
      "Epoch:382 elapsed time: 15.16 [s]\n",
      ">>> Epoch: 383, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 1.784, acc: 0.000]\n",
      "Epoch:383 elapsed time: 14.72 [s]\n",
      ">>> Epoch: 384, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 1.857, acc: 0.000]\n",
      "Epoch:384 elapsed time: 14.49 [s]\n",
      ">>> Epoch: 385, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.383, acc: 0.062] || [Gen loss: 1.817, acc: 0.000]\n",
      "Epoch:385 elapsed time: 14.34 [s]\n",
      ">>> Epoch: 386, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.019, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 1.863, acc: 0.000]\n",
      "Epoch:386 elapsed time: 14.12 [s]\n",
      ">>> Epoch: 387, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.336, acc: 0.062] || [Gen loss: 1.808, acc: 0.000]\n",
      "Epoch:387 elapsed time: 14.01 [s]\n",
      ">>> Epoch: 388, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.404, acc: 0.062] || [Gen loss: 1.917, acc: 0.000]\n",
      "Epoch:388 elapsed time: 13.96 [s]\n",
      ">>> Epoch: 389, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.307, acc: 0.062] || [Gen loss: 1.899, acc: 0.000]\n",
      "Epoch:389 elapsed time: 14.21 [s]\n",
      ">>> Epoch: 390, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.005, acc: 0.938] || [F-Dis loss: 0.405, acc: 0.062] || [Gen loss: 1.833, acc: 0.000]\n",
      "Epoch:390 elapsed time: 14.34 [s]\n",
      ">>> Epoch: 391, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.014, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.860, acc: 0.000]\n",
      "Epoch: 391 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.412\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.016\n",
      "Epoch:391 elapsed time: 18.31 [s]\n",
      ">>> Epoch: 392, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.279, acc: 0.354] || [F-Dis loss: 0.381, acc: 0.062] || [Gen loss: 1.877, acc: 0.000]\n",
      "Epoch:392 elapsed time: 14.95 [s]\n",
      ">>> Epoch: 393, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.410, acc: 0.062] || [Gen loss: 1.755, acc: 0.000]\n",
      "Epoch:393 elapsed time: 15.99 [s]\n",
      ">>> Epoch: 394, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.403, acc: 0.062] || [Gen loss: 1.665, acc: 0.000]\n",
      "Epoch:394 elapsed time: 16.85 [s]\n",
      ">>> Epoch: 395, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.464, acc: 0.062] || [Gen loss: 1.565, acc: 0.000]\n",
      "Epoch:395 elapsed time: 19.03 [s]\n",
      ">>> Epoch: 396, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 1.572, acc: 0.000]\n",
      "Epoch:396 elapsed time: 20.26 [s]\n",
      ">>> Epoch: 397, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.454, acc: 0.062] || [Gen loss: 1.486, acc: 0.000]\n",
      "Epoch:397 elapsed time: 19.86 [s]\n",
      ">>> Epoch: 398, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 1.657, acc: 0.000]\n",
      "Epoch:398 elapsed time: 22.51 [s]\n",
      ">>> Epoch: 399, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.035, acc: 0.938] || [F-Dis loss: 0.445, acc: 0.062] || [Gen loss: 1.604, acc: 0.000]\n",
      "Epoch:399 elapsed time: 24.04 [s]\n",
      ">>> Epoch: 400, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.408, acc: 0.062] || [Gen loss: 1.550, acc: 0.000]\n",
      "Epoch:400 elapsed time: 25.04 [s]\n",
      ">>> Epoch: 401, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 1.591, acc: 0.000]\n",
      "Epoch: 401 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.405\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.024\n",
      "Epoch:401 elapsed time: 35.51 [s]\n",
      ">>> Epoch: 402, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.278, acc: 0.354] || [F-Dis loss: 0.382, acc: 0.062] || [Gen loss: 1.564, acc: 0.000]\n",
      "Epoch:402 elapsed time: 27.42 [s]\n",
      ">>> Epoch: 403, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.389, acc: 0.062] || [Gen loss: 1.635, acc: 0.000]\n",
      "Epoch:403 elapsed time: 25.65 [s]\n",
      ">>> Epoch: 404, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.310, acc: 0.062] || [Gen loss: 1.718, acc: 0.000]\n",
      "Epoch:404 elapsed time: 25.80 [s]\n",
      ">>> Epoch: 405, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.057, acc: 0.938] || [F-Dis loss: 0.434, acc: 0.062] || [Gen loss: 1.540, acc: 0.000]\n",
      "Epoch:405 elapsed time: 27.40 [s]\n",
      ">>> Epoch: 406, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.419, acc: 0.062] || [Gen loss: 1.471, acc: 0.000]\n",
      "Epoch:406 elapsed time: 25.23 [s]\n",
      ">>> Epoch: 407, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 1.499, acc: 0.000]\n",
      "Epoch:407 elapsed time: 24.09 [s]\n",
      ">>> Epoch: 408, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.298, acc: 0.062] || [Gen loss: 1.467, acc: 0.000]\n",
      "Epoch:408 elapsed time: 24.00 [s]\n",
      ">>> Epoch: 409, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 1.623, acc: 0.000]\n",
      "Epoch:409 elapsed time: 25.15 [s]\n",
      ">>> Epoch: 410, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 1.548, acc: 0.000]\n",
      "Epoch:410 elapsed time: 24.41 [s]\n",
      ">>> Epoch: 411, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.383, acc: 0.062] || [Gen loss: 1.613, acc: 0.000]\n",
      "Epoch: 411 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.413\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.016\n",
      "Epoch:411 elapsed time: 30.07 [s]\n",
      ">>> Epoch: 412, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.286, acc: 0.354] || [F-Dis loss: 0.399, acc: 0.062] || [Gen loss: 1.566, acc: 0.000]\n",
      "Epoch:412 elapsed time: 23.66 [s]\n",
      ">>> Epoch: 413, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.442, acc: 0.062] || [Gen loss: 1.420, acc: 0.000]\n",
      "Epoch:413 elapsed time: 22.85 [s]\n",
      ">>> Epoch: 414, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.428, acc: 0.062] || [Gen loss: 1.493, acc: 0.000]\n",
      "Epoch:414 elapsed time: 23.18 [s]\n",
      ">>> Epoch: 415, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.515, acc: 0.000]\n",
      "Epoch:415 elapsed time: 21.70 [s]\n",
      ">>> Epoch: 416, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 1.627, acc: 0.000]\n",
      "Epoch:416 elapsed time: 18.50 [s]\n",
      ">>> Epoch: 417, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.446, acc: 0.062] || [Gen loss: 1.583, acc: 0.000]\n",
      "Epoch:417 elapsed time: 17.19 [s]\n",
      ">>> Epoch: 418, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 1.625, acc: 0.000]\n",
      "Epoch:418 elapsed time: 20.16 [s]\n",
      ">>> Epoch: 419, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.383, acc: 0.062] || [Gen loss: 1.653, acc: 0.000]\n",
      "Epoch:419 elapsed time: 19.28 [s]\n",
      ">>> Epoch: 420, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.010, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 1.714, acc: 0.000]\n",
      "Epoch:420 elapsed time: 19.55 [s]\n",
      ">>> Epoch: 421, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.007, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.814, acc: 0.000]\n",
      "Epoch: 421 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.387\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.028\n",
      "Epoch:421 elapsed time: 26.26 [s]\n",
      ">>> Epoch: 422, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.260, acc: 0.354] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.838, acc: 0.000]\n",
      "Epoch:422 elapsed time: 25.42 [s]\n",
      ">>> Epoch: 423, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.028, acc: 0.938] || [F-Dis loss: 0.467, acc: 0.062] || [Gen loss: 1.631, acc: 0.000]\n",
      "Epoch:423 elapsed time: 20.11 [s]\n",
      ">>> Epoch: 424, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.385, acc: 0.062] || [Gen loss: 1.689, acc: 0.000]\n",
      "Epoch:424 elapsed time: 21.75 [s]\n",
      ">>> Epoch: 425, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.045, acc: 0.938] || [F-Dis loss: 0.396, acc: 0.062] || [Gen loss: 1.650, acc: 0.000]\n",
      "Epoch:425 elapsed time: 19.53 [s]\n",
      ">>> Epoch: 426, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.387, acc: 0.062] || [Gen loss: 1.701, acc: 0.000]\n",
      "Epoch:426 elapsed time: 19.59 [s]\n",
      ">>> Epoch: 427, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 1.598, acc: 0.000]\n",
      "Epoch:427 elapsed time: 19.07 [s]\n",
      ">>> Epoch: 428, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.456, acc: 0.062] || [Gen loss: 1.436, acc: 0.000]\n",
      "Epoch:428 elapsed time: 17.25 [s]\n",
      ">>> Epoch: 429, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.378, acc: 0.062] || [Gen loss: 1.493, acc: 0.000]\n",
      "Epoch:429 elapsed time: 20.36 [s]\n",
      ">>> Epoch: 430, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.439, acc: 0.062] || [Gen loss: 1.496, acc: 0.000]\n",
      "Epoch:430 elapsed time: 18.88 [s]\n",
      ">>> Epoch: 431, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 1.447, acc: 0.000]\n",
      "Epoch: 431 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.374\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.033\n",
      "Epoch:431 elapsed time: 27.83 [s]\n",
      ">>> Epoch: 432, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.256, acc: 0.354] || [F-Dis loss: 0.409, acc: 0.062] || [Gen loss: 1.529, acc: 0.000]\n",
      "Epoch:432 elapsed time: 19.28 [s]\n",
      ">>> Epoch: 433, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 1.544, acc: 0.000]\n",
      "Epoch:433 elapsed time: 17.84 [s]\n",
      ">>> Epoch: 434, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 1.642, acc: 0.000]\n",
      "Epoch:434 elapsed time: 15.91 [s]\n",
      ">>> Epoch: 435, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.410, acc: 0.062] || [Gen loss: 1.696, acc: 0.000]\n",
      "Epoch:435 elapsed time: 16.31 [s]\n",
      ">>> Epoch: 436, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.041, acc: 0.938] || [F-Dis loss: 0.377, acc: 0.062] || [Gen loss: 1.766, acc: 0.000]\n",
      "Epoch:436 elapsed time: 14.17 [s]\n",
      ">>> Epoch: 437, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.509, acc: 0.000]\n",
      "Epoch:437 elapsed time: 13.93 [s]\n",
      ">>> Epoch: 438, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.375, acc: 0.062] || [Gen loss: 1.590, acc: 0.000]\n",
      "Epoch:438 elapsed time: 16.60 [s]\n",
      ">>> Epoch: 439, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.013, acc: 0.938] || [F-Dis loss: 0.464, acc: 0.062] || [Gen loss: 1.538, acc: 0.000]\n",
      "Epoch:439 elapsed time: 16.86 [s]\n",
      ">>> Epoch: 440, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.392, acc: 0.062] || [Gen loss: 1.510, acc: 0.000]\n",
      "Epoch:440 elapsed time: 16.62 [s]\n",
      ">>> Epoch: 441, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.438, acc: 0.062] || [Gen loss: 1.598, acc: 0.000]\n",
      "Epoch: 441 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.405\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.024\n",
      "Epoch:441 elapsed time: 23.79 [s]\n",
      ">>> Epoch: 442, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.275, acc: 0.354] || [F-Dis loss: 0.411, acc: 0.062] || [Gen loss: 1.639, acc: 0.000]\n",
      "Epoch:442 elapsed time: 16.24 [s]\n",
      ">>> Epoch: 443, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.458, acc: 0.062] || [Gen loss: 1.597, acc: 0.000]\n",
      "Epoch:443 elapsed time: 16.40 [s]\n",
      ">>> Epoch: 444, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.369, acc: 0.062] || [Gen loss: 1.654, acc: 0.000]\n",
      "Epoch:444 elapsed time: 15.68 [s]\n",
      ">>> Epoch: 445, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.004, acc: 0.938] || [F-Dis loss: 0.480, acc: 0.062] || [Gen loss: 1.673, acc: 0.000]\n",
      "Epoch:445 elapsed time: 15.16 [s]\n",
      ">>> Epoch: 446, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.022, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 1.669, acc: 0.000]\n",
      "Epoch:446 elapsed time: 15.29 [s]\n",
      ">>> Epoch: 447, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.423, acc: 0.062] || [Gen loss: 1.863, acc: 0.000]\n",
      "Epoch:447 elapsed time: 16.38 [s]\n",
      ">>> Epoch: 448, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.712, acc: 0.000]\n",
      "Epoch:448 elapsed time: 15.49 [s]\n",
      ">>> Epoch: 449, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.006, acc: 0.938] || [F-Dis loss: 0.402, acc: 0.062] || [Gen loss: 1.857, acc: 0.000]\n",
      "Epoch:449 elapsed time: 15.34 [s]\n",
      ">>> Epoch: 450, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.346, acc: 0.062] || [Gen loss: 1.784, acc: 0.000]\n",
      "Epoch:450 elapsed time: 15.73 [s]\n",
      ">>> Epoch: 451, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.621, acc: 0.000]\n",
      "Epoch: 451 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.448\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.032\n",
      "Epoch:451 elapsed time: 26.81 [s]\n",
      ">>> Epoch: 452, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.312, acc: 0.354] || [F-Dis loss: 0.373, acc: 0.062] || [Gen loss: 1.685, acc: 0.000]\n",
      "Epoch:452 elapsed time: 18.81 [s]\n",
      ">>> Epoch: 453, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.017, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 1.632, acc: 0.000]\n",
      "Epoch:453 elapsed time: 18.52 [s]\n",
      ">>> Epoch: 454, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.450, acc: 0.062] || [Gen loss: 1.490, acc: 0.000]\n",
      "Epoch:454 elapsed time: 17.76 [s]\n",
      ">>> Epoch: 455, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 1.531, acc: 0.000]\n",
      "Epoch:455 elapsed time: 18.45 [s]\n",
      ">>> Epoch: 456, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.369, acc: 0.062] || [Gen loss: 1.552, acc: 0.000]\n",
      "Epoch:456 elapsed time: 19.17 [s]\n",
      ">>> Epoch: 457, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.341, acc: 0.062] || [Gen loss: 1.764, acc: 0.000]\n",
      "Epoch:457 elapsed time: 18.57 [s]\n",
      ">>> Epoch: 458, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.038, acc: 0.938] || [F-Dis loss: 0.419, acc: 0.062] || [Gen loss: 1.656, acc: 0.000]\n",
      "Epoch:458 elapsed time: 18.88 [s]\n",
      ">>> Epoch: 459, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.357, acc: 0.062] || [Gen loss: 1.626, acc: 0.000]\n",
      "Epoch:459 elapsed time: 18.88 [s]\n",
      ">>> Epoch: 460, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.364, acc: 0.062] || [Gen loss: 1.578, acc: 0.000]\n",
      "Epoch:460 elapsed time: 18.70 [s]\n",
      ">>> Epoch: 461, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.030, acc: 0.938] || [F-Dis loss: 0.367, acc: 0.062] || [Gen loss: 1.521, acc: 0.000]\n",
      "Epoch: 461 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.412\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.038\n",
      "Epoch:461 elapsed time: 23.50 [s]\n",
      ">>> Epoch: 462, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.287, acc: 0.354] || [F-Dis loss: 0.412, acc: 0.062] || [Gen loss: 1.390, acc: 0.000]\n",
      "Epoch:462 elapsed time: 18.57 [s]\n",
      ">>> Epoch: 463, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.034, acc: 0.938] || [F-Dis loss: 0.436, acc: 0.062] || [Gen loss: 1.420, acc: 0.000]\n",
      "Epoch:463 elapsed time: 16.77 [s]\n",
      ">>> Epoch: 464, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.054, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 1.802, acc: 0.000]\n",
      "Epoch:464 elapsed time: 15.99 [s]\n",
      ">>> Epoch: 465, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.042, acc: 0.938] || [F-Dis loss: 0.431, acc: 0.062] || [Gen loss: 1.943, acc: 0.000]\n",
      "Epoch:465 elapsed time: 15.22 [s]\n",
      ">>> Epoch: 466, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.384, acc: 0.062] || [Gen loss: 1.802, acc: 0.000]\n",
      "Epoch:466 elapsed time: 16.22 [s]\n",
      ">>> Epoch: 467, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.020, acc: 0.938] || [F-Dis loss: 0.397, acc: 0.062] || [Gen loss: 1.909, acc: 0.000]\n",
      "Epoch:467 elapsed time: 14.64 [s]\n",
      ">>> Epoch: 468, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.036, acc: 0.938] || [F-Dis loss: 0.457, acc: 0.062] || [Gen loss: 1.842, acc: 0.000]\n",
      "Epoch:468 elapsed time: 14.52 [s]\n",
      ">>> Epoch: 469, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.427, acc: 0.062] || [Gen loss: 1.826, acc: 0.000]\n",
      "Epoch:469 elapsed time: 14.08 [s]\n",
      ">>> Epoch: 470, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.018, acc: 0.938] || [F-Dis loss: 0.371, acc: 0.062] || [Gen loss: 2.016, acc: 0.000]\n",
      "Epoch:470 elapsed time: 14.68 [s]\n",
      ">>> Epoch: 471, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.352, acc: 0.062] || [Gen loss: 2.116, acc: 0.000]\n",
      "Epoch: 471 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.391\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.038\n",
      "Epoch:471 elapsed time: 20.09 [s]\n",
      ">>> Epoch: 472, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.270, acc: 0.354] || [F-Dis loss: 0.361, acc: 0.062] || [Gen loss: 2.112, acc: 0.000]\n",
      "Epoch:472 elapsed time: 14.93 [s]\n",
      ">>> Epoch: 473, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.369, acc: 0.062] || [Gen loss: 1.965, acc: 0.000]\n",
      "Epoch:473 elapsed time: 15.01 [s]\n",
      ">>> Epoch: 474, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.049, acc: 0.938] || [F-Dis loss: 0.400, acc: 0.062] || [Gen loss: 1.758, acc: 0.000]\n",
      "Epoch:474 elapsed time: 16.96 [s]\n",
      ">>> Epoch: 475, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.364, acc: 0.062] || [Gen loss: 1.712, acc: 0.000]\n",
      "Epoch:475 elapsed time: 18.08 [s]\n",
      ">>> Epoch: 476, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.023, acc: 0.938] || [F-Dis loss: 0.359, acc: 0.062] || [Gen loss: 1.845, acc: 0.000]\n",
      "Epoch:476 elapsed time: 16.92 [s]\n",
      ">>> Epoch: 477, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.016, acc: 0.938] || [F-Dis loss: 0.418, acc: 0.062] || [Gen loss: 1.698, acc: 0.000]\n",
      "Epoch:477 elapsed time: 18.51 [s]\n",
      ">>> Epoch: 478, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.440, acc: 0.062] || [Gen loss: 1.588, acc: 0.000]\n",
      "Epoch:478 elapsed time: 16.78 [s]\n",
      ">>> Epoch: 479, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.390, acc: 0.062] || [Gen loss: 1.611, acc: 0.000]\n",
      "Epoch:479 elapsed time: 15.87 [s]\n",
      ">>> Epoch: 480, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.031, acc: 0.938] || [F-Dis loss: 0.393, acc: 0.062] || [Gen loss: 1.727, acc: 0.000]\n",
      "Epoch:480 elapsed time: 16.51 [s]\n",
      ">>> Epoch: 481, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.345, acc: 0.062] || [Gen loss: 1.757, acc: 0.000]\n",
      "Epoch: 481 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.399\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.035\n",
      "Epoch:481 elapsed time: 22.92 [s]\n",
      ">>> Epoch: 482, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.277, acc: 0.354] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 1.932, acc: 0.000]\n",
      "Epoch:482 elapsed time: 15.96 [s]\n",
      ">>> Epoch: 483, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.050, acc: 0.938] || [F-Dis loss: 0.410, acc: 0.062] || [Gen loss: 1.607, acc: 0.000]\n",
      "Epoch:483 elapsed time: 15.88 [s]\n",
      ">>> Epoch: 484, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.371, acc: 0.062] || [Gen loss: 1.627, acc: 0.000]\n",
      "Epoch:484 elapsed time: 14.46 [s]\n",
      ">>> Epoch: 485, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.472, acc: 0.062] || [Gen loss: 1.640, acc: 0.000]\n",
      "Epoch:485 elapsed time: 14.05 [s]\n",
      ">>> Epoch: 486, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.026, acc: 0.938] || [F-Dis loss: 0.422, acc: 0.062] || [Gen loss: 1.622, acc: 0.000]\n",
      "Epoch:486 elapsed time: 13.93 [s]\n",
      ">>> Epoch: 487, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.360, acc: 0.062] || [Gen loss: 1.708, acc: 0.000]\n",
      "Epoch:487 elapsed time: 14.02 [s]\n",
      ">>> Epoch: 488, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.024, acc: 0.938] || [F-Dis loss: 0.441, acc: 0.062] || [Gen loss: 1.845, acc: 0.000]\n",
      "Epoch:488 elapsed time: 15.31 [s]\n",
      ">>> Epoch: 489, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.015, acc: 0.938] || [F-Dis loss: 0.366, acc: 0.062] || [Gen loss: 1.832, acc: 0.000]\n",
      "Epoch:489 elapsed time: 15.97 [s]\n",
      ">>> Epoch: 490, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.043, acc: 0.938] || [F-Dis loss: 0.475, acc: 0.062] || [Gen loss: 1.593, acc: 0.000]\n",
      "Epoch:490 elapsed time: 15.52 [s]\n",
      ">>> Epoch: 491, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.025, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 1.684, acc: 0.000]\n",
      "Epoch: 491 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.401\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.026\n",
      "Epoch:491 elapsed time: 20.55 [s]\n",
      ">>> Epoch: 492, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.277, acc: 0.354] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 1.768, acc: 0.000]\n",
      "Epoch:492 elapsed time: 15.84 [s]\n",
      ">>> Epoch: 493, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.040, acc: 0.938] || [F-Dis loss: 0.372, acc: 0.062] || [Gen loss: 1.721, acc: 0.000]\n",
      "Epoch:493 elapsed time: 16.12 [s]\n",
      ">>> Epoch: 494, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.375, acc: 0.062] || [Gen loss: 1.708, acc: 0.000]\n",
      "Epoch:494 elapsed time: 15.99 [s]\n",
      ">>> Epoch: 495, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.401, acc: 0.062] || [Gen loss: 1.781, acc: 0.000]\n",
      "Epoch:495 elapsed time: 14.97 [s]\n",
      ">>> Epoch: 496, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.395, acc: 0.062] || [Gen loss: 1.687, acc: 0.000]\n",
      "Epoch:496 elapsed time: 14.88 [s]\n",
      ">>> Epoch: 497, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.046, acc: 0.938] || [F-Dis loss: 0.394, acc: 0.062] || [Gen loss: 1.664, acc: 0.000]\n",
      "Epoch:497 elapsed time: 14.74 [s]\n",
      ">>> Epoch: 498, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.027, acc: 0.938] || [F-Dis loss: 0.486, acc: 0.062] || [Gen loss: 1.532, acc: 0.000]\n",
      "Epoch:498 elapsed time: 14.84 [s]\n",
      ">>> Epoch: 499, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.037, acc: 0.938] || [F-Dis loss: 0.380, acc: 0.062] || [Gen loss: 1.465, acc: 0.000]\n",
      "Epoch:499 elapsed time: 14.73 [s]\n",
      ">>> Epoch: 500, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.029, acc: 0.938] || [F-Dis loss: 0.406, acc: 0.062] || [Gen loss: 1.784, acc: 0.000]\n",
      "Epoch:500 elapsed time: 14.76 [s]\n",
      ">>> Epoch: 501, B/Ep: 1/1, Batch S: 32 -> [R-Dis loss: 0.032, acc: 0.938] || [F-Dis loss: 0.417, acc: 0.062] || [Gen loss: 1.677, acc: 0.000]\n",
      "Epoch: 501 Saving the training progress...\n",
      "No handles with labels found to put in legend.\n",
      "Batch Size 32 -> Samples: Fake: 16 & Real: 16\n",
      ">>> Test Fake -> Acc: 0.062 || Loss: 0.380\n",
      ">>> Test Real -> Acc: 0.938 || Loss: 0.025\n",
      "Epoch:501 elapsed time: 22.41 [s]\n"
     ]
    }
   ],
   "source": [
    "# training_gan(gen_model, dis_model, gan_model, gan_data, gan_train_cfg)\n",
    "training_gan(cgen_img_model, cdis_img_model, cgan_img_model, gan_data, gan_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}